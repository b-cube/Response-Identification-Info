{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Does it get better or worse?\n",
    "\n",
    "Presence of elements in ISO/FGDC:\n",
    "\n",
    "- data quality\n",
    "- data quality with lineage\n",
    "- attribute definitions\n",
    "- distribution information\n",
    "- metadata reference section\n",
    "\n",
    "Some of the above will include a word count value extracted from certain elements. For data quality, the count will come from the quality descriptions excluding lineage. The lineage word count will be included separately based on the process step descriptions. Attribute word counts will be taken for FGDC only, and from the main description plus any attribute descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json as js  # name conflict with sqla\n",
    "import sqlalchemy as sqla\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.dialects.postgresql import *\n",
    "from sqlalchemy import and_\n",
    "from semproc.xml_utils import *\n",
    "from mpp.models import Response\n",
    "from datetime import datetime\n",
    "from lxml import etree\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fgdc xpath sets for text extraction\n",
    "\n",
    "# for data quality sans lineage\n",
    "fgdc_dq_xpaths = [\n",
    "    ['dataqual', 'attracc', 'attraccr'],\n",
    "    ['dataqual', 'attracc', 'qattracc', 'attracce'],\n",
    "    ['dataqual', 'logic'],\n",
    "    ['dataqual', 'complete'],\n",
    "    ['dataqual', 'posacc', 'horizpa', 'horizpar'],\n",
    "    ['dataqual', 'posacc', 'horizpa', 'qhorizpa', 'horizpae'],\n",
    "    ['dataqual', 'posacc', 'vertacc', 'vertaccr'],\n",
    "    ['dataqual', 'posacc', 'vertacc', 'qhorizpa', 'vertacce']\n",
    "]\n",
    "\n",
    "# for data quality lineage\n",
    "fgdc_lineage_xpaths = [\n",
    "    ['dataqual', 'lineage', 'procstep', 'procdesc']\n",
    "]\n",
    "\n",
    "# for attributes\n",
    "fgdc_attr_xpaths = [\n",
    "    ['eainfo', 'overview'],\n",
    "    ['eainfo', 'eadetcit'],\n",
    "    ['eainfo', 'detailed', 'attr', 'attrdef'],\n",
    "    ['eainfo', 'detailed', 'attr', 'attrlabl']\n",
    "]\n",
    "\n",
    "# for identifying number of distribution links vs offline resources\n",
    "# xpath returns the number of elements\n",
    "fgdc_distributions = [\n",
    "    ('online_refs', 'count(distinfo/stdorder/digform/digtopt/onlinopt/computer/networka/networkr)'),\n",
    "    ('offline_refs', 'count(distinfo/stdorder/digform/digtopt/offoptn/offmedia)'),\n",
    "    ('nondigital_refs', 'count(distinfo/stdorder/nondig)')\n",
    "]\n",
    "\n",
    "# checks just for a decent existence\n",
    "# mandatory doesn't mean they exist :/\n",
    "fgdc_existences = [\n",
    "    ('data_quality', 'dataqual/logic or dataqual/complete'),\n",
    "    ('lineage', 'count(dataqual/lineage/procstep) > 0'),\n",
    "    ('attribute_ref', 'eainfo/detailed/attrdef or eainfo/detailed/attrlabl'),\n",
    "    ('metadata_ref', 'count(metainfo/metstdn) > 0')  # just check for the standard name\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iso xpath sets\n",
    "\n",
    "# for data quality sans lineage\n",
    "# NOTE: starting with these, not sure the pattern report/{name}/*Description/CharacterString is viable\n",
    "iso_dq_xpaths = [\n",
    "    ['*', 'dataQualityInfo', 'DQ_DataQuality', 'report', 'DQ_CompletenessOmission', 'evaluationMethodDescription', 'CharacterString'],\n",
    "    ['*', 'dataQualityInfo', 'DQ_DataQuality', 'report', 'DQ_CompletenessCommission', 'evaluationMethodDescription', 'CharacterString'],\n",
    "    ['*', 'dataQualityInfo', 'DQ_DataQuality', 'report', 'DQ_ConceptualConsistency', 'methodDescription', 'CharacterString']\n",
    "]\n",
    "\n",
    "# for data quality lineage\n",
    "iso_lineage_xpaths = [\n",
    "    ['*', 'dataQualityInfo', 'DQ_DataQuality', 'lineage', 'LI_Lineage', 'processStep', 'LI_ProcessStep', 'description', 'CharacterString']\n",
    "]\n",
    "\n",
    "# for attributes\n",
    "# NOTE: \n",
    "iso_attr_xpaths = [\n",
    "    \n",
    "]\n",
    "\n",
    "iso_distributions = [\n",
    "    ('online_refs', 'count(//*/*[local-name()=\"MD_DigitalTransferOptions\"]/*[local-name()=\"onLine\"]/*[local-name()=\"CI_OnlineResource\"]/*[local-name()=\"linkage\"]/*[local-name()=\"URL\"])')\n",
    "]\n",
    "\n",
    "# NOTE: not counting bands as attribute definitions here.\n",
    "iso_existences = [\n",
    "    ('data_quality', 'count(*[local-name()=\"dataQualityInfo\"]/*[local-name()=\"DQ_DataQuality\"]) > 0'),\n",
    "    ('lineage', 'count(*[local-name()=\"dataQualityInfo\"]/*[local-name()=\"DQ_DataQuality\"]/*[local-name()=\"lineage\"]/*/*/*[local-name()=\"LI_ProcessStep\"]) > 0'),\n",
    "    ('metadata_ref', 'count(metainfo/metstdn) > 0')  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return a dict of fq xpaths: text from one of our sets\n",
    "# in this case, we aren't interested in element attributes\n",
    "# or in iterating over each child, just elements where\n",
    "# there's an expectation (based on cultural practices)\n",
    "# of finding descriptive text.\n",
    "def extract(xml, xpath):\n",
    "    elems = extract_elems(xml, xpath)\n",
    "    for elem in elems:\n",
    "        text = elem.text if elem.text else ''\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        # xpath definition doesn't necessarily include\n",
    "        # every elem name from parent, so return exact path\n",
    "        tags = '/'.join(_taggify(elem))\n",
    "        yield (tags, text.strip())\n",
    "\n",
    "def _extract_tag(t):\n",
    "    if not t:\n",
    "        return\n",
    "    return t.split('}')[-1]\n",
    "\n",
    "def _taggify(e):\n",
    "    tags = [e.tag] + [m.tag for m in e.iterancestors()]\n",
    "    tags.reverse()\n",
    "\n",
    "    try:\n",
    "        return [_extract_tag(t) for t in tags]\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "def convert_to_bag(arr):\n",
    "    # we have some array of strings and we want\n",
    "    # tokens. not going to worry about numbers\n",
    "    # or urns or what have you today.\n",
    "    return ' '.join([a[1] for a in arr]).split()\n",
    "\n",
    "def check_existence(xml, check):\n",
    "    return xml.xpath(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the postgres connection file\n",
    "with open('../local/big_rds.conf', 'r') as f:\n",
    "    conf = js.loads(f.read())\n",
    "\n",
    "# our connection\n",
    "engine = sqla.create_engine(conf.get('connection'))\n",
    "Session = sessionmaker()\n",
    "Session.configure(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sketchy_sql = '''with i\n",
    "as (\n",
    "    select d.response_id, jsonb_array_elements(d.identity::jsonb) ident\n",
    "    from identities d\n",
    "    where d.identity is not null\n",
    ")\n",
    "\n",
    "select r.id, r.source_url, r.source_url_sha, r.cleaned_content, i.ident->'protocol' as protocol\n",
    "from responses r join i on i.response_id = r.id\n",
    "where i.ident->>'protocol' = 'ISO'\n",
    "limit %s\n",
    "offset %s;\n",
    "'''\n",
    "\n",
    "# where i.ident->>'protocol' = 'FGDC' or i.ident->>'protocol' = 'ISO'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xml fail 145560\n",
      "xml fail 183293\n",
      "xml fail 196865\n",
      "xml fail 219701\n",
      "xml fail 223566\n",
      "xml fail 252783\n",
      "xml fail 307810\n",
      "xml fail 351247\n",
      "xml fail 402936\n",
      "xml fail 453490\n",
      "xml fail 503992\n",
      "xml fail 539074\n",
      "xml fail 563466\n",
      "xml fail 576196\n",
      "xml fail 653347\n",
      "xml fail 667256\n",
      "xml fail 721563\n",
      "xml fail 722226\n"
     ]
    }
   ],
   "source": [
    "# LIMIT=500\n",
    "# for i in xrange(0, 46000, LIMIT):\n",
    "\n",
    "# 26300 for fgdc\n",
    "# 19700 for ISO\n",
    "\n",
    "LIMIT = 500\n",
    "for i in xrange(0, 19700, LIMIT):\n",
    "    sql = sketchy_sql % (LIMIT, i)\n",
    "    result = session.execute(sql)\n",
    "    for r in result:\n",
    "        if os.path.exists('outputs/metrics/%s.json' % r['id']):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            xml = etree.fromstring(r['cleaned_content'].encode('utf-8'))\n",
    "        except:\n",
    "            print 'xml fail', r['id']\n",
    "            continue\n",
    "            \n",
    "        metrics = {\n",
    "            \"data_quality\": False,\n",
    "            \"data_quality_bow\": 0,\n",
    "            \"lineage\": False,\n",
    "            \"lineage_bow\": 0,\n",
    "            \"attribute_ref\": False,\n",
    "            \"attribute_bow\": 0,\n",
    "            \"metadata_ref\": False,\n",
    "            \"distribution\": {}\n",
    "        }\n",
    "\n",
    "        if r['protocol'] == 'ISO':\n",
    "            for ename, expath in iso_existences:\n",
    "                metrics[ename] = check_existence(xml, expath)\n",
    "            \n",
    "            # data quality\n",
    "            arr = []\n",
    "            for xp in iso_dq_xpaths:\n",
    "                arr += [d for d in extract(xml, xp)]\n",
    "            \n",
    "            metrics['data_quality_bow'] = len(convert_to_bag(arr))\n",
    "            \n",
    "            # dataqual lineage\n",
    "            arr = []\n",
    "            for xp in iso_lineage_xpaths:\n",
    "                arr += [d for d in extract(xml, xp)]\n",
    "            \n",
    "            metrics['lineage_bow'] = len(convert_to_bag(arr))\n",
    "            \n",
    "#             # eainfo\n",
    "#             arr = []\n",
    "#             for xp in iso_attr_xpaths:\n",
    "#                 arr += [d for d in extract(xml, xp)]\n",
    "            \n",
    "#             metrics['attribute_bow'] = len(convert_to_bag(arr))\n",
    "            \n",
    "            # count the kinds of distribution access points\n",
    "            for dname, dxpath in iso_distributions:\n",
    "                metrics['distribution'][dname] = check_existence(xml, dxpath)\n",
    "                \n",
    "            del metrics['attribute_bow']\n",
    "            del metrics['attribute_ref']\n",
    "\n",
    "        elif r['protocol'] == 'FGDC':\n",
    "            for ename, expath in fgdc_existences:\n",
    "                metrics[ename] = check_existence(xml, expath)\n",
    "            \n",
    "            # data quality\n",
    "            arr = []\n",
    "            for xp in fgdc_dq_xpaths:\n",
    "                arr += [d for d in extract(xml, xp)]\n",
    "            \n",
    "            metrics['data_quality_bow'] = len(convert_to_bag(arr))\n",
    "            \n",
    "            # dataqual lineage\n",
    "            arr = []\n",
    "            for xp in fgdc_lineage_xpaths:\n",
    "                arr += [d for d in extract(xml, xp)]\n",
    "            \n",
    "            metrics['lineage_bow'] = len(convert_to_bag(arr))\n",
    "            \n",
    "            # eainfo\n",
    "            arr = []\n",
    "            for xp in fgdc_attr_xpaths:\n",
    "                arr += [d for d in extract(xml, xp)]\n",
    "            \n",
    "            metrics['attribute_bow'] = len(convert_to_bag(arr))\n",
    "            \n",
    "            # count the kinds of distribution access points\n",
    "            for dname, dxpath in fgdc_distributions:\n",
    "                metrics['distribution'][dname] = check_existence(xml, dxpath)\n",
    "        \n",
    "#         print r['id'], r['source_url']\n",
    "#         print metrics\n",
    "#         print\n",
    "#         print\n",
    "            \n",
    "        with open('outputs/metrics/%s.json' % r['id'], 'w') as g:\n",
    "            g.write(js.dumps(metrics, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the metrics into the rds\n",
    "import glob\n",
    "import json as js\n",
    "import sqlalchemy as sqla\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import (\n",
    "    MetaData,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    "    Boolean,\n",
    "    DateTime,\n",
    ")\n",
    "from sqlalchemy.dialects.postgresql import *\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class Metric(Base):\n",
    "    __tablename__ = 'metadata_age_metrics'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    completeness = Column(JSON)\n",
    "    response_id = Column(Integer)\n",
    "    \n",
    "# load the postgres connection file\n",
    "with open('../local/big_rds.conf', 'r') as f:\n",
    "    conf = js.loads(f.read())\n",
    "\n",
    "# our connection\n",
    "engine = sqla.create_engine(conf.get('connection'))\n",
    "Session = sessionmaker()\n",
    "Session.configure(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "files = glob.glob('outputs/metrics/*.json')\n",
    "for f in files:\n",
    "    response_id = f.split('/')[-1].replace('.json', '')\n",
    "    \n",
    "    if session.query(Metric).filter(Metric.response_id==response_id).count() > 0:\n",
    "        continue\n",
    "    \n",
    "    with open(f, 'r') as g:\n",
    "        data = js.loads(g.read())\n",
    "    \n",
    "    metric = Metric(\n",
    "        response_id=response_id,\n",
    "        completeness=data\n",
    "    )\n",
    "    try:\n",
    "        session.add(metric)\n",
    "        session.commit()\n",
    "    except:\n",
    "        session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unfortunate lapse\n",
    "# let's also grab the URLs/media definitions to\n",
    "# say of the online (only) references, which are\n",
    "# actually externally referencable\n",
    "\n",
    "import json as js\n",
    "import requests\n",
    "from rfc3987 import parse as uparse\n",
    "import sqlalchemy as sqla\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import (\n",
    "    MetaData,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    "    Boolean,\n",
    "    DateTime,\n",
    ")\n",
    "from sqlalchemy.dialects.postgresql import *\n",
    "from datetime import datetime\n",
    "import os\n",
    "from lxml import etree\n",
    "\n",
    "# load the postgres connection file\n",
    "with open('../local/big_rds.conf', 'r') as f:\n",
    "    conf = js.loads(f.read())\n",
    "\n",
    "# our connection\n",
    "engine = sqla.create_engine(conf.get('connection'))\n",
    "Session = sessionmaker()\n",
    "Session.configure(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "sketchy_sql = '''with i\n",
    "as (\n",
    "    select d.response_id, jsonb_array_elements(d.identity::jsonb) ident\n",
    "    from identities d\n",
    "    where d.identity is not null\n",
    ")\n",
    "\n",
    "select r.id, r.source_url, r.source_url_sha, r.cleaned_content, i.ident->'protocol' as protocol\n",
    "from responses r join i on i.response_id = r.id\n",
    "where (i.ident->>'protocol' = 'ISO' or i.ident->>'protocol' = 'FGDC') and r.format = 'xml'\n",
    "limit %s\n",
    "offset %s;\n",
    "'''\n",
    "\n",
    "# 26300 for fgdc\n",
    "# 19700 for ISO\n",
    "\n",
    "LIMIT = 500\n",
    "END = 19700+26300\n",
    "# END = 5\n",
    "# LIMIT=5\n",
    "for i in xrange(0, END, LIMIT):\n",
    "    sql = sketchy_sql % (LIMIT, i)\n",
    "    result = session.execute(sql)\n",
    "    for r in result:\n",
    "        if os.path.exists('outputs/online_refs/%s.json' % r['id']):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            xml = etree.fromstring(r['cleaned_content'].encode('utf-8'))\n",
    "        except Exception as ex:\n",
    "            print 'xml fail', r['id']\n",
    "            continue\n",
    "\n",
    "        if r['protocol'] == 'ISO':\n",
    "            xp = '//*/*[local-name()=\"MD_DigitalTransferOptions\"]/*[local-name()=\"onLine\"]/*[local-name()=\"CI_OnlineResource\"]/*[local-name()=\"linkage\"]/*[local-name()=\"URL\"]'\n",
    "        elif r['protocol'] == 'FGDC':\n",
    "            xp = 'distinfo/stdorder/digform/digtopt/onlinopt/computer/networka/networkr'\n",
    "        \n",
    "        refs = []\n",
    "        elems = xml.xpath(xp)\n",
    "        for elem in elems:\n",
    "            text = elem.text\n",
    "            if not text:\n",
    "                continue\n",
    "            \n",
    "            text = text.strip()\n",
    "            \n",
    "            # is it a valid URL and, you know, we're here so let's \n",
    "            # just make a little HEAD request to ask\n",
    "            ref = {\n",
    "                \"url\": text,\n",
    "                \"checked\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                u = uparse(text, rule='URI')\n",
    "                \n",
    "                if u['scheme'] == 'file':\n",
    "                    ref['error'] = 'file path'\n",
    "                    refs.append(ref)\n",
    "                    continue\n",
    "            except:\n",
    "                # it's not a valid scheme://location/path (http or otherwise)\n",
    "                ref[\"error\"] = 'probable local path'\n",
    "                refs.append(ref)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                rsp = requests.head(text, timeout=30)\n",
    "            except:\n",
    "                ref[\"error\"] = \"HEAD request failed\"\n",
    "                refs.append(ref)\n",
    "                continue\n",
    "            \n",
    "            # just get the status code\n",
    "            ref['status'] = rsp.status_code\n",
    "            \n",
    "            refs.append(ref)\n",
    "            \n",
    "            \n",
    "        with open('outputs/online_refs/%s.json' % r['id'], 'w') as g:\n",
    "            g.write(js.dumps(refs, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'authority': 'www.someinth.com',\n",
       " 'fragment': None,\n",
       " 'path': '/f',\n",
       " 'query': None,\n",
       " 'scheme': 'http'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rfc3987 import parse as uparse\n",
    "\n",
    "# (‘IRI’, \n",
    "#  ‘absolute_IRI’, \n",
    "#  ‘irelative_ref’, \n",
    "#  ‘irelative_part’, \n",
    "#  ‘URI_reference’, \n",
    "#  ‘URI’, \n",
    "#  ‘absolute_URI’, \n",
    "#  ‘relative_ref’, \n",
    "#  ‘relative_part’)\n",
    "\n",
    "uparse('http://www.someinth.com/f', rule='URI')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
