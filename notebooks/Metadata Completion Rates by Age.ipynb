{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Does it get better or worse?\n",
    "\n",
    "Presence of elements in ISO/FGDC:\n",
    "\n",
    "- data quality\n",
    "- data quality with lineage\n",
    "- attribute definitions\n",
    "- distribution information\n",
    "- metadata reference section\n",
    "\n",
    "Some of the above will include a word count value extracted from certain elements. For data quality, the count will come from the quality descriptions excluding lineage. The lineage word count will be included separately based on the process step descriptions. Attribute word counts will be taken for FGDC only, and from the main description plus any attribute descriptions.\n",
    "\n",
    "What we want for the data quality, lineage and FGDC attribute info:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"superset\": [\n",
    "        ('tag', [element as text])\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "For the distributions:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"online\": [\n",
    "        ('tag', 'probable url', status_code, 'date_verified')\n",
    "    ],\n",
    "    \"offline\": [\n",
    "        ('tag', 'file')\n",
    "    ],\n",
    "    \"nondigital\": [\n",
    "        ('tag', 'type')\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "For the existence measures (a little different, includes the metadata section):\n",
    "\n",
    "```\n",
    "{\n",
    "    \"superset\": boolean //if element count > 0\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json as js  # name conflict with sqla\n",
    "import sqlalchemy as sqla\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.dialects.postgresql import *\n",
    "from sqlalchemy import and_\n",
    "from semproc.xml_utils import *\n",
    "from mpp.models import Response\n",
    "from datetime import datetime\n",
    "from lxml import etree\n",
    "import requests\n",
    "import os\n",
    "from rfc3987 import parse as uparse\n",
    "from sqlalchemy import (\n",
    "    MetaData,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    "    Boolean,\n",
    "    DateTime,\n",
    ")\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "Base = declarative_base()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xpaths = {\n",
    "    \"ISO\": {\n",
    "        \"bags\": {\n",
    "            \"data_quality\": [\n",
    "                ['//*', 'dataQualityInfo', 'DQ_DataQuality', 'report', 'DQ_CompletenessOmission', 'evaluationMethodDescription', 'CharacterString'],\n",
    "                ['//*', 'dataQualityInfo', 'DQ_DataQuality', 'report', 'DQ_CompletenessCommission', 'evaluationMethodDescription', 'CharacterString'],\n",
    "                ['//*', 'dataQualityInfo', 'DQ_DataQuality', 'report', 'DQ_ConceptualConsistency', 'methodDescription', 'CharacterString']\n",
    "            ],\n",
    "            \"lineage\": [\n",
    "                ['//*', 'dataQualityInfo', 'DQ_DataQuality', 'lineage', 'LI_Lineage', 'processStep', 'LI_ProcessStep', 'description', 'CharacterString'],\n",
    "                ['//*', 'dataQualityInfo', 'DQ_DataQuality', 'lineage', 'LI_Lineage', 'statement', 'CharacterString']\n",
    "            ],\n",
    "            \"attributes\": []\n",
    "        },\n",
    "        \"distributions\": {\n",
    "            \"online\": [\n",
    "                ['//*', 'MD_DigitalTransferOptions', 'onLine', 'CI_OnlineResource', 'linkage', 'URL']\n",
    "            ],\n",
    "            \"offline\": [],\n",
    "            \"nondigital\": []\n",
    "        },\n",
    "        \"existences\": {\n",
    "            \"data_quality\": \"\"\"count(//*[local-name()=\"dataQualityInfo\"]/*[local-name()=\"DQ_DataQuality\"]) > 0\"\"\",\n",
    "            \"lineage\": \"\"\"count(//*[local-name()=\"dataQualityInfo\"]/*[local-name()=\"DQ_DataQuality\"]/*[local-name()=\"lineage\"]/*/*/*[local-name()=\"LI_ProcessStep\"]) > 0\"\"\",\n",
    "            \"attribute\": \"\",\n",
    "            \"metadata\": \"\"\"count(//*[local-name()=\"metadataMaintenance\"]/*[local-name()=\"maintenanceAndUpdateFrequency\"]) > 0\"\"\"\n",
    "        }\n",
    "    },\n",
    "    \"FGDC\": {\n",
    "        \"bags\": {\n",
    "            \"data_quality\": [\n",
    "                ['dataqual', 'attracc', 'attraccr'],\n",
    "                ['dataqual', 'attracc', 'qattracc', 'attracce'],\n",
    "                ['dataqual', 'logic'],\n",
    "                ['dataqual', 'complete'],\n",
    "                ['dataqual', 'posacc', 'horizpa', 'horizpar'],\n",
    "                ['dataqual', 'posacc', 'horizpa', 'qhorizpa', 'horizpae'],\n",
    "                ['dataqual', 'posacc', 'vertacc', 'vertaccr'],\n",
    "                ['dataqual', 'posacc', 'vertacc', 'qhorizpa', 'vertacce']\n",
    "            ],\n",
    "            \"lineage\": [\n",
    "                ['dataqual', 'lineage', 'procstep', 'procdesc']\n",
    "            ],\n",
    "            \"attributes\": [\n",
    "                ['eainfo', 'overview'],\n",
    "                ['eainfo', 'eadetcit'],\n",
    "                ['eainfo', 'detailed', 'attr', 'attrdef'],\n",
    "                ['eainfo', 'detailed', 'attr', 'attrlabl']\n",
    "            ]\n",
    "        },\n",
    "        \"distributions\": {\n",
    "            \"online\": [\n",
    "                ['distinfo','stdorder','digform','digtopt','onlinopt','computer','networka','networkr']\n",
    "            ],\n",
    "            \"offline\": [\n",
    "                ['distinfo','stdorder','digform','digtopt','offoptn','offmedia']\n",
    "            ],\n",
    "            \"nondigital\": [\n",
    "                ['distinfo','stdorder','nondig']\n",
    "            ]\n",
    "        },\n",
    "        \"existences\": {\n",
    "            \"data_quality\": \"dataqual/logic or dataqual/complete\",\n",
    "            \"lineage\": \"count(dataqual/lineage/procstep) > 0\",\n",
    "            \"attribute\": \"eainfo/detailed/attrdef or eainfo/detailed/attrlabl\",\n",
    "            \"metadata\": \"count(metainfo/metstdn) > 0\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return a dict of fq xpaths: text from one of our sets\n",
    "# in this case, we aren't interested in element attributes\n",
    "# or in iterating over each child, just elements where\n",
    "# there's an expectation (based on cultural practices)\n",
    "# of finding descriptive text.\n",
    "def extract(xml, xpath):\n",
    "    elems = extract_elems(xml, xpath)\n",
    "    for elem in elems:\n",
    "        text = elem.text if elem.text else ''\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        # xpath definition doesn't necessarily include\n",
    "        # every elem name from parent, so return exact path\n",
    "        tags = '/'.join(_taggify(elem))\n",
    "        yield (tags, text.strip(), len(text.split()))\n",
    "\n",
    "def _extract_tag(t):\n",
    "    if not t:\n",
    "        return\n",
    "    return t.split('}')[-1]\n",
    "\n",
    "def _taggify(e):\n",
    "    tags = [e.tag] + [m.tag for m in e.iterancestors()]\n",
    "    tags.reverse()\n",
    "\n",
    "    try:\n",
    "        return [_extract_tag(t) for t in tags]\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "def convert_to_bag(arr):\n",
    "    # we have some array of strings and we want\n",
    "    # tokens. not going to worry about numbers\n",
    "    # or urns or what have you today.\n",
    "    return ' '.join([a[1] for a in arr]).split()\n",
    "\n",
    "def check_existence(xml, check):\n",
    "    return xml.xpath(check)\n",
    "\n",
    "def check_link(url):\n",
    "    # return status, error\n",
    "    try:\n",
    "        u = uparse(url, rule='URI')\n",
    "\n",
    "        if u['scheme'] == 'file':\n",
    "            return 999, 'file path'\n",
    "    except:\n",
    "        # it's not a valid scheme://location/path (http or otherwise)\n",
    "        return 999, 'probable file path'\n",
    "\n",
    "    try:\n",
    "        rsp = requests.head(url, timeout=30)\n",
    "    except:\n",
    "        return 999, 'failed HEAD request'\n",
    "\n",
    "    # just get the status code\n",
    "    return rsp.status_code, ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the postgres connection file\n",
    "with open('../local/big_rds.conf', 'r') as f:\n",
    "    conf = js.loads(f.read())\n",
    "\n",
    "# our connection\n",
    "engine = sqla.create_engine(conf.get('connection'))\n",
    "Session = sessionmaker()\n",
    "Session.configure(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Complete(Base):\n",
    "    __tablename__ = 'metadata_completeness'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    wordcounts = Column(JSON)\n",
    "    distributions = Column(JSON)\n",
    "    existences = Column(JSON)\n",
    "    response_id = Column(Integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the query\n",
    "sketchy_sql = '''with i\n",
    "as (\n",
    "    select d.response_id, jsonb_array_elements(d.identity::jsonb) ident\n",
    "    from identities d\n",
    "    where d.identity is not null\n",
    ")\n",
    "\n",
    "select r.id, r.source_url, r.source_url_sha, r.cleaned_content, i.ident->'protocol' as protocol\n",
    "from responses r join i on i.response_id = r.id\n",
    "where i.ident->>'protocol' = 'ISO' or i.ident->>'protocol' = 'FGDC'\n",
    "limit %s\n",
    "offset %s;\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xml fail 145560\n",
      "xml fail 183293\n",
      "xml fail 196865\n",
      "xml fail 219701\n",
      "xml fail 223566\n",
      "xml fail 252783\n",
      "xml fail 307810\n",
      "xml fail 351247\n",
      "xml fail 402936\n",
      "xml fail 453490\n",
      "xml fail 503992\n",
      "xml fail 539074\n",
      "xml fail 563466\n",
      "xml fail 576196\n",
      "xml fail 653347\n",
      "xml fail 667256\n",
      "xml fail 721563\n",
      "xml fail 722226\n"
     ]
    }
   ],
   "source": [
    "# 26300 for fgdc\n",
    "# 19700 for ISO\n",
    "\n",
    "END = 46000\n",
    "LIMIT = 50\n",
    "\n",
    "# END = 10\n",
    "# LIMIT = 10\n",
    "for i in xrange(0, END, LIMIT):\n",
    "    sql = sketchy_sql % (LIMIT, i)\n",
    "    result = session.execute(sql)\n",
    "    for r in result:\n",
    "        if os.path.exists('outputs/metrics/%s.json' % r['id']):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            xml = etree.fromstring(r['cleaned_content'].encode('utf-8'))\n",
    "        except:\n",
    "            print 'xml fail', r['id']\n",
    "            continue\n",
    "        \n",
    "        protocol = r['protocol']\n",
    "        \n",
    "        options = xpaths.get(protocol, {})\n",
    "        \n",
    "        bag_xpaths = options.get('bags')\n",
    "        distribution_xpaths = options.get('distributions')\n",
    "        existence_xpaths = options.get('existences')\n",
    "        \n",
    "        bag_info = {}\n",
    "        dist_info = {}\n",
    "        existence_info = {}\n",
    "        \n",
    "        for key, bxps in bag_xpaths.iteritems():\n",
    "            bag_info[key] = []\n",
    "            for bxp in bxps:\n",
    "                bag_info[key] += [{\"tag\": tag, \"text\": text, \"tokens\": length} for tag, text, length in extract(xml, bxp)]\n",
    "            \n",
    "        \n",
    "        for key, dxps in distribution_xpaths.iteritems():\n",
    "            # extract \n",
    "            paths = []\n",
    "            for dxp in dxps:\n",
    "                paths += [{\"path\": p.text} for p in extract_elems(xml, dxp)]\n",
    "            \n",
    "            # if online, try a request\n",
    "            if key == 'online':\n",
    "                for i, p in enumerate(paths):\n",
    "                    status, error = check_link(p.get('path'))\n",
    "                    p.update({\n",
    "                        \"status\": status, \n",
    "                        \"error\": error, \n",
    "                        \"date_verified\": datetime.now().isoformat()\n",
    "                    })\n",
    "                    paths[i] = p\n",
    "            dist_info[key] = paths\n",
    "        \n",
    "        for key, exp in existence_xpaths.iteritems():\n",
    "            existence_info[key] = check_existence(xml, exp) if exp else False\n",
    "        \n",
    "#         complete = Complete(\n",
    "#             response_id=r['id'],\n",
    "#             wordcounts=bag_info,\n",
    "#             distributions=dist_info,\n",
    "#             existences=existence_info\n",
    "#         )\n",
    "        \n",
    "#         try:\n",
    "#             session.add(complete)\n",
    "#             session.commit()\n",
    "#         except Exception as ex:\n",
    "#             print 'commit failed'\n",
    "#             session.rollback()\n",
    "            \n",
    "        with open('outputs/metrics/%s.json' % r['id'], 'w') as g:\n",
    "            g.write(js.dumps({\"bags\": bag_info, \"dists\": dist_info, \"exists\": existence_info}, indent=4))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob('outputs/metrics/*.json')\n",
    "\n",
    "for f in files:\n",
    "    with open(f, 'r') as g:\n",
    "        data = js.loads(g.read())\n",
    "    \n",
    "    r = int(f.split('/')[-1].replace('.json', ''))\n",
    "    \n",
    "    if session.query(Complete).filter(Complete.response_id==r).count() > 0:\n",
    "        continue\n",
    "\n",
    "    complete = Complete(\n",
    "        response_id=r,\n",
    "        wordcounts=data.get('bags'),\n",
    "        distributions=data.get('dists'),\n",
    "        existences=data.get('exists')\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        session.add(complete)\n",
    "        session.commit()\n",
    "    except Exception as ex:\n",
    "        print 'commit failed'\n",
    "        session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element {http://www.isotc211.org/2005/gco}CharacterString at 0x105db7cf8>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res, = session.query(Response.cleaned_content).filter(Response.id==137754).first()\n",
    "xml = etree.fromstring(res.encode('utf-8'))\n",
    "extract_elems(xml, ['//*', 'dataQualityInfo', 'DQ_DataQuality', 'lineage', 'LI_Lineage', 'statement', 'CharacterString'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fgdc xpath sets for text extraction\n",
    "\n",
    "# for data quality sans lineage\n",
    "fgdc_dq_xpaths = [\n",
    "    ['dataqual', 'attracc', 'attraccr'],\n",
    "    ['dataqual', 'attracc', 'qattracc', 'attracce'],\n",
    "    ['dataqual', 'logic'],\n",
    "    ['dataqual', 'complete'],\n",
    "    ['dataqual', 'posacc', 'horizpa', 'horizpar'],\n",
    "    ['dataqual', 'posacc', 'horizpa', 'qhorizpa', 'horizpae'],\n",
    "    ['dataqual', 'posacc', 'vertacc', 'vertaccr'],\n",
    "    ['dataqual', 'posacc', 'vertacc', 'qhorizpa', 'vertacce']\n",
    "]\n",
    "\n",
    "# for data quality lineage\n",
    "fgdc_lineage_xpaths = [\n",
    "    ['dataqual', 'lineage', 'procstep', 'procdesc']\n",
    "]\n",
    "\n",
    "# for attributes\n",
    "fgdc_attr_xpaths = [\n",
    "    ['eainfo', 'overview'],\n",
    "    ['eainfo', 'eadetcit'],\n",
    "    ['eainfo', 'detailed', 'attr', 'attrdef'],\n",
    "    ['eainfo', 'detailed', 'attr', 'attrlabl']\n",
    "]\n",
    "\n",
    "# for identifying number of distribution links vs offline resources\n",
    "# xpath returns the number of elements\n",
    "fgdc_distributions = [\n",
    "    ('online_refs', 'count(distinfo/stdorder/digform/digtopt/onlinopt/computer/networka/networkr)'),\n",
    "    ('offline_refs', 'count(distinfo/stdorder/digform/digtopt/offoptn/offmedia)'),\n",
    "    ('nondigital_refs', 'count(distinfo/stdorder/nondig)')\n",
    "]\n",
    "\n",
    "# checks just for a decent existence\n",
    "# mandatory doesn't mean they exist :/\n",
    "fgdc_existences = [\n",
    "    ('data_quality', 'dataqual/logic or dataqual/complete'),\n",
    "    ('lineage', 'count(dataqual/lineage/procstep) > 0'),\n",
    "    ('attribute_ref', 'eainfo/detailed/attrdef or eainfo/detailed/attrlabl'),\n",
    "    ('metadata_ref', 'count(metainfo/metstdn) > 0')  # just check for the standard name\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iso xpath sets\n",
    "\n",
    "# for data quality sans lineage\n",
    "# NOTE: starting with these, not sure the pattern report/{name}/*Description/CharacterString is viable\n",
    "iso_dq_xpaths = [\n",
    "    ['*', 'dataQualityInfo', 'DQ_DataQuality', 'report', 'DQ_CompletenessOmission', 'evaluationMethodDescription', 'CharacterString'],\n",
    "    ['*', 'dataQualityInfo', 'DQ_DataQuality', 'report', 'DQ_CompletenessCommission', 'evaluationMethodDescription', 'CharacterString'],\n",
    "    ['*', 'dataQualityInfo', 'DQ_DataQuality', 'report', 'DQ_ConceptualConsistency', 'methodDescription', 'CharacterString']\n",
    "]\n",
    "\n",
    "# for data quality lineage\n",
    "iso_lineage_xpaths = [\n",
    "    ['*', 'dataQualityInfo', 'DQ_DataQuality', 'lineage', 'LI_Lineage', 'processStep', 'LI_ProcessStep', 'description', 'CharacterString']\n",
    "    ['*', 'dataQualityInfo', 'DQ_DataQuality', 'lineage', 'LI_Lineage', 'statement', 'CharacterString']\n",
    "]\n",
    "\n",
    "# for attributes\n",
    "# NOTE: \n",
    "iso_attr_xpaths = [\n",
    "    \n",
    "]\n",
    "\n",
    "iso_distributions = [\n",
    "    ('online_refs', 'count(//*/*[local-name()=\"MD_DigitalTransferOptions\"]/*[local-name()=\"onLine\"]/*[local-name()=\"CI_OnlineResource\"]/*[local-name()=\"linkage\"]/*[local-name()=\"URL\"])')\n",
    "]\n",
    "\n",
    "# NOTE: not counting bands as attribute definitions here.\n",
    "iso_existences = [\n",
    "    ('data_quality', 'count(*[local-name()=\"dataQualityInfo\"]/*[local-name()=\"DQ_DataQuality\"]) > 0'),\n",
    "    ('lineage', 'count(*[local-name()=\"dataQualityInfo\"]/*[local-name()=\"DQ_DataQuality\"]/*[local-name()=\"lineage\"]/*/*/*[local-name()=\"LI_ProcessStep\"]) > 0'),\n",
    "    ('metadata_ref', 'count(metainfo/metstdn) > 0')  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sketchy_sql = '''with i\n",
    "as (\n",
    "    select d.response_id, jsonb_array_elements(d.identity::jsonb) ident\n",
    "    from identities d\n",
    "    where d.identity is not null\n",
    ")\n",
    "\n",
    "select r.id, r.source_url, r.source_url_sha, r.cleaned_content, i.ident->'protocol' as protocol\n",
    "from responses r join i on i.response_id = r.id\n",
    "where i.ident->>'protocol' = 'ISO'\n",
    "limit %s\n",
    "offset %s;\n",
    "'''\n",
    "\n",
    "# where i.ident->>'protocol' = 'FGDC' or i.ident->>'protocol' = 'ISO'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xml fail 145560\n",
      "xml fail 183293\n",
      "xml fail 196865\n",
      "xml fail 219701\n",
      "xml fail 223566\n",
      "xml fail 252783\n",
      "xml fail 307810\n",
      "xml fail 351247\n",
      "xml fail 402936\n",
      "xml fail 453490\n",
      "xml fail 503992\n",
      "xml fail 539074\n",
      "xml fail 563466\n",
      "xml fail 576196\n",
      "xml fail 653347\n",
      "xml fail 667256\n",
      "xml fail 721563\n",
      "xml fail 722226\n"
     ]
    }
   ],
   "source": [
    "# LIMIT=500\n",
    "# for i in xrange(0, 46000, LIMIT):\n",
    "\n",
    "# 26300 for fgdc\n",
    "# 19700 for ISO\n",
    "\n",
    "LIMIT = 500\n",
    "for i in xrange(0, 19700, LIMIT):\n",
    "    sql = sketchy_sql % (LIMIT, i)\n",
    "    result = session.execute(sql)\n",
    "    for r in result:\n",
    "        if os.path.exists('outputs/metrics/%s.json' % r['id']):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            xml = etree.fromstring(r['cleaned_content'].encode('utf-8'))\n",
    "        except:\n",
    "            print 'xml fail', r['id']\n",
    "            continue\n",
    "            \n",
    "        metrics = {\n",
    "            \"data_quality\": False,\n",
    "            \"data_quality_bow\": 0,\n",
    "            \"lineage\": False,\n",
    "            \"lineage_bow\": 0,\n",
    "            \"attribute_ref\": False,\n",
    "            \"attribute_bow\": 0,\n",
    "            \"metadata_ref\": False,\n",
    "            \"distribution\": {}\n",
    "        }\n",
    "\n",
    "        if r['protocol'] == 'ISO':\n",
    "            for ename, expath in iso_existences:\n",
    "                metrics[ename] = check_existence(xml, expath)\n",
    "            \n",
    "            # data quality\n",
    "            arr = []\n",
    "            for xp in iso_dq_xpaths:\n",
    "                arr += [d for d in extract(xml, xp)]\n",
    "            \n",
    "            metrics['data_quality_bow'] = len(convert_to_bag(arr))\n",
    "            \n",
    "            # dataqual lineage\n",
    "            arr = []\n",
    "            for xp in iso_lineage_xpaths:\n",
    "                arr += [d for d in extract(xml, xp)]\n",
    "            \n",
    "            metrics['lineage_bow'] = len(convert_to_bag(arr))\n",
    "            \n",
    "#             # eainfo\n",
    "#             arr = []\n",
    "#             for xp in iso_attr_xpaths:\n",
    "#                 arr += [d for d in extract(xml, xp)]\n",
    "            \n",
    "#             metrics['attribute_bow'] = len(convert_to_bag(arr))\n",
    "            \n",
    "            # count the kinds of distribution access points\n",
    "            for dname, dxpath in iso_distributions:\n",
    "                metrics['distribution'][dname] = check_existence(xml, dxpath)\n",
    "                \n",
    "            del metrics['attribute_bow']\n",
    "            del metrics['attribute_ref']\n",
    "\n",
    "        elif r['protocol'] == 'FGDC':\n",
    "            for ename, expath in fgdc_existences:\n",
    "                metrics[ename] = check_existence(xml, expath)\n",
    "            \n",
    "            # data quality\n",
    "            arr = []\n",
    "            for xp in fgdc_dq_xpaths:\n",
    "                arr += [d for d in extract(xml, xp)]\n",
    "            \n",
    "            metrics['data_quality_bow'] = len(convert_to_bag(arr))\n",
    "            \n",
    "            # dataqual lineage\n",
    "            arr = []\n",
    "            for xp in fgdc_lineage_xpaths:\n",
    "                arr += [d for d in extract(xml, xp)]\n",
    "            \n",
    "            metrics['lineage_bow'] = len(convert_to_bag(arr))\n",
    "            \n",
    "            # eainfo\n",
    "            arr = []\n",
    "            for xp in fgdc_attr_xpaths:\n",
    "                arr += [d for d in extract(xml, xp)]\n",
    "            \n",
    "            metrics['attribute_bow'] = len(convert_to_bag(arr))\n",
    "            \n",
    "            # count the kinds of distribution access points\n",
    "            for dname, dxpath in fgdc_distributions:\n",
    "                metrics['distribution'][dname] = check_existence(xml, dxpath)\n",
    "        \n",
    "#         print r['id'], r['source_url']\n",
    "#         print metrics\n",
    "#         print\n",
    "#         print\n",
    "            \n",
    "        with open('outputs/metrics/%s.json' % r['id'], 'w') as g:\n",
    "            g.write(js.dumps(metrics, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the metrics into the rds\n",
    "import glob\n",
    "import json as js\n",
    "import sqlalchemy as sqla\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import (\n",
    "    MetaData,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    "    Boolean,\n",
    "    DateTime,\n",
    ")\n",
    "from sqlalchemy.dialects.postgresql import *\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class Metric(Base):\n",
    "    __tablename__ = 'metadata_age_metrics'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    completeness = Column(JSON)\n",
    "    response_id = Column(Integer)\n",
    "    \n",
    "# load the postgres connection file\n",
    "with open('../local/big_rds.conf', 'r') as f:\n",
    "    conf = js.loads(f.read())\n",
    "\n",
    "# our connection\n",
    "engine = sqla.create_engine(conf.get('connection'))\n",
    "Session = sessionmaker()\n",
    "Session.configure(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "files = glob.glob('outputs/metrics/*.json')\n",
    "for f in files:\n",
    "    response_id = f.split('/')[-1].replace('.json', '')\n",
    "    \n",
    "    if session.query(Metric).filter(Metric.response_id==response_id).count() > 0:\n",
    "        continue\n",
    "    \n",
    "    with open(f, 'r') as g:\n",
    "        data = js.loads(g.read())\n",
    "    \n",
    "    metric = Metric(\n",
    "        response_id=response_id,\n",
    "        completeness=data\n",
    "    )\n",
    "    try:\n",
    "        session.add(metric)\n",
    "        session.commit()\n",
    "    except:\n",
    "        session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unfortunate lapse\n",
    "# let's also grab the URLs/media definitions to\n",
    "# say of the online (only) references, which are\n",
    "# actually externally referencable\n",
    "\n",
    "import json as js\n",
    "import requests\n",
    "from rfc3987 import parse as uparse\n",
    "import sqlalchemy as sqla\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import (\n",
    "    MetaData,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    "    Boolean,\n",
    "    DateTime,\n",
    ")\n",
    "from sqlalchemy.dialects.postgresql import *\n",
    "from datetime import datetime\n",
    "import os\n",
    "from lxml import etree\n",
    "\n",
    "# load the postgres connection file\n",
    "with open('../local/big_rds.conf', 'r') as f:\n",
    "    conf = js.loads(f.read())\n",
    "\n",
    "# our connection\n",
    "engine = sqla.create_engine(conf.get('connection'))\n",
    "Session = sessionmaker()\n",
    "Session.configure(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "sketchy_sql = '''with i\n",
    "as (\n",
    "    select d.response_id, jsonb_array_elements(d.identity::jsonb) ident\n",
    "    from identities d\n",
    "    where d.identity is not null\n",
    ")\n",
    "\n",
    "select r.id, r.source_url, r.source_url_sha, r.cleaned_content, i.ident->'protocol' as protocol\n",
    "from responses r join i on i.response_id = r.id\n",
    "where (i.ident->>'protocol' = 'ISO' or i.ident->>'protocol' = 'FGDC') and r.format = 'xml'\n",
    "limit %s\n",
    "offset %s;\n",
    "'''\n",
    "\n",
    "# 26300 for fgdc\n",
    "# 19700 for ISO\n",
    "\n",
    "LIMIT = 500\n",
    "END = 19700+26300\n",
    "# END = 5\n",
    "# LIMIT=5\n",
    "for i in xrange(0, END, LIMIT):\n",
    "    sql = sketchy_sql % (LIMIT, i)\n",
    "    result = session.execute(sql)\n",
    "    for r in result:\n",
    "        if os.path.exists('outputs/online_refs/%s.json' % r['id']):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            xml = etree.fromstring(r['cleaned_content'].encode('utf-8'))\n",
    "        except Exception as ex:\n",
    "            print 'xml fail', r['id']\n",
    "            continue\n",
    "\n",
    "        if r['protocol'] == 'ISO':\n",
    "            xp = '//*/*[local-name()=\"MD_DigitalTransferOptions\"]/*[local-name()=\"onLine\"]/*[local-name()=\"CI_OnlineResource\"]/*[local-name()=\"linkage\"]/*[local-name()=\"URL\"]'\n",
    "        elif r['protocol'] == 'FGDC':\n",
    "            xp = 'distinfo/stdorder/digform/digtopt/onlinopt/computer/networka/networkr'\n",
    "        \n",
    "        refs = []\n",
    "        elems = xml.xpath(xp)\n",
    "        for elem in elems:\n",
    "            text = elem.text\n",
    "            if not text:\n",
    "                continue\n",
    "            \n",
    "            text = text.strip()\n",
    "            \n",
    "            # is it a valid URL and, you know, we're here so let's \n",
    "            # just make a little HEAD request to ask\n",
    "            ref = {\n",
    "                \"url\": text,\n",
    "                \"checked\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                u = uparse(text, rule='URI')\n",
    "                \n",
    "                if u['scheme'] == 'file':\n",
    "                    ref['error'] = 'file path'\n",
    "                    refs.append(ref)\n",
    "                    continue\n",
    "            except:\n",
    "                # it's not a valid scheme://location/path (http or otherwise)\n",
    "                ref[\"error\"] = 'probable local path'\n",
    "                refs.append(ref)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                rsp = requests.head(text, timeout=30)\n",
    "            except:\n",
    "                ref[\"error\"] = \"HEAD request failed\"\n",
    "                refs.append(ref)\n",
    "                continue\n",
    "            \n",
    "            # just get the status code\n",
    "            ref['status'] = rsp.status_code\n",
    "            \n",
    "            refs.append(ref)\n",
    "            \n",
    "            \n",
    "        with open('outputs/online_refs/%s.json' % r['id'], 'w') as g:\n",
    "            g.write(js.dumps(refs, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'authority': 'www.someinth.com',\n",
       " 'fragment': None,\n",
       " 'path': '/f',\n",
       " 'query': None,\n",
       " 'scheme': 'http'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rfc3987 import parse as uparse\n",
    "\n",
    "# (‘IRI’, \n",
    "#  ‘absolute_IRI’, \n",
    "#  ‘irelative_ref’, \n",
    "#  ‘irelative_part’, \n",
    "#  ‘URI_reference’, \n",
    "#  ‘URI’, \n",
    "#  ‘absolute_URI’, \n",
    "#  ‘relative_ref’, \n",
    "#  ‘relative_part’)\n",
    "\n",
    "uparse('http://www.someinth.com/f', rule='URI')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Second unfortunate lapse. Let's also grab the number of elements per set that contained the text generating the word counts. With just the raw token count, we don't know if the change over time is related to simply more elements, each with less text, or similar number of elements with less text. (Assuming less text, but either way, are those text changes related to structure?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json as js\n",
    "import requests\n",
    "from rfc3987 import parse as uparse\n",
    "import sqlalchemy as sqla\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import (\n",
    "    MetaData,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    "    Boolean,\n",
    "    DateTime,\n",
    ")\n",
    "from sqlalchemy.dialects.postgresql import *\n",
    "from datetime import datetime\n",
    "import os\n",
    "from lxml import etree\n",
    "from mpp.models import Response, Metric\n",
    "from sqlalchemy.orm.attributes import flag_modified\n",
    "\n",
    "# load the postgres connection file\n",
    "with open('../local/big_rds.conf', 'r') as f:\n",
    "    conf = js.loads(f.read())\n",
    "\n",
    "# our connection\n",
    "engine = sqla.create_engine(conf.get('connection'))\n",
    "Session = sessionmaker()\n",
    "Session.configure(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# the two sets won't match but we're using the first item in the key\n",
    "# to match the bag of words part, sum by that option (don't want to here\n",
    "# just in case we need finer-grained anything)\n",
    "fgdc_counts = [\n",
    "    ('lineage_processingsteps', 'count(dataqual/lineage/procstep/procdesc)'),\n",
    "    ('dataqual_attribute_accuracy', 'count(dataqual/attracc/attraccr)'),\n",
    "    ('dataqual_attribute_quality', 'count(dataqual/attracc/qattracc/attracce)'),\n",
    "    ('dataqual_logic', 'count(dataqual/logic)'),\n",
    "    ('dataqual_completeness', 'count(dataqual/complete)'),\n",
    "    ('dataqual_horizontal_position', 'count(dataqual/posacc/horizpa/horizpar)'),\n",
    "    ('dataqual_horizontal_position_quality', 'count(dataqual/posacc/horizpa/qhorizpa/horizpae)'),\n",
    "    ('dataqual_vertical_position', 'count(dataqual/posacc/vertacc/vertaccr)'),\n",
    "    ('dataqual_vertical_position_quality', 'count(dataqual/posacc/vertacc/qhorizpa/vertacce)'),\n",
    "    ('attribute_overview', 'count(eainfo/overview)'),\n",
    "    ('attribute_citation', 'count(eainfo/eadetcit)'),\n",
    "    ('attribute_definitions', 'count(eainfo/detailed/attr/attrdef)'),\n",
    "    ('attribute_labels', 'count(eainfo/detailed/attr/attrlabl)')\n",
    "]\n",
    "\n",
    "# these are going to be ugly\n",
    "iso_counts = [\n",
    "    ('lineage_processingsteps', 'count(*/*[local-name()=\"dataQualityInfo\"]/*[local-name()=\"DQ_DataQuality\"]/*[local-name()=\"lineage\"]/*[local-name()=\"LI_Lineage\"]/*[local-name()=\"processStep\"]/*[local-name()=\"LI_ProcessStep\"]/*[local-name()=\"description\"]/*[local-name()=\"CharacterString\"])'),\n",
    "    ('dataqual_completenessomission', 'count(*/*[local-name()=\"dataQualityInfo\"]/*[local-name()=\"DQ_DataQuality\"]/*[local-name()=\"report\"]/*[local-name()=\"DQ_CompletenessOmission\"]/*[local-name()=\"evaluationMethodDescription\"]/*[local-name()=\"CharacterString\"])'),\n",
    "    ('dataqual_completenesscomission', 'count(*/*[local-name()=\"dataQualityInfo\"]/*[local-name()=\"DQ_DataQuality\"]/*[local-name()=\"report\"]/*[local-name()=\"DQ_CompletenessComission\"]/*[local-name()=\"evaluationMethodDescription\"]/*[local-name()=\"CharacterString\"])'),\n",
    "    ('dataqual_consistency', 'count(*/*[local-name()=\"dataQualityInfo\"]/*[local-name()=\"DQ_DataQuality\"]/*[local-name()=\"report\"]/*[local-name()=\"DQ_ConceptualConsistency\"]/*[local-name()=\"methodDescription\"]/*[local-name()=\"CharacterString\"])')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 26300 for fgdc\n",
    "# 19700 for ISO\n",
    "\n",
    "LIMIT = 500\n",
    "END = 45970\n",
    "# END = 5\n",
    "# LIMIT=5\n",
    "for i in xrange(0, END, LIMIT):\n",
    "    for metric in session.query(Metric).limit(LIMIT).offset(i).all():\n",
    "        \n",
    "        # didn't add the relate in sqla models, apologies\n",
    "        r = session.query(Response).filter(Response.id==metric.response_id).first()\n",
    "        if not r:\n",
    "            print 'failed join: ', metric.response_id\n",
    "            continue\n",
    "        try:\n",
    "            xml = etree.fromstring(r.cleaned_content.encode('utf-8'))\n",
    "        except Exception as ex:\n",
    "            print 'xml fail', r.id\n",
    "            continue\n",
    "            \n",
    "        protocol = next(iter([x.get('protocol') for x in r.identities[0].identity]), '')\n",
    "        \n",
    "        element_counts = {}\n",
    "        if protocol == 'FGDC':\n",
    "            count_xpaths = fgdc_counts\n",
    "        elif protocol == 'ISO':\n",
    "            count_xpaths = iso_counts\n",
    "            \n",
    "        for key, xp in count_xpaths:\n",
    "            element_counts[key] = xml.xpath(xp)\n",
    "            \n",
    "        metric.completeness.update({\"element_counts\": element_counts})   \n",
    "        flag_modified(metric, \"completeness\")\n",
    "        \n",
    "#         print r.source_url\n",
    "#         print metric.completeness\n",
    "#         print\n",
    "        try:\n",
    "            session.commit()\n",
    "        except Exception as ex:\n",
    "            print ex\n",
    "            session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
