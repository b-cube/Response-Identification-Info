{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Duplicate Detection\n",
    "\n",
    "These are demonstrations to highlight the issues with common document similarity/duplication detection methods. It will cover two main issues:\n",
    "\n",
    "1. Using TF-IDF (or any vector option) to determine metadata similarity. \n",
    "2. Using vector-based approaches to identify similar identifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Metadata Similarity\n",
    "\n",
    "TF-IDF is commonly used for determining duplicate content on websites. It has been presented as the solution for determining duplicate metadata content. However, work demonstrating its utility in these kinds of tasks are generally limited to a single metadata specification and/or metadata generated from a single system. This is not a realistic approach as we deal with metadata across different kinds of federated platforms.\n",
    "\n",
    "For this demonstration, we have three sets of metadata - from three data portals, we've collected multiple representations of the same dataset. So for one dataset, for example, we have a DIF record, an FGDC record and an ISO record. We are assuming that much of the text within those documents is the same and that the majority of the difference is related to the differences between the standards.\n",
    "\n",
    "So these are sets of metadata we know describe one dataset (each set). Using common tools, can we identify those automatically?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import os\n",
    "from lxml import etree\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import metrics\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = etree.XMLParser(\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "def find_nodes(xml):\n",
    "    nodes = []\n",
    "    for elem in xml.iter():\n",
    "        t = elem.text.strip() if elem.text else ''\n",
    "        tags = [elem.tag] + [e.tag for e in elem.iterancestors()]\n",
    "        tags.reverse()\n",
    "\n",
    "        att_texts = parse_node_attributes(elem)\n",
    "\n",
    "        nodes += [a for a in [t] + att_texts if a]\n",
    "  \n",
    "    return nodes\n",
    "\n",
    "def parse_node_attributes(node):\n",
    "    if not node.attrib:\n",
    "        return []\n",
    "    return node.attrib.values() if node.attrib else []\n",
    "\n",
    "def strip_punctuation(text, simple_pattern=r'[;|>+:=<?(){}`\\'\"]', replace_char=' '):\n",
    "    text = re.sub(simple_pattern, replace_char, text)\n",
    "    return text.replace(\"/\", ' ')\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    return ' '.join([w for w in words if w not in _stopwords and w])\n",
    "\n",
    "# bag of words parsing\n",
    "# we'll drop stopwords but nothing else\n",
    "def get_bag(text):\n",
    "    try:\n",
    "        xml = etree.fromstring(text, parser=parser)\n",
    "    except Exception as ex:\n",
    "        print ex\n",
    "        xml = None\n",
    "        \n",
    "    if xml is None:\n",
    "        print 'failed to parse'\n",
    "        return ''\n",
    "    \n",
    "    nodes = find_nodes(xml)\n",
    "    content = ' '.join(nodes if nodes else [])\n",
    "    if not content:\n",
    "        print 'failed to iterate'\n",
    "        return ''\n",
    "\n",
    "    content = strip_punctuation(content)\n",
    "    words = tokenize(content)\n",
    "    bag = remove_stopwords(words)\n",
    "    bag = strip_punctuation(bag, r'[.,]', '')\n",
    "    return bag\n",
    "\n",
    "def prep_set(files):\n",
    "    identifiers = []\n",
    "    bags = []\n",
    "    for f in files:\n",
    "        bag = ''\n",
    "        with open(f, 'r') as g:\n",
    "            text = g.read()\n",
    "            \n",
    "        identifier = os.path.basename(f).split('_')[-1].replace('.xml', '')\n",
    "        bag = get_bag(text)\n",
    "        \n",
    "        bags.append(bag)\n",
    "        identifiers.append(identifier)\n",
    "    \n",
    "    return identifiers, bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf-idf set up\n",
    "def similarity(all_identifiers, all_bags):\n",
    "    '''\n",
    "    run through the tf-idf calcs using each bag as the initial comparator\n",
    "    the scores vary based on that first object\n",
    "    '''\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # store the sets\n",
    "    similarity_scores = {}\n",
    "    for i in xrange(len(all_identifiers)):\n",
    "        tf_identifiers = [all_identifiers[i]] + all_identifiers\n",
    "        tf_data = [all_bags[i]] + all_bags\n",
    "\n",
    "        tfidf_matrix_trainer = tfidf_vectorizer.fit_transform(tf_data)\n",
    "\n",
    "        cos_sim = cosine_similarity(tfidf_matrix_trainer[0:1], tfidf_matrix_trainer)\n",
    "\n",
    "        lk = linear_kernel(tfidf_matrix_trainer[0:1], tfidf_matrix_trainer).flatten()\n",
    "\n",
    "        related_indices = cos_sim.argsort()[:-len(tf_data)-1:-1] \n",
    "        related_indices_with_lk_value = [r for r in related_indices[0] if lk[r]]\n",
    "        related_indices_with_lk_value.reverse()\n",
    "\n",
    "        related_set = [(lk[k], tf_data[k], tf_identifiers[k]) for k in related_indices_with_lk_value]\n",
    "\n",
    "        similarity_scores[all_identifiers[i]] = [\n",
    "            (similarity, identifier) for similarity, bag, identifier in related_set[1:]\n",
    "        ]\n",
    "    \n",
    "    return similarity_scores\n",
    "\n",
    "def print_matrix(scores):\n",
    "    keys = scores.keys()\n",
    "    keys.sort()\n",
    "    \n",
    "    print '|'.join([' '*10] + ['{:^11s}'.format(k) for k in keys])\n",
    "    print '-' * ((len(keys) * 11) + len(keys) + 10)\n",
    "    \n",
    "    for k in keys:\n",
    "        vert = scores[k]\n",
    "        vert_vals = ['{:^10s}'.format(k)]\n",
    "        for e in keys:\n",
    "            val = next(iter([v[0] for v in vert if v[1] == e]), -99)\n",
    "            vert_vals.append('{:^10.2f} '.format(val))\n",
    "        \n",
    "        print '|'.join(vert_vals)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          |    dc     |    dif    |    iso    \n",
      "----------------------------------------------\n",
      "    dc    |   1.00    |   0.56    |   0.27    \n",
      "   dif    |   0.59    |   1.00    |   0.27    \n",
      "   iso    |   0.32    |   0.30    |   1.00    \n"
     ]
    }
   ],
   "source": [
    "# start with medin\n",
    "medins = glob.glob('inputs/medin/*.xml')\n",
    "identifiers, bags = prep_set(medins)\n",
    "medin_scores = similarity(identifiers, bags)\n",
    "print_matrix(medin_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          |   fgdc    |    iso    |    wfs    |    wms    | wms19119  \n",
      "----------------------------------------------------------------------\n",
      "   fgdc   |   1.00    |   0.70    |   0.44    |   0.45    |   0.09    \n",
      "   iso    |   0.73    |   1.00    |   0.37    |   0.38    |   0.19    \n",
      "   wfs    |   0.42    |   0.35    |   1.00    |   0.71    |   0.08    \n",
      "   wms    |   0.44    |   0.37    |   0.72    |   1.00    |   0.09    \n",
      " wms19119 |   0.11    |   0.22    |   0.09    |   0.10    |   1.00    \n"
     ]
    }
   ],
   "source": [
    "# now gstore\n",
    "gstores = glob.glob('inputs/gstore/*.xml')\n",
    "identifiers, bags = prep_set(gstores)\n",
    "gstore_scores = similarity(identifiers, bags)\n",
    "print_matrix(gstore_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          |   atom    |    csw    |    dif    |   fgdc    |    iso    \n",
      "----------------------------------------------------------------------\n",
      "   atom   |   1.00    |   0.89    |   0.77    |   0.86    |   0.28    \n",
      "   csw    |   0.88    |   1.00    |   0.80    |   0.92    |   0.26    \n",
      "   dif    |   0.79    |   0.82    |   1.00    |   0.84    |   0.24    \n",
      "   fgdc   |   0.85    |   0.91    |   0.81    |   1.00    |   0.26    \n",
      "   iso    |   0.32    |   0.30    |   0.28    |   0.30    |   1.00    \n"
     ]
    }
   ],
   "source": [
    "# and finally devotes\n",
    "devotes = glob.glob('inputs/devotes/*.xml')\n",
    "identifiers, bags = prep_set(devotes)\n",
    "devotes_scores = similarity(identifiers, bags)\n",
    "print_matrix(devotes_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Outcome\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Identifier Similarity\n",
    "\n",
    "For this we're relying on the kinds of non-cryptographic hashing methods common in crawling projects.\n",
    "\n",
    "**Simhashes**\n",
    "\n",
    "These are a kind of non-cryptographic hash described by Google for performant similarity checks at scale during web crawling activities. \n",
    "\n",
    "Two methods - a simple demonstration of simhashes with unmondified identifiers and The Daniel Method (splitting, sorting, concatenating, simhashing).\n",
    "\n",
    "Refs:\n",
    "\n",
    "https://liangsun.org/posts/a-python-implementation-of-simhash-algorithm/\n",
    "\n",
    "https://github.com/liangsun/simhash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from simhash import Simhash\n",
    "from itertools import chain\n",
    "from operator import itemgetter\n",
    "import collections\n",
    "\n",
    "# a shallow fork of the index - we want to return\n",
    "# the scores for demonstration purposes\n",
    "\n",
    "class Indexer(object):\n",
    "    def __init__(self, f=64, k=2):\n",
    "        self.bucket = collections.defaultdict(set)\n",
    "        self.k = k\n",
    "        self.f = f\n",
    "\n",
    "    def get_near_dups(self, simhash):\n",
    "        \"\"\"\n",
    "        `simhash` is an instance of Simhash\n",
    "        return a list of obj_id (pipe-delimited string of sha|text|distance)\n",
    "        \"\"\"\n",
    "        assert simhash.f == self.f\n",
    "\n",
    "        ans = set()\n",
    "\n",
    "        for key in self.get_keys(simhash):\n",
    "            dups = self.bucket.get(key, set())\n",
    "\n",
    "            for dup in dups:\n",
    "                sim2, obj_blob = dup.split(',', 1)\n",
    "                sim2 = Simhash(long(sim2, 16), self.f)\n",
    "\n",
    "                d = simhash.distance(sim2)\n",
    "                if d <= self.k:\n",
    "                    ans.add('{0}|{1}'.format(obj_blob, d))\n",
    "        return list(ans)\n",
    "    \n",
    "    def add(self, obj_id, obj_str, simhash):\n",
    "        \"\"\"\n",
    "        `obj_id` is a string\n",
    "        `simhash` is an instance of Simhash\n",
    "        \"\"\"\n",
    "        assert simhash.f == self.f\n",
    "\n",
    "        for key in self.get_keys(simhash):\n",
    "            v = '%x,%s|%s' % (simhash.value, obj_id, obj_str)\n",
    "\n",
    "            self.bucket.setdefault(key, set())\n",
    "            self.bucket[key].add(v)\n",
    "\n",
    "    def delete(self, obj_id, obj_str, simhash):\n",
    "        \"\"\"\n",
    "        `obj_id` is a string\n",
    "        `simhash` is an instance of Simhash\n",
    "        \"\"\"\n",
    "        assert simhash.f == self.f\n",
    "\n",
    "        for key in self.get_keys(simhash):\n",
    "            v = '%x,%s|%s' % (simhash.value, obj_id, obj_str)\n",
    "\n",
    "            if v in self.bucket.get(key, set()):\n",
    "                self.bucket[key].remove(v)\n",
    "    \n",
    "    @property\n",
    "    def offsets(self):\n",
    "        \"\"\"\n",
    "        You may optimize this method according to <http://www.wwwconference.org/www2007/papers/paper215.pdf>\n",
    "        \"\"\"\n",
    "        return [self.f // (self.k + 1) * i for i in range(self.k + 1)]\n",
    "\n",
    "    def get_keys(self, simhash):\n",
    "        for i, offset in enumerate(self.offsets):\n",
    "            m = (i == len(self.offsets) - 1 and 2 ** (self.f - offset) - 1 or 2 ** (self.offsets[i + 1] - offset) - 1)\n",
    "            c = simhash.value >> offset & m\n",
    "            yield '%x:%x' % (c, i)\n",
    "\n",
    "    def bucket_size(self):\n",
    "        return len(self.bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_set(test_set, k=20):\n",
    "    duplicates = {}\n",
    "    index = Indexer(k=k)\n",
    "    # build the index\n",
    "    for test_id, test_item, test_simhash in test_set:\n",
    "        index.add(test_id, test_item, test_simhash)\n",
    "        \n",
    "    # run per test string\n",
    "    for test_id, test_item, test_simhash in test_set:\n",
    "        dupes = index.get_near_dups(test_simhash)\n",
    "        \n",
    "        # as id, string, score\n",
    "        duplicates[test_item] = [d.split('|') for d in dupes]\n",
    "    return duplicates\n",
    "\n",
    "def print_dupes(dupes):\n",
    "    for key, v in dupes.iteritems():\n",
    "        print key\n",
    "        \n",
    "        for i, t, s in v:\n",
    "            if i == key or int(s) == 0:\n",
    "                continue\n",
    "            print '\\t{0} : {1}'.format(i, s)\n",
    "        \n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our set of identifiers\n",
    "\n",
    "# dois\n",
    "dois = [\n",
    "    'http://dx.doi.org/10.7916/D85B019G',\n",
    "    '10.7916/D85B019G',\n",
    "    'doi:10.7916/D85B019G'\n",
    "]\n",
    "\n",
    "\n",
    "# some mnemonic things (one or two characters difference only)\n",
    "mnemonics = []\n",
    "\n",
    "\n",
    "# thredds things (vector size issues )\n",
    "thredds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://dx.doi.org/10.7916/D85B019G\n",
      "\t10.7916/D85B019G : 19\n",
      "\n",
      "doi:10.7916/D85B019G\n",
      "\t10.7916/D85B019G : 16\n",
      "\n",
      "10.7916/D85B019G\n",
      "\thttp://dx.doi.org/10.7916/D85B019G : 19\n",
      "\tdoi:10.7916/D85B019G : 16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doi_index = [(d, d, Simhash(d)) for d in dois]\n",
    "dupes = evaluate_set(doi_index)\n",
    "print_dupes(dupes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.7916D85B019G\n",
      "\thttp://dx.doi.org/10.7916/D85B019G : 13\n",
      "\n",
      "D85B019Gdoi:10.7916\n",
      "\thttp://dx.doi.org/10.7916/D85B019G : 17\n",
      "\n",
      "10.7916D85B019Gdx.doi.orghttp:\n",
      "\t10.7916/D85B019G : 13\n",
      "\tdoi:10.7916/D85B019G : 17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the daniel method:\n",
    "# split strinf\n",
    "# sort\n",
    "# concatenate\n",
    "# score\n",
    "\n",
    "sorted_dois = []\n",
    "for d in dois:\n",
    "    x = d.split('/')\n",
    "    x.sort()\n",
    "    \n",
    "    sorted_dois.append((d, ''.join(x)))\n",
    "    \n",
    "sorted_dois\n",
    "doi_index = [(i, d, Simhash(d)) for i, d in sorted_dois]\n",
    "dupes = evaluate_set(doi_index)\n",
    "print_dupes(dupes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
