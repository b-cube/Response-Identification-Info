{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Duplicate Detection\n",
    "\n",
    "These are demonstrations to highlight the issues with common document similarity/duplication detection methods. It will cover two main issues:\n",
    "\n",
    "1. Using TF-IDF (or any vector option) to determine metadata similarity. \n",
    "2. Using vector-based approaches to identify similar identifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Metadata Similarity\n",
    "\n",
    "TF-IDF is commonly used for determining duplicate content on websites. It has been presented as the solution for determining duplicate metadata content. However, work demonstrating its utility in these kinds of tasks are generally limited to a single metadata specification and/or metadata generated from a single system. This is not a realistic approach as we deal with metadata across different kinds of federated platforms.\n",
    "\n",
    "For this demonstration, we have three sets of metadata - from three data portals, we've collected multiple representations of the same dataset. So for one dataset, for example, we have a DIF record, an FGDC record and an ISO record. We are assuming that much of the text within those documents is the same and that the majority of the difference is related to the differences between the standards.\n",
    "\n",
    "So these are sets of metadata we know describe one dataset (each set). Using common tools, can we identify those automatically?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import os\n",
    "from lxml import etree\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import metrics\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = etree.XMLParser(\n",
    "    encoding='utf-8'\n",
    ")\n",
    "\n",
    "def find_nodes(xml):\n",
    "    nodes = []\n",
    "    for elem in xml.iter():\n",
    "        t = elem.text.strip() if elem.text else ''\n",
    "        tags = [elem.tag] + [e.tag for e in elem.iterancestors()]\n",
    "        tags.reverse()\n",
    "\n",
    "        att_texts = parse_node_attributes(elem)\n",
    "\n",
    "        nodes += [a for a in [t] + att_texts if a]\n",
    "  \n",
    "    return nodes\n",
    "\n",
    "def parse_node_attributes(node):\n",
    "    if not node.attrib:\n",
    "        return []\n",
    "    return node.attrib.values() if node.attrib else []\n",
    "\n",
    "def strip_punctuation(text, simple_pattern=r'[;|>+:=<?(){}`\\'\"]', replace_char=' '):\n",
    "    text = re.sub(simple_pattern, replace_char, text)\n",
    "    return text.replace(\"/\", ' ')\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    return ' '.join([w for w in words if w not in _stopwords and w])\n",
    "\n",
    "# bag of words parsing\n",
    "# we'll drop stopwords but nothing else\n",
    "def get_bag(text):\n",
    "    try:\n",
    "        xml = etree.fromstring(text, parser=parser)\n",
    "    except Exception as ex:\n",
    "        print ex\n",
    "        xml = None\n",
    "        \n",
    "    if xml is None:\n",
    "        print 'failed to parse'\n",
    "        return ''\n",
    "    \n",
    "    nodes = find_nodes(xml)\n",
    "    content = ' '.join(nodes if nodes else [])\n",
    "    if not content:\n",
    "        print 'failed to iterate'\n",
    "        return ''\n",
    "\n",
    "    content = strip_punctuation(content)\n",
    "    words = tokenize(content)\n",
    "    bag = remove_stopwords(words)\n",
    "    bag = strip_punctuation(bag, r'[.,]', '')\n",
    "    return bag\n",
    "\n",
    "def prep_set(files):\n",
    "    identifiers = []\n",
    "    bags = []\n",
    "    for f in files:\n",
    "        bag = ''\n",
    "        with open(f, 'r') as g:\n",
    "            text = g.read()\n",
    "            \n",
    "        identifier = os.path.basename(f).split('_')[-1].replace('.xml', '')\n",
    "        bag = get_bag(text)\n",
    "        \n",
    "        bags.append(bag)\n",
    "        identifiers.append(identifier)\n",
    "    \n",
    "    return identifiers, bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf-idf set up\n",
    "def similarity(all_identifiers, all_bags):\n",
    "    '''\n",
    "    run through the tf-idf calcs using each bag as the initial comparator\n",
    "    the scores vary based on that first object\n",
    "    '''\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # store the sets\n",
    "    similarity_scores = {}\n",
    "    for i in xrange(len(all_identifiers)):\n",
    "        tf_identifiers = [all_identifiers[i]] + all_identifiers\n",
    "        tf_data = [all_bags[i]] + all_bags\n",
    "\n",
    "        tfidf_matrix_trainer = tfidf_vectorizer.fit_transform(tf_data)\n",
    "\n",
    "        cos_sim = cosine_similarity(tfidf_matrix_trainer[0:1], tfidf_matrix_trainer)\n",
    "\n",
    "        lk = linear_kernel(tfidf_matrix_trainer[0:1], tfidf_matrix_trainer).flatten()\n",
    "\n",
    "        related_indices = cos_sim.argsort()[:-len(tf_data)-1:-1] \n",
    "        related_indices_with_lk_value = [r for r in related_indices[0] if lk[r]]\n",
    "        related_indices_with_lk_value.reverse()\n",
    "\n",
    "        related_set = [(lk[k], tf_data[k], tf_identifiers[k]) for k in related_indices_with_lk_value]\n",
    "\n",
    "        similarity_scores[all_identifiers[i]] = [\n",
    "            (similarity, identifier) for similarity, bag, identifier in related_set[1:]\n",
    "        ]\n",
    "    \n",
    "    return similarity_scores\n",
    "\n",
    "def print_matrix(scores):\n",
    "    keys = scores.keys()\n",
    "    keys.sort()\n",
    "    \n",
    "    print '|'.join([' '*10] + ['{:^11s}'.format(k) for k in keys])\n",
    "    print '-' * ((len(keys) * 11) + len(keys) + 10)\n",
    "    \n",
    "    for k in keys:\n",
    "        vert = scores[k]\n",
    "        vert_vals = ['{:^10s}'.format(k)]\n",
    "        for e in keys:\n",
    "            val = next(iter([v[0] for v in vert if v[1] == e]), -99)\n",
    "            vert_vals.append('{:^10.2f} '.format(val))\n",
    "        \n",
    "        print '|'.join(vert_vals)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          |    dc     |    dif    |    iso    \n",
      "----------------------------------------------\n",
      "    dc    |   1.00    |   0.56    |   0.27    \n",
      "   dif    |   0.59    |   1.00    |   0.27    \n",
      "   iso    |   0.32    |   0.30    |   1.00    \n"
     ]
    }
   ],
   "source": [
    "# start with medin\n",
    "medins = glob.glob('inputs/medin/*.xml')\n",
    "identifiers, bags = prep_set(medins)\n",
    "medin_scores = similarity(identifiers, bags)\n",
    "print_matrix(medin_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dc\n",
      "http wwwopenarchivesorg OAI 20 oai_dc http wwwopenarchivesorg OAI 20 oai_dcxsd British Geological Survey BGS Geophysical Survey 1995 2 The Wash 25 07 1995 01 08 1995 dataset bgsnercacuk DC abc9f747-537c-0f38-e044-0003ba9b0d98 This British Geological Survey BGS survey took place July August 1995 Wash board Greyhound Tracker  Technical details contained BGS Report WB 95 36  Report Brett  CP  1995  LOEPS Shallow Seismic Survey  Wash  N Norfolk  Humber Operations Report Project 95 02  1995-08-01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "dif\n",
      "http gcmdgsfcnasagov Aboutus xml dif http gcmdgsfcnasagov Aboutus xml dif dif_v94xsd bgsnercacuk DIF BGS_CMD_REF272 British Geological Survey BGS Geophysical Survey 1995 2 The Wash 25 07 1995 01 08 1995 NDGO0001 Geology soil sediment crust Bathymetry Elevation Two-dimensional seismic reflection Side-scan sonar geoscientificInformation NDGO0001 Geology soil sediment crust Bathymetry Elevation Two-dimensional seismic reflection Side-scan sonar NOTE ! This GCMD DIF produced automatically ISO19139 MEDIN v23 profile  Not MEDIN info reproduced - conversion intended provide basic information reproduced NDG MEDIN discovery service 1995-07-25 1995-08-01 528801 536478 00840 12011 British Geological Survey BGS custodian British Geological Survey BGS originator British Geological Survey BGS distributor This British Geological Survey BGS survey took place July August 1995 Wash board Greyhound Tracker  Technical details contained BGS Report WB 95 36  Report Brett  CP  1995  LOEPS Shallow Seismic Survey  Wash  N Norfolk  Humber Operations Report Project 95 02 http wwwbgsacuk GeoIndex offshorehtm 94 [ CEOS IDN DIF ] 1995-08-01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "iso\n",
      "http wwwisotc211org 2005 gco http edenignfr xsd isotc211 isofull 20090316 gco gcoxsd http wwwisotc211org 2005 gmd http edenignfr xsd isotc211 isofull 20090316 gmd gmdxsd http wwwopengisnet gml 32 http edenignfr xsd isotc211 isofull 20090316 gml gmlxsd http wwwisotc211org 2005 gmx http edenignfr xsd isotc211 isofull 20090316 gmx gmxxsd http wwwisotc211org 2005 gsr http edenignfr xsd isotc211 isofull 20090316 gsr gsrxsd http wwwisotc211org 2005 gss http edenignfr xsd isotc211 isofull 20090316 gss gssxsd http wwwisotc211org 2005 gts http edenignfr xsd isotc211 isofull 20090316 gts gtsxsd http wwwisotc211org 2005 srv http edenignfr xsd isotc211 isofull 20090316 srv srvxsd abc9f747-537c-0f38-e044-0003ba9b0d98 English http wwwlocgov standards iso639-2 php code_listphp eng dataset http standardsisoorg ittf PubliclyAvailableStandards ISO_19139_Schemas resources codelist gmxCodelistsxml # MD_ScopeCode dataset British Geological Survey BGS 44 0 131 667 1000 44 0 131 668 4140 Murchison House  West Mains Road Edinburgh EH9 3LA UK enquiries @ bgsacuk pointOfContact http standardsisoorg ittf PubliclyAvailableStandards ISO_19139_Schemas resources codelist gmxCodelistsxml # CI_RoleCode pointOfContact 2015-01-12 MEDIN Discovery Metadata Standard Version 235 wwwepsgorg 2005 revision http standardsisoorg ittf PubliclyAvailableStandards ISO_19139_Schemas resources codelist gmxCodelistsxml # CI_DateTypeCode revision urn ogc def crs EPSG 4326 OGP British Geological Survey BGS Geophysical Survey 1995 2 The Wash 25 07 1995 01 08 1995 1995-08-01 publication http standardsisoorg ittf PubliclyAvailableStandards ISO_19139_Schemas resources codelist gmxCodelistsxml # CI_DateTypeCode publication BGS_CMD_REF272 British Geological Survey This British Geological Survey BGS survey took place July August 1995 Wash board Greyhound Tracker  Technical details contained BGS Report WB 95 36  Report Brett  CP  1995  LOEPS Shallow Seismic Survey  Wash  N Norfolk  Humber Operations Report Project 95 02  British Geological Survey BGS 44 0 131 667 1000 44 0 131 668 4140 Murchison House  West Mains Road Edinburgh EH9 3LA UK enquiries @ bgsacuk custodian http standardsisoorg ittf PubliclyAvailableStandards ISO_19139_Schemas resources codelist gmxCodelistsxml # CI_RoleCode custodian British Geological Survey BGS 44 0 131 667 1000 44 0 131 668 4140 Murchison House  West Mains Road Edinburgh EH9 3LA UK offshore @ bgsacuk originator http standardsisoorg ittf PubliclyAvailableStandards ISO_19139_Schemas resources codelist gmxCodelistsxml # CI_RoleCode originator notPlanned http standardsisoorg ittf PubliclyAvailableStandards ISO_19139_Schemas resources codelist gmxCodelistsxml # MD_MaintenanceFrequencyCode notPlanned NDGO0001 http vocabnercacuk collection N01 current NDGO0001 NERC OAI Harvesting Geology http wwweioneteuropaeu gemet concept cp 13 & langcode en & ns 5 Geology GEMET - INSPIRE themes 2008-06-01 publication http standardsisoorg ittf PubliclyAvailableStandards ISO_19139_Schemas resources codelist gmxCodelistsxml # CI_DateTypeCode publication soil sediment http vocabnercacuk collection L13 current SD soil sediment crust http vocabnercacuk collection L13 current CR crust SeaVoX Vertical Co-ordinate Coverages L131 2010-05-18 revision http standardsisoorg ittf PubliclyAvailableStandards ISO_19139_Schemas resources codelist gmxCodelistsxml # CI_DateTypeCode revision Bathymetry Elevation http vocabnercacuk collection P02 current MBAN Bathymetry Elevation Two-dimensional seismic reflection http vocabnercacuk collection P02 current SR2D Two-dimensional seismic reflection Side-scan sonar http vocabnercacuk collection P02 current SSCN Side-scan sonar SeaDataNet Parameter Discovery Vocabulary P021 2011-07-27 revision http standardsisoorg ittf PubliclyAvailableStandards ISO_19139_Schemas resources codelist gmxCodelistsxml # CI_DateTypeCode revision The data free academic use  though conditions may apply  Charges cover reasonable staff time apply commercial usage  otherRestrictions http standardsisoorg ittf PubliclyAvailableStandards ISO_19139_Schemas resources codelist gmxCodelistsxml # MD_RestrictionCode otherRestrictions limitation listed 5 urn ogc def uom EPSG 9001 gml LengthType English http wwwlocgov standards iso639-2 php code_listphp eng geoscientificInformation BGS Gazetteer 2015-01-12 revision http standardsisoorg ittf PubliclyAvailableStandards ISO_19139_Schemas resources codelist gmxCodelistsxml # CI_DateTypeCode revision East Anglia BGS Gazetteer 2015-01-12 revision http standardsisoorg ittf PubliclyAvailableStandards ISO_19139_Schemas resources codelist gmxCodelistsxml # CI_DateTypeCode revision Spurn Charting Progress 2 Regional Sea Boundaries 2009-06-18 creation http standardsisoorg ittf PubliclyAvailableStandards ISO_19139_Schemas resources codelist gmxCodelistsxml # CI_DateTypeCode creation Southern North Sea The SeaVoX Salt Fresh Water Body Gazetteer Sub-Ocean Category 1954-01-01 creation http standardsisoorg ittf PubliclyAvailableStandards ISO_19139_Schemas resources codelist gmxCodelistsxml # CI_DateTypeCode creation NORTH SEA 00840 12011 528801 536478 ID1 1995-07-25 unknown 1995-08-01 unknown inapplicable British Geological Survey BGS 44 0 131 667 1000 44 0 131 668 4140 Murchison House  West Mains Road Edinburgh EH9 3LA UK enquiries @ bgsacuk distributor http standardsisoorg ittf PubliclyAvailableStandards ISO_19139_Schemas resources codelist gmxCodelistsxml # CI_RoleCode distributor http wwwbgsacuk GeoIndex offshorehtm HTTP BGS Offshore Geoindex GeoIndex allows users search information BGS data collections covering UK areas world wide  Access free  interface easy use  developed enable users check coverage different types data find background information data  More detailed information obtained enquiry website  search http standardsisoorg ittf PubliclyAvailableStandards ISO_19139_Schemas resources codelist gmxCodelistsxml # CI_OnLineFunctionCode search dataset http standardsisoorg ittf PubliclyAvailableStandards ISO_19139_Schemas resources codelist gmxCodelistsxml # MD_ScopeCode dataset INSPIRE Implementing rules laying technical arrangements interoperability harmonisation unknown theme 2011 publication http standardsisoorg ittf PubliclyAvailableStandards ISO_19139_Schemas resources codelist gmxCodelistsxml # CI_DateTypeCode publication See referenced specification false This dataset collected Geophysical Survey 1995 2 The Wash 25 07 1995 01 08 1995 programme work This British Geological Survey BGS survey took place July August 1995 Wash board Greyhound Tracker  Technical details contained BGS Report WB 95 36  Report Brett  CP  1995  LOEPS Shallow Seismic Survey  Wash  N Norfolk  Humber Operations Report Project 95 02  Geophysical equipment types used included Echo Sounder  Sidescan Sonar Surface Tow Boomer  The survey undertaken British Geological Survey  For detailed information acquisition equipment data collection techniques  operational standards  data processing methods quality control procedures used survey see Report Survey Cruise Report associated documentation available  The information available may vary depending age survey  Data checked loaded BGS Coastal Marine data management system following BGS marine data management procedures \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the medin bags of words, just to show\n",
    "for i, b in enumerate(bags):\n",
    "    f = identifiers[i]\n",
    "    print f\n",
    "    print b\n",
    "    print '-'* 100\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          |   fgdc    |    iso    |    wfs    |    wms    | wms19119  \n",
      "----------------------------------------------------------------------\n",
      "   fgdc   |   1.00    |   0.70    |   0.44    |   0.45    |   0.09    \n",
      "   iso    |   0.73    |   1.00    |   0.37    |   0.38    |   0.19    \n",
      "   wfs    |   0.42    |   0.35    |   1.00    |   0.71    |   0.08    \n",
      "   wms    |   0.44    |   0.37    |   0.72    |   1.00    |   0.09    \n",
      " wms19119 |   0.11    |   0.22    |   0.09    |   0.10    |   1.00    \n"
     ]
    }
   ],
   "source": [
    "# now gstore\n",
    "gstores = glob.glob('inputs/gstore/*.xml')\n",
    "identifiers, bags = prep_set(gstores)\n",
    "gstore_scores = similarity(identifiers, bags)\n",
    "print_matrix(gstore_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          |   atom    |    csw    |    dif    |   fgdc    |    iso    \n",
      "----------------------------------------------------------------------\n",
      "   atom   |   1.00    |   0.89    |   0.77    |   0.86    |   0.28    \n",
      "   csw    |   0.88    |   1.00    |   0.80    |   0.92    |   0.26    \n",
      "   dif    |   0.79    |   0.82    |   1.00    |   0.84    |   0.24    \n",
      "   fgdc   |   0.85    |   0.91    |   0.81    |   1.00    |   0.26    \n",
      "   iso    |   0.32    |   0.30    |   0.28    |   0.30    |   1.00    \n"
     ]
    }
   ],
   "source": [
    "# and finally devotes\n",
    "devotes = glob.glob('inputs/devotes/*.xml')\n",
    "identifiers, bags = prep_set(devotes)\n",
    "devotes_scores = similarity(identifiers, bags)\n",
    "print_matrix(devotes_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devotes-80c47f28-d0bc-11e3-9261-00163c43a2bd_iso.xml\n",
      "\t0.259552891407: medin-abc9f747-537c-0f38-e044-0003ba9b0d98_iso.xml\n",
      "\t0.235627369434: gstore-b4ae8f53-8dff-46bb-9058-e5501cabdd1b_iso.xml\n",
      "medin-abc9f747-537c-0f38-e044-0003ba9b0d98_iso.xml\n",
      "\t0.267971178365: devotes-80c47f28-d0bc-11e3-9261-00163c43a2bd_iso.xml\n",
      "\t0.225254351844: gstore-b4ae8f53-8dff-46bb-9058-e5501cabdd1b_iso.xml\n",
      "gstore-b4ae8f53-8dff-46bb-9058-e5501cabdd1b_iso.xml\n",
      "\t0.24774153906: devotes-80c47f28-d0bc-11e3-9261-00163c43a2bd_iso.xml\n",
      "\t0.227559645423: medin-abc9f747-537c-0f38-e044-0003ba9b0d98_iso.xml\n"
     ]
    }
   ],
   "source": [
    "# because i am curious and we've got a tiny set here.\n",
    "# let's just compare the three iso records and see those scores\n",
    "# the text should not be similar, just knowing they are different datasets\n",
    "\n",
    "isos = [\n",
    "    'inputs/devotes/devotes-80c47f28-d0bc-11e3-9261-00163c43a2bd_iso.xml',\n",
    "    'inputs/gstore/gstore-b4ae8f53-8dff-46bb-9058-e5501cabdd1b_iso.xml',\n",
    "    'inputs/medin/medin-abc9f747-537c-0f38-e044-0003ba9b0d98_iso.xml'\n",
    "]\n",
    "\n",
    "identifiers = []\n",
    "bags = []\n",
    "for f in isos:\n",
    "    bag = ''\n",
    "    with open(f, 'r') as g:\n",
    "        text = g.read()\n",
    "\n",
    "    identifier = os.path.basename(f)\n",
    "    bag = get_bag(text)\n",
    "\n",
    "    bags.append(bag)\n",
    "    identifiers.append(identifier)\n",
    "\n",
    "iso_scores = similarity(identifiers, bags)\n",
    "\n",
    "for k, v in iso_scores.iteritems():\n",
    "    print k\n",
    "    print '\\n'.join(['\\t{0}: {1}'.format(s, i) for s, i in v if i != k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Outcome\n",
    "\n",
    "Disparities in vector size matters - there's more standards cruft in the ISO compared to DC and the DC tends to have fewer text elements. So we could have the same abstract and title, but with the additonal text found in the lineage, for example, the mismatch lowers the similarity score.\n",
    "\n",
    "The order in which the bags of words are compared also affects the scores. Not hugely, but it could make the difference between similar or not depending ong the acceptance threshold. And for this we would have to set the threshold low to find any matches.\n",
    "\n",
    "There are content issues, as well. For cross-standards similarity discussions, we can only rely on certain types of text - titles, abstracts, perhaps a keyword set. This is not unreasonable until we consider a certian cultural practice, that of using an organizational description or project description as the abstract for any dataset published. That changes the similarity question to a kind of forensic data portal analysis. And if the titles aren't highly variable, we have then near-duplicate metadata for likely highly variable datasets. \n",
    "\n",
    "Still, this is a small demonstration simply to highlight issues with these vector-based approaches. Identifying similar or duplicate metadata information requires a different approach and likely one that employs a variety of tools.\n",
    "\n",
    "Finally, phew, at least there's not enough standards cruft in the ISO to render all of the similar here. Although that does read like a stable number. Someone should go sort out how much of an ISO record is likely to be the same across any ISO record (I am more on the side of better methods to identify high value text rather than generating some standards-specific extractor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Identifier Similarity\n",
    "\n",
    "For this we're relying on the kinds of non-cryptographic hashing methods common in crawling projects.\n",
    "\n",
    "This is as much about understanding what it means to provide different representations of identifiers across (or within) systems. It may not be the best method for getting at this but it does suggest that there's likely not one method to reliably identify an object across different identifier representations. For certain kinds of identifiers, more so certain representations of those, I'd suggest that none of the well-known vector methods are suitable. Specifically, mnemonic URLs where one character difference indicates an entirely different dataset, these methods will all return a very high similarity/near-duplicate score for the identifier. These kinds of representations are more likely to return false positives. And we can see, in the examples below, that other very well-defined PIDs might not be identifiable as referencing the same object because of how they're represented. In some cases, the set of representations appears stable and fairly limited, so a system can reliably extract the PID component of the representation. In others, it again becomes a server by server issue and that code burdern very quickly becomes unmanagable. \n",
    "\n",
    "The GCIS is an early system for providing very limited concordances (mapping short names and \"cool\" URIs, for example). It's not meant to be a generic identifier concordance provider - it's specific to NASA/NOAA and the assumptions they make about PIDs. \n",
    "\n",
    "This effort comes from less of a metadata quality perspective so it does have different biases. We're starting from an open world understanding. That the data here can come from any server, any service, so long as it's XML.\n",
    "\n",
    "**Simhashes**\n",
    "\n",
    "These are a kind of non-cryptographic hash described by Google for performant similarity checks at scale during web crawling activities. \n",
    "\n",
    "Two methods - a simple demonstration of simhashes with unmondified identifiers and The Daniel Method (splitting, sorting, concatenating, simhashing).\n",
    "\n",
    "Refs:\n",
    "\n",
    "https://liangsun.org/posts/a-python-implementation-of-simhash-algorithm/\n",
    "\n",
    "https://github.com/liangsun/simhash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from simhash import Simhash\n",
    "from itertools import chain\n",
    "from operator import itemgetter\n",
    "import collections\n",
    "\n",
    "# a shallow fork of the index - we want to return\n",
    "# the scores for demonstration purposes\n",
    "\n",
    "class Indexer(object):\n",
    "    def __init__(self, f=64, k=2):\n",
    "        self.bucket = collections.defaultdict(set)\n",
    "        self.k = k\n",
    "        self.f = f\n",
    "        \n",
    "        # note: bucket mods were made for a different \n",
    "        # process, not really necessary to init the \n",
    "        # thing outside of the indexer proper\n",
    "\n",
    "    def get_near_dups(self, simhash):\n",
    "        \"\"\"\n",
    "        `simhash` is an instance of Simhash\n",
    "        return a list of obj_id (pipe-delimited string of sha|text|distance)\n",
    "        \"\"\"\n",
    "        assert simhash.f == self.f\n",
    "\n",
    "        ans = set()\n",
    "\n",
    "        for key in self.get_keys(simhash):\n",
    "            dups = self.bucket.get(key, set())\n",
    "\n",
    "            for dup in dups:\n",
    "                sim2, obj_blob = dup.split(',', 1)\n",
    "                sim2 = Simhash(long(sim2, 16), self.f)\n",
    "\n",
    "                d = simhash.distance(sim2)\n",
    "                if d <= self.k:\n",
    "                    ans.add('{0}|{1}'.format(obj_blob, d))\n",
    "        return list(ans)\n",
    "    \n",
    "    def add(self, obj_id, obj_str, simhash):\n",
    "        \"\"\"\n",
    "        `obj_id` is a string\n",
    "        `simhash` is an instance of Simhash\n",
    "        \"\"\"\n",
    "        assert simhash.f == self.f\n",
    "\n",
    "        for key in self.get_keys(simhash):\n",
    "            v = '%x,%s|%s' % (simhash.value, obj_id, obj_str)\n",
    "\n",
    "            self.bucket.setdefault(key, set())\n",
    "            self.bucket[key].add(v)\n",
    "\n",
    "    def delete(self, obj_id, obj_str, simhash):\n",
    "        \"\"\"\n",
    "        `obj_id` is a string\n",
    "        `simhash` is an instance of Simhash\n",
    "        \"\"\"\n",
    "        assert simhash.f == self.f\n",
    "\n",
    "        for key in self.get_keys(simhash):\n",
    "            v = '%x,%s|%s' % (simhash.value, obj_id, obj_str)\n",
    "\n",
    "            if v in self.bucket.get(key, set()):\n",
    "                self.bucket[key].remove(v)\n",
    "    \n",
    "    @property\n",
    "    def offsets(self):\n",
    "        \"\"\"\n",
    "        You may optimize this method according to <http://www.wwwconference.org/www2007/papers/paper215.pdf>\n",
    "        \"\"\"\n",
    "        return [self.f // (self.k + 1) * i for i in range(self.k + 1)]\n",
    "\n",
    "    def get_keys(self, simhash):\n",
    "        for i, offset in enumerate(self.offsets):\n",
    "            m = (i == len(self.offsets) - 1 and 2 ** (self.f - offset) - 1 or 2 ** (self.offsets[i + 1] - offset) - 1)\n",
    "            c = simhash.value >> offset & m\n",
    "            yield '%x:%x' % (c, i)\n",
    "\n",
    "    def bucket_size(self):\n",
    "        return len(self.bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_set(test_set, k=20):\n",
    "    duplicates = {}\n",
    "    index = Indexer(k=k)\n",
    "    # build the index\n",
    "    for test_id, test_item, test_simhash in test_set:\n",
    "        index.add(test_id, test_item, test_simhash)\n",
    "        \n",
    "    # run per test string\n",
    "    for test_id, test_item, test_simhash in test_set:\n",
    "        dupes = index.get_near_dups(test_simhash)\n",
    "        \n",
    "        # as id, string, score\n",
    "        duplicates[test_item] = [d.split('|') for d in dupes]\n",
    "    return duplicates\n",
    "\n",
    "def print_dupes(dupes):\n",
    "    for key, v in dupes.iteritems():\n",
    "        print key\n",
    "        \n",
    "        for i, t, s in v:\n",
    "            if i == key or int(s) == 0:\n",
    "                continue\n",
    "            print '\\t{0} : {1} ({2})'.format(i, s, t)\n",
    "        \n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our set of identifiers\n",
    "\n",
    "# dois\n",
    "dois = [\n",
    "    # one set of different doi representations\n",
    "    'http://dx.doi.org/10.7916/D85B019G',\n",
    "    '10.7916/D85B019G',\n",
    "    'doi:10.7916/D85B019G',\n",
    "    # additional strings\n",
    "    '10.7916/D85B0121',\n",
    "    'http://dx.doi.org/10.5257/iea/ept/2011q3',\n",
    "    '10.3334/ORNLDAAC/887',\n",
    "    'doi:10.5281/zenodo.11169',\n",
    "    '10.5281/zenodo.15638'\n",
    "]\n",
    "\n",
    "\n",
    "# some mnemonic things (one or two characters difference only)\n",
    "mnemonics = [\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "# thredds things (vector size issues )\n",
    "thredds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://dx.doi.org/10.7916/D85B019G\n",
      "\t10.7916/D85B019G : 19 (10.7916/D85B019G)\n",
      "\thttp://dx.doi.org/10.5257/iea/ept/2011q3 : 19 (http://dx.doi.org/10.5257/iea/ept/2011q3)\n",
      "\n",
      "http://dx.doi.org/10.5257/iea/ept/2011q3\n",
      "\thttp://dx.doi.org/10.7916/D85B019G : 19 (http://dx.doi.org/10.7916/D85B019G)\n",
      "\n",
      "10.7916/D85B0121\n",
      "\t10.7916/D85B019G : 16 (10.7916/D85B019G)\n",
      "\n",
      "doi:10.5281/zenodo.11169\n",
      "\t10.5281/zenodo.15638 : 16 (10.5281/zenodo.15638)\n",
      "\n",
      "10.3334/ORNLDAAC/887\n",
      "\n",
      "doi:10.7916/D85B019G\n",
      "\t10.7916/D85B019G : 16 (10.7916/D85B019G)\n",
      "\n",
      "10.5281/zenodo.15638\n",
      "\tdoi:10.5281/zenodo.11169 : 16 (doi:10.5281/zenodo.11169)\n",
      "\n",
      "10.7916/D85B019G\n",
      "\thttp://dx.doi.org/10.7916/D85B019G : 19 (http://dx.doi.org/10.7916/D85B019G)\n",
      "\tdoi:10.7916/D85B019G : 16 (doi:10.7916/D85B019G)\n",
      "\t10.7916/D85B0121 : 16 (10.7916/D85B0121)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doi_index = [(d, d, Simhash(d)) for d in dois]\n",
    "dupes = evaluate_set(doi_index)\n",
    "print_dupes(dupes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107916D85B019Gdoidxhttporg\n",
      "\tdoi:10.7916/D85B019G : 17 (107916D85B019Gdoi)\n",
      "\t10.7916/D85B019G : 15 (107916D85B019G)\n",
      "\n",
      "107916D85B019Gdoi\n",
      "\thttp://dx.doi.org/10.7916/D85B019G : 17 (107916D85B019Gdoidxhttporg)\n",
      "\t10.7916/D85B019G : 14 (107916D85B019G)\n",
      "\n",
      "107916D85B019G\n",
      "\tdoi:10.7916/D85B019G : 14 (107916D85B019Gdoi)\n",
      "\thttp://dx.doi.org/10.7916/D85B019G : 15 (107916D85B019Gdoidxhttporg)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the daniel method:\n",
    "# split string\n",
    "# sort\n",
    "# concatenate\n",
    "# score\n",
    "\n",
    "import re\n",
    "pttn = r'[:./]'\n",
    "\n",
    "sorted_dois = []\n",
    "for d in dois:\n",
    "    x = [s for s in re.split(pttn, d) if s]\n",
    "    x.sort()\n",
    "    \n",
    "    sorted_dois.append((d, ''.join(x)))\n",
    "    \n",
    "doi_index = [(i, d, Simhash(d)) for i, d in sorted_dois]\n",
    "dupes = evaluate_set(doi_index)\n",
    "print_dupes(dupes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like splitting & sorting reduces the score, ie the string are slightly more similar, but that could simply be removing the punctuation. [Totally making blanket statements about effectiveness based on three things. ¯\\\\_(ツ)_/¯]\n",
    "\n",
    "From the [Manku paper](http://www2007.cpsc.ucalgary.ca/papers/paper215.pdf):\n",
    "> Figure 1 clearly shows the trade-offs for various values of k: A very low value misses near-duplicates (false negatives), and a very high value tags incorrect pairs as near-duplicates (false positives). Choosing k = 3 is reasonable because both precision and recall are near 0.75. So, for 64-bit fingerprints, declaring two documents as near-duplicates when their fingerprints differ in at most 3 bits gives fairly high accuracy.\n",
    "\n",
    "So the python implementation defaults to a 64 bit hash and we're setting the *k* value to 20. We are working with much shorter inputs. \n",
    "\n",
    "Running all of the identifier sets, as is and restructured, across a range of *k* values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's just remove the punctuation to verify that here. \n",
    "\n",
    "(Also, not going to do anything without the larger tests against types of things.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "httpdxdoiorg107916D85B019G\n",
      "\t10.7916/D85B019G : 19 (107916D85B019G)\n",
      "\n",
      "doi107916D85B019G\n",
      "\t10.7916/D85B019G : 16 (107916D85B019G)\n",
      "\n",
      "107916D85B019G\n",
      "\thttp://dx.doi.org/10.7916/D85B019G : 19 (httpdxdoiorg107916D85B019G)\n",
      "\tdoi:10.7916/D85B019G : 16 (doi107916D85B019G)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the \"is it just removing punctuation?\" question\n",
    "# on the first three known same object, different representation question\n",
    "\n",
    "import re\n",
    "pttn = r'[:./]'\n",
    "\n",
    "unsorted_dois = []\n",
    "for d in dois[:3]:\n",
    "    x = [s for s in re.split(pttn, d) if s]\n",
    "    \n",
    "    unsorted_dois.append((d, ''.join(x)))\n",
    "    \n",
    "doi_index = [(i, d, Simhash(d)) for i, d in unsorted_dois]\n",
    "dupes = evaluate_set(doi_index)\n",
    "print_dupes(dupes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, no, it isn't just stripping out the punctuation. We'll stick with The Daniel Method for comparing normal simhashing to this modified string simhashing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
