{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "This is demonstration only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json as js  # name conflict with sqla\n",
    "import sqlalchemy as sqla\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "from mpp.models import Response\n",
    "# from semproc.unique_identifiers import IdentifierExtractor\n",
    "\n",
    "import re\n",
    "from semproc.bag_parser import BagParser\n",
    "from semproc.nlp_utils import load_token_list\n",
    "from semproc.utils import unquote, break_url, match\n",
    "import dateutil.parser as dateparser\n",
    "from itertools import chain, izip\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for sorting out excludes\n",
    "\n",
    "_pattern_set = [\n",
    "    ('url', re.compile(ur\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\", re.IGNORECASE)),\n",
    "    # a urn that isn't a url\n",
    "    ('urn', re.compile(ur\"(?![http://])(?![https://])(?![ftp://])(([a-z0-9.\\S][a-z0-9-.\\S]{0,}\\S:{1,2}\\S)+[a-z0-9()+,\\-.=@;$_!*'%/?#]+)\", re.IGNORECASE)),\n",
    "    # ('urn', re.compile(ur\"\\burn:[a-z0-9][a-z0-9-]{0,31}:[a-z0-9()+,\\-.:=@;$_!*'%/?#]+\", re.IGNORECASE)),\n",
    "    ('uuid', re.compile(ur'([a-f\\d]{8}(-[a-f\\d]{4}){3}-[a-f\\d]{12}?)', re.IGNORECASE)),\n",
    "    ('doi', re.compile(ur\"(10[.][0-9]{4,}(?:[/][0-9]+)*/(?:(?![\\\"&\\\\'])\\S)+)\", re.IGNORECASE)),\n",
    "    ('md5', re.compile(ur\"([a-f0-9]{32})\", re.IGNORECASE))\n",
    "]\n",
    "\n",
    "_rule_set = [\n",
    "    ('uri', 'fileIdentifier/CharacterString'),  # ISO\n",
    "    ('uri', 'identifier/*/code/CharacterString'),\n",
    "    ('uri', 'dataSetURI/CharacterString'),\n",
    "    ('uri', 'parentIdentifier/CharacterString'),\n",
    "    ('uri', 'Entry_ID'),  # DIF\n",
    "    ('uri', 'dc/identifier'),  # DC\n",
    "    ('basic', 'Layer/Name'),  # WMS\n",
    "    ('basic', 'dataset/@ID'),  # THREDDS\n",
    "    ('uri', '@URI'),  # ddi\n",
    "    ('uri', '@IDNo')  # ddi\n",
    "]\n",
    "\n",
    "\n",
    "class Identifier(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            tag,\n",
    "            extraction_type,\n",
    "            match_type,\n",
    "            original_text,\n",
    "            potential_identifier):\n",
    "        self.tag = tag\n",
    "        self.extraction_type = extraction_type\n",
    "        self.match_type = match_type\n",
    "        self.original_text = original_text\n",
    "        self.potential_identifier = potential_identifier\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (isinstance(other, self.__class__)\n",
    "                and self.__dict__ == other.__dict__)\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return not self.__eq__(other)\n",
    "\n",
    "    def has(self, comparison_identifier):\n",
    "        return self.potential_identifier == comparison_identifier\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<Identifier %s>' % js.dumps(self.__dict__)\n",
    "\n",
    "    def to_json(self):\n",
    "        return {\n",
    "            \"tag\": self.tag,\n",
    "            \"extraction_type\": self.extraction_type,\n",
    "            \"match_type\": self.match_type,\n",
    "            \"original_text\": self.original_text,\n",
    "            \"potential_identifier\": self.potential_identifier\n",
    "        }\n",
    "\n",
    "\n",
    "class IdentifierExtractor(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            source_url,\n",
    "            source_xml_as_str,\n",
    "            equality_excludes=[],\n",
    "            contains_excludes=[],\n",
    "            tag_excludes=[]\n",
    "        ):\n",
    "        self.source_url = source_url\n",
    "        self.source_xml_as_str = source_xml_as_str\n",
    "        self.identifieds = []\n",
    "        self.texts = [('', self.source_url)]\n",
    "        self.seen_texts = []\n",
    "        self.equality_excludes = equality_excludes\n",
    "        self.contains_excludes = contains_excludes\n",
    "        self.tag_excludes = tag_excludes\n",
    "\n",
    "        self._parse()\n",
    "\n",
    "    def _parse(self):\n",
    "        try:\n",
    "            parser = BagParser(self.source_xml_as_str, False, False)\n",
    "        except Exception as ex:\n",
    "            print ex\n",
    "            raise ex\n",
    "        if not parser or parser.parser.xml is None:\n",
    "            raise Exception('failed to parse')\n",
    "\n",
    "        for tag, txt in parser.strip_text():\n",
    "            self.texts.append((tag, txt))\n",
    "\n",
    "    def _strip_punctuation(self, text):\n",
    "        terminal_punctuation = '(){}[].,~|\":&-'\n",
    "        text = text.strip(terminal_punctuation)\n",
    "        return text.strip()\n",
    "\n",
    "    def _strip_dates(self, text):\n",
    "        # this should still make it an invalid date\n",
    "        # text = text[3:] if text.startswith('NaN') else text\n",
    "        try:\n",
    "            d = dateparser.parse(text)\n",
    "            return ''\n",
    "        except ValueError:\n",
    "            return text\n",
    "\n",
    "    def _strip_scales(self, text):\n",
    "        scale_pttn = ur\"(1:[\\d]{0,}(,[\\d]{3}){1,})\"\n",
    "        m = match(text, scale_pttn)\n",
    "        if m:\n",
    "            return ''\n",
    "        return text\n",
    "\n",
    "    def _strip_excludes(self, match_type, tag, text):\n",
    "        # TODO: how to handle the excludes for skipping\n",
    "        #       by tag and skipping by value and what was\n",
    "        #       the thinking a few months ago?\n",
    "        \n",
    "        # if it came from any known elem/attrib\n",
    "        if any([t.lower() in tag.lower() for t in self.tag_excludes]):\n",
    "            return ''\n",
    "        \n",
    "        # if it's a url, must equal an item\n",
    "        if match_type == 'url':\n",
    "            if text in self.equality_excludes:\n",
    "                # equality only\n",
    "                return ''\n",
    "            return text\n",
    "\n",
    "        # if it's *not* a url and contains any of these patterns\n",
    "        if any(e.lower() in text.lower() for e in self.contains_excludes):\n",
    "            return ''\n",
    "        return text\n",
    "\n",
    "    def _strip_whitespace(self, text):\n",
    "        space_pattern = re.compile(' ')\n",
    "        if space_pattern.subn(' ', text)[1] > 0:\n",
    "            # i actually don't know if this is the right index. huh.\n",
    "            return text.split(' ')[0]\n",
    "        return text\n",
    "\n",
    "    def _tidy_text(self, text):\n",
    "        text = self._strip_punctuation(text)\n",
    "        if not text:\n",
    "            return ''\n",
    "\n",
    "        text = self._strip_scales(text)\n",
    "        if not text:\n",
    "            return ''\n",
    "\n",
    "        text = self._strip_dates(text)\n",
    "        if not text:\n",
    "            return ''\n",
    "\n",
    "        text = self._strip_whitespace(text)\n",
    "        if not text:\n",
    "            return ''\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _extract_url(self, text):\n",
    "        pttn = re.compile(ur\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\",\n",
    "                          re.IGNORECASE)\n",
    "        m = match(text, pttn)\n",
    "        if not m:\n",
    "            return '', []\n",
    "\n",
    "        url = self._tidy_text(unquote(m))\n",
    "        base_url, values = break_url(url)\n",
    "        values = values.split(' ') + [base_url]\n",
    "\n",
    "        # return the original extracted url, and the values plus \n",
    "        # the base_url for more extracting\n",
    "        return url, filter(None, [self._tidy_text(v) for v in values])\n",
    "\n",
    "    def _extract_identifiers(self, text):\n",
    "        for pattern_type, pattern in _pattern_set:\n",
    "            m = match(text, pattern)\n",
    "            if not m:\n",
    "                continue\n",
    "            yield pattern_type, m\n",
    "\n",
    "    def _check(self, comparison_identifier, arr):\n",
    "        return len([a for a in arr if a.has(comparison_identifier)]) > 0\n",
    "\n",
    "    def process_text(self):\n",
    "        while self.texts:\n",
    "            tag, text = self.texts.pop()\n",
    "            if self._check(text, self.seen_texts) or not text.strip():\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                j = json.loads(text)\n",
    "                j.keys()  # it will decode a quoted string without error\n",
    "                continue\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            url, values = self._extract_url(text)\n",
    "            values = [v for v in values if not self._check(v, self.seen_texts)]\n",
    "            self.texts += list(iter(izip([tag] * len(values), values)))\n",
    "\n",
    "            if url and not self._check(url, self.identifieds) \\\n",
    "                    and not self._check(url, self.seen_texts):\n",
    "                identify = Identifier(tag, 'regex', 'url', text, url)\n",
    "                self.identifieds.append(identify)\n",
    "                self.seen_texts.append(identify)\n",
    "\n",
    "            # now run the OTHER regex\n",
    "            for match_type, match_text in self._extract_identifiers(text):\n",
    "                # clean it up (make sure it's still a good thing)\n",
    "                # before checking against knowns and appending\n",
    "                cleaned_text = self._tidy_text(match_text)\n",
    "\n",
    "                if cleaned_text and not self._check(\n",
    "                        cleaned_text, self.identifieds):\n",
    "                    self.identifieds.append(\n",
    "                        Identifier(\n",
    "                            tag, 'regex', match_type, text, cleaned_text\n",
    "                        )\n",
    "                    )\n",
    "                if not self._check(cleaned_text, self.seen_texts):\n",
    "                    self.texts.append((tag, cleaned_text))\n",
    "                    self.seen_texts.append(\n",
    "                        Identifier(\n",
    "                            tag, 'regex', match_type, text, cleaned_text)\n",
    "                    )\n",
    "                    \n",
    "        # exclude from here\n",
    "        for i in self.identifieds:\n",
    "            # type tag text\n",
    "            text = self._strip_excludes(i.match_type, i.tag, i.potential_identifier)\n",
    "            if text:\n",
    "                yield i\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exclude lists\n",
    "\n",
    "# the equality excludes \n",
    "equality = []\n",
    "files = ['namespaces.txt']\n",
    "for f in files:\n",
    "    equality += load_token_list(f)\n",
    "    \n",
    "equality = ['http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml']\n",
    "    \n",
    "contains = []\n",
    "files = ['cat_interop_urns.txt', 'mimetypes.txt', 'namespaces.txt']\n",
    "for f in files:\n",
    "    contains += load_token_list(f)\n",
    "\n",
    "tag_paths = ['@codeList', '@schemaLocation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def strip_excludes(match_type, tag, text):\n",
    "    # TODO: how to handle the excludes for skipping\n",
    "    #       by tag and skipping by value and what was\n",
    "    #       the thinking a few months ago?\n",
    "\n",
    "    # if it came from any known elem/attrib\n",
    "    if any([t.lower() in tag.lower() for t in tag_paths]):\n",
    "        return ''\n",
    "\n",
    "    # if it's a url, must equal an item\n",
    "    if match_type == 'url':\n",
    "        if text in equality:\n",
    "            # equality only\n",
    "            return ''\n",
    "        return text\n",
    "\n",
    "    # if it's *not* a url and contains any of these patterns\n",
    "    if any(e.lower() in text.lower() for e in contains):\n",
    "        return ''\n",
    "    return text\n",
    "\n",
    "strip_excludes(\n",
    "    'url',\n",
    "    'MD_Metadata/dataQualityInfo/DQ_DataQuality/scope/DQ_Scope/level/MD_ScopeCode',\n",
    "    'http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grab the clean text from the rds\n",
    "with open('../local/big_rds.conf', 'r') as f:\n",
    "    conf = js.loads(f.read())\n",
    "\n",
    "# our connection\n",
    "engine = sqla.create_engine(conf.get('connection'))\n",
    "Session = sessionmaker()\n",
    "Session.configure(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response_id = 138960\n",
    "response_url, response_cleaned, = session.query(\n",
    "    Response.source_url,\n",
    "    Response.cleaned_content\n",
    ").filter(Response.id==response_id).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extractor = IdentifierExtractor(response_url, response_cleaned, equality, contains, tag_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original_text': 'https://geo.azmag.gov/gismag/rest/services/maps/WI_Zoning/MapServer', 'extraction_type': 'regex', 'tag': 'MD_Metadata/distributionInfo/MD_Distribution/transferOptions/MD_DigitalTransferOptions/onLine/CI_OnlineResource/linkage/URL', 'match_type': 'url', 'potential_identifier': 'https://geo.azmag.gov/gismag/rest/services/maps/WI_Zoning/MapServer'}\n",
      "{'original_text': 'usginres:service', 'extraction_type': 'regex', 'tag': 'MD_Metadata/identificationInfo/MD_DataIdentification/descriptiveKeywords/MD_Keywords/keyword/CharacterString', 'match_type': 'urn', 'potential_identifier': 'usginres:service'}\n",
      "{'original_text': 'D31C9D5DD17447BAA4980066B6EAA61F', 'extraction_type': 'regex', 'tag': 'MD_Metadata/fileIdentifier/CharacterString', 'match_type': 'md5', 'potential_identifier': 'D31C9D5DD17447BAA4980066B6EAA61F'}\n",
      "{'original_text': u'http://repository.stategeothermaldata.org/resources/metadata/TestNRRC/D31C9D5DD17447BAA4980066B6EAA61F.xml', 'extraction_type': 'regex', 'tag': '', 'match_type': 'url', 'potential_identifier': u'http://repository.stategeothermaldata.org/resources/metadata/TestNRRC/D31C9D5DD17447BAA4980066B6EAA61F.xml'}\n"
     ]
    }
   ],
   "source": [
    "for i in extractor.process_text():\n",
    "    print i.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's run some identifiers (a few k to see what borks, etc)\n",
    "\n",
    "# session.rollback()\n",
    "\n",
    "with open('../local/local_pg.conf', 'r') as f:\n",
    "    local_conf = js.loads(f.read())\n",
    "\n",
    "# our connection\n",
    "local_engine = sqla.create_engine(local_conf.get('connection'))\n",
    "local_Session = sessionmaker()\n",
    "local_Session.configure(bind=local_engine)\n",
    "local_session = local_Session()\n",
    "\n",
    "sketchy_sql ='''WITH t AS (SELECT *, row_number() OVER () AS rn FROM responses WHERE format = 'xml')\n",
    "SELECT * FROM (\n",
    "    SELECT trunc(random() * (SELECT max(rn) FROM t))::int + 1 AS rn\n",
    "    FROM   generate_series(1, 1000) g\n",
    "    ) r\n",
    "JOIN   t USING (rn);'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the model\n",
    "import sqlalchemy as sqla\n",
    "from sqlalchemy import (\n",
    "    MetaData,\n",
    "    Column,\n",
    "    String,\n",
    "    DateTime,\n",
    "    Integer,\n",
    "    Boolean,\n",
    "    ForeignKey,\n",
    ")\n",
    "from sqlalchemy.dialects.postgresql import *\n",
    "from sqlalchemy.orm import relationship, backref\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "local_Base = declarative_base()\n",
    "\n",
    "class Extracted(local_Base):\n",
    "    __tablename__ = 'identifiers'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    source_url = Column(String)\n",
    "    tag = Column(String)\n",
    "    extraction_type = Column(String)\n",
    "    match_type = Column(String)\n",
    "    original_text = Column(String)\n",
    "    potential_identifier = Column(String)\n",
    "    response_id = Column(Integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fails = []\n",
    "for result in session.execute(sketchy_sql):\n",
    "#     print result['id']\n",
    "    identifiers = []\n",
    "    try:\n",
    "        extractor = IdentifierExtractor(result['source_url'], result['cleaned_content'], equality, contains, tag_paths)\n",
    "        for i in extractor.process_text():\n",
    "            identifiers.append(i.to_json())\n",
    "    except:\n",
    "        fails.append(result['id'])\n",
    "        continue\n",
    "    \n",
    "    for e in extractor.process_text():\n",
    "        ex = Extracted(\n",
    "            source_url=result['source_url'],\n",
    "            tag=e.tag,\n",
    "            extraction_type=e.extraction_type,\n",
    "            match_type=e.match_type,\n",
    "            original_text=e.original_text,\n",
    "            potential_identifier=e.potential_identifier,\n",
    "            response_id=result['id']\n",
    "        )\n",
    "        try:\n",
    "            local_session.add(ex)\n",
    "            local_session.commit()\n",
    "        except Exception as a:\n",
    "            print 'failed identifier:', e.to_json()\n",
    "            print a\n",
    "            local_session.rollback()\n",
    "            continue\n",
    "        \n",
    "        \n",
    "len(fails)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original_text': u'http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml#MD_ScopeCode', 'extraction_type': 'regex', 'tag': 'MD_Metadata/dataQualityInfo/DQ_DataQuality/scope/DQ_Scope/level/MD_ScopeCode/@codeList', 'match_type': 'url', 'potential_identifier': u'http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml#MD_ScopeCode'}\n",
      "{'original_text': u'http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml', 'extraction_type': 'regex', 'tag': 'MD_Metadata/dataQualityInfo/DQ_DataQuality/scope/DQ_Scope/level/MD_ScopeCode/@codeList', 'match_type': 'url', 'potential_identifier': u'http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml'}\n",
      "{'original_text': 'https://geo.azmag.gov/gismag/rest/services/maps/WI_Zoning/MapServer', 'extraction_type': 'regex', 'tag': 'MD_Metadata/distributionInfo/MD_Distribution/transferOptions/MD_DigitalTransferOptions/onLine/CI_OnlineResource/linkage/URL', 'match_type': 'url', 'potential_identifier': 'https://geo.azmag.gov/gismag/rest/services/maps/WI_Zoning/MapServer'}\n",
      "{'original_text': u'http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml#CI_RoleCode', 'extraction_type': 'regex', 'tag': 'MD_Metadata/distributionInfo/MD_Distribution/distributor/MD_Distributor/distributorContact/CI_ResponsibleParty/role/CI_RoleCode/@codeList', 'match_type': 'url', 'potential_identifier': u'http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml#CI_RoleCode'}\n",
      "{'original_text': u'http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml#MD_KeywordTypeCode', 'extraction_type': 'regex', 'tag': 'MD_Metadata/identificationInfo/MD_DataIdentification/descriptiveKeywords/MD_Keywords/type/MD_KeywordTypeCode/@codeList', 'match_type': 'url', 'potential_identifier': u'http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml#MD_KeywordTypeCode'}\n",
      "{'original_text': 'usginres:service', 'extraction_type': 'regex', 'tag': 'MD_Metadata/identificationInfo/MD_DataIdentification/descriptiveKeywords/MD_Keywords/keyword/CharacterString', 'match_type': 'urn', 'potential_identifier': 'usginres:service'}\n",
      "{'original_text': u'http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml#MD_ProgressCode', 'extraction_type': 'regex', 'tag': 'MD_Metadata/identificationInfo/MD_DataIdentification/status/MD_ProgressCode/@codeList', 'match_type': 'url', 'potential_identifier': u'http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml#MD_ProgressCode'}\n",
      "{'original_text': u'http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml#CI_DateTypeCode', 'extraction_type': 'regex', 'tag': 'MD_Metadata/identificationInfo/MD_DataIdentification/citation/CI_Citation/date/CI_Date/dateType/CI_DateTypeCode/@codeList', 'match_type': 'url', 'potential_identifier': u'http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml#CI_DateTypeCode'}\n",
      "{'original_text': u'http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml#MD_CharacterSetCode', 'extraction_type': 'regex', 'tag': 'MD_Metadata/characterSet/MD_CharacterSetCode/@codeList', 'match_type': 'url', 'potential_identifier': u'http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml#MD_CharacterSetCode'}\n",
      "{'original_text': 'D31C9D5DD17447BAA4980066B6EAA61F', 'extraction_type': 'regex', 'tag': 'MD_Metadata/fileIdentifier/CharacterString', 'match_type': 'md5', 'potential_identifier': 'D31C9D5DD17447BAA4980066B6EAA61F'}\n",
      "{'original_text': u'http://www.isotc211.org/2005/gmd http://schemas.opengis.net/csw/2.0.2/profiles/apiso/1.0.0/apiso.xsd', 'extraction_type': 'regex', 'tag': 'MD_Metadata/@schemaLocation', 'match_type': 'url', 'potential_identifier': u'http://www.isotc211.org/2005/gmd'}\n",
      "{'original_text': u'http://repository.stategeothermaldata.org/resources/metadata/TestNRRC/D31C9D5DD17447BAA4980066B6EAA61F.xml', 'extraction_type': 'regex', 'tag': '', 'match_type': 'url', 'potential_identifier': u'http://repository.stategeothermaldata.org/resources/metadata/TestNRRC/D31C9D5DD17447BAA4980066B6EAA61F.xml'}\n"
     ]
    }
   ],
   "source": [
    "for e in extractor.identifieds:\n",
    "    print e.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extractor.identifieds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
