{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is demonstration only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json as js  # name conflict with sqla\n",
    "import sqlalchemy as sqla\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "from mpp.models import Response\n",
    "# from semproc.unique_identifiers import IdentifierExtractor\n",
    "\n",
    "import re\n",
    "from semproc.bag_parser import BagParser\n",
    "from semproc.nlp_utils import load_token_list\n",
    "from semproc.utils import unquote, break_url, match\n",
    "from semproc.xml_utils import extract_items\n",
    "import dateutil.parser as dateparser\n",
    "from datetime import datetime\n",
    "from itertools import chain, izip\n",
    "from rfc3987 import parse as uparse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for sorting out excludes\n",
    "\n",
    "_pattern_set = [\n",
    "    ('url', re.compile(ur\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\", re.IGNORECASE)),\n",
    "    # a urn that isn't a url\n",
    "#     ('urn', re.compile(ur\"(?!(?:http://))(?!(?:https://))(?!(?:ftp://))(([a-z0-9.\\S][a-z0-9-.\\S]{0,}\\S:{1,2}\\S)+[a-z0-9()+,\\-.=@;$_!*'%/?#]+)(?=.*/)\", re.IGNORECASE)),\n",
    "#     ('urn', re.compile(ur\"(([a-z0-9.\\S][a-z0-9-.\\S]{0,}\\S:{1,2}\\S)+[a-z0-9()+,\\-.=@;$_!*'%/?#]+)(?=.*/)\", re.IGNORECASE)),\n",
    "    # ('urn', re.compile(ur\"\\burn:[a-z0-9][a-z0-9-]{0,31}:[a-z0-9()+,\\-.:=@;$_!*'%/?#]+\", re.IGNORECASE)),\n",
    "    # this one is okay\n",
    "    #('urn', re.compile(ur'([a-z0-9]{0,}:[a-z0-9][a-z0-9-]{0,31}:[a-z0-9()+,\\-.:=@;$_!*\\'%/?#]+)', re.IGNORECASE)),\n",
    "    ('urn', re.compile(ur'^([a-z0-9.#]{0,}:([a-z0-9][a-z0-9.-]{0,31}):[a-z0-9A-Z_()+,-.:=@;$!*\\'%/?#\\[\\]]+)', re.IGNORECASE)),\n",
    "    ('uuid', re.compile(ur'([a-f\\d]{8}(-[a-f\\d]{4}){3}-[a-f\\d]{12}?)', re.IGNORECASE)),\n",
    "    ('doi', re.compile(ur\"(10[.][0-9]{4,}(?:[/][0-9]+)*/(?:(?![\\\"&\\\\'])\\S)+)\", re.IGNORECASE)),\n",
    "    ('md5', re.compile(ur\"(?=(\\b[A-Fa-f0-9]{32}\\b))\", re.IGNORECASE))\n",
    "]\n",
    "\n",
    "_rule_set = [\n",
    "    ('uri', 'fileIdentifier/CharacterString'),  # ISO\n",
    "    ('uri', 'identifier/'),\n",
    "    ('uri', 'dataSetURI/CharacterString'),\n",
    "    ('uri', 'parentIdentifier/CharacterString'),\n",
    "    ('uri', 'Entry_ID'),  # DIF\n",
    "    ('uri', 'dc/identifier'),  # DC\n",
    "    ('basic', 'Layer/Name'),  # WMS\n",
    "    ('basic', 'dataset/ID'),  # THREDDS\n",
    "    ('uri', '@URI'),  # ddi\n",
    "    ('uri', '@IDNo'),  # ddi\n",
    "    ('uri', '@ID')\n",
    "]\n",
    "\n",
    "\n",
    "class Identifier(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            tag,\n",
    "            extraction_type,\n",
    "            match_type,\n",
    "            original_text,\n",
    "            potential_identifier):\n",
    "        self.tag = tag\n",
    "        self.extraction_type = extraction_type\n",
    "        self.match_type = match_type\n",
    "        self.original_text = original_text\n",
    "        self.potential_identifier = potential_identifier\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (isinstance(other, self.__class__)\n",
    "                and self.__dict__ == other.__dict__)\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return not self.__eq__(other)\n",
    "\n",
    "    def has(self, comparison_identifier):\n",
    "        return self.potential_identifier == comparison_identifier\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<Identifier %s>' % js.dumps(self.__dict__)\n",
    "\n",
    "    def to_json(self):\n",
    "        return {\n",
    "            \"tag\": self.tag,\n",
    "            \"extraction_type\": self.extraction_type,\n",
    "            \"match_type\": self.match_type,\n",
    "            \"original_text\": self.original_text,\n",
    "            \"potential_identifier\": self.potential_identifier\n",
    "        }\n",
    "\n",
    "\n",
    "class IdentifierExtractor(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            source_url,\n",
    "            source_xml_as_str,\n",
    "            equality_excludes=[],\n",
    "            contains_excludes=[],\n",
    "            tag_excludes=[]\n",
    "        ):\n",
    "        self.source_url = source_url\n",
    "        self.source_xml_as_str = source_xml_as_str\n",
    "        self.identifieds = []\n",
    "        self.texts = [('', self.source_url)]\n",
    "        self.seen_texts = []\n",
    "        self.equality_excludes = equality_excludes\n",
    "        self.contains_excludes = contains_excludes\n",
    "        self.tag_excludes = tag_excludes\n",
    "\n",
    "        self._parse()\n",
    "\n",
    "    def _parse(self):\n",
    "        try:\n",
    "            parser = BagParser(self.source_xml_as_str, True, False)\n",
    "        except Exception as ex:\n",
    "            print ex\n",
    "            raise ex\n",
    "        if not parser or parser.parser.xml is None:\n",
    "            raise Exception('failed to parse')\n",
    "\n",
    "        for tag, txt in parser.strip_text():\n",
    "            # if it's from a tag we know we nee to exclude\n",
    "            if any(t in tag for t in self.tag_excludes):\n",
    "                continue\n",
    "                \n",
    "            if not txt.strip():\n",
    "                continue\n",
    "                \n",
    "            # do not split if it comes form an identifier field\n",
    "            self.texts += ((tag, t) for t in txt.split()) if not any(r[1] in tag for r in _rule_set) else [(tag, txt)]           \n",
    "            \n",
    "    def _strip_punctuation(self, text):\n",
    "        terminal_punctuation = '(){}[],~|\":&-<>.'\n",
    "        text = text.strip(terminal_punctuation)\n",
    "        return text.strip()\n",
    "\n",
    "    def _strip_dates(self, text):\n",
    "        # this should still make it an invalid date\n",
    "        # text = text[3:] if text.startswith('NaN') else text\n",
    "        def try_format(fmt):\n",
    "            try:\n",
    "                d = datetime.strptime(text, fmt)\n",
    "            except:\n",
    "                return False\n",
    "            return True\n",
    "                \n",
    "        try:\n",
    "            d = dateparser.parse(text)\n",
    "            return ''\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        known_formats = ['%d/%m/%Y [%H:%M:%S:%f]', '%H:%M:%S%f']\n",
    "        tests = [try_format(kf) for kf in known_formats]\n",
    "        return '' if sum(tests) > 0 else text\n",
    "\n",
    "    def _strip_scales(self, text):\n",
    "        #'1:100:000'\n",
    "        scale_pttn = ur\"(1:[\\d]{0,}(,[\\d]{3}){1,})\"\n",
    "        m = match(text, scale_pttn)\n",
    "        if m:\n",
    "            return ''\n",
    "        return text\n",
    "\n",
    "    def _strip_excludes(self, match_type, tag, text):\n",
    "        # TODO: how to handle the excludes for skipping\n",
    "        #       by tag and skipping by value and what was\n",
    "        #       the thinking a few months ago?\n",
    "        \n",
    "        # if it came from any known elem/attrib\n",
    "        if any([t.lower() in tag.lower() for t in self.tag_excludes]):\n",
    "            return ''\n",
    "        \n",
    "        # if it's a url, must equal an item\n",
    "        if match_type == 'url':\n",
    "            if text in self.equality_excludes:\n",
    "                # equality only\n",
    "                return ''\n",
    "            return text\n",
    "\n",
    "        # if it's *not* a url and contains any of these patterns\n",
    "        if any(e.lower() in text.lower() for e in self.contains_excludes):\n",
    "            return ''\n",
    "        return text\n",
    "\n",
    "    def _strip_whitespace(self, text):\n",
    "        space_pattern = re.compile(' ')\n",
    "        if space_pattern.subn(' ', text)[1] > 0:\n",
    "            # i actually don't know if this is the right index. huh.\n",
    "            return text.split(' ')[0]\n",
    "        return text\n",
    "\n",
    "    def _tidy_text(self, text):\n",
    "        text = self._strip_punctuation(text)\n",
    "        if not text:\n",
    "            return ''\n",
    "\n",
    "        text = self._strip_scales(text)\n",
    "        if not text:\n",
    "            return ''\n",
    "\n",
    "        text = self._strip_dates(text)\n",
    "        if not text:\n",
    "            return ''\n",
    "\n",
    "        text = self._strip_whitespace(text)\n",
    "        if not text:\n",
    "            return ''\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _verify_urn(self, potential_urn):\n",
    "        # not much point to this but we're just going to leave it\n",
    "        pttn = re.compile(ur'^([a-z0-9.#]{0,}:([a-z0-9][a-z0-9.-]{0,31}):[a-z0-9A-Z_()+,-.:=@;$!*\\'%/?#\\[\\]]+)',\n",
    "            re.IGNORECASE)\n",
    "        m = match(potential_urn, pttn)\n",
    "        return self._tidy_text(potential_urn) if m else ''\n",
    "\n",
    "    def _verify_url(self, potential_url):\n",
    "        # ugh\n",
    "        if 'mailto:' in potential_url:\n",
    "            return ''\n",
    "        \n",
    "        # is it a urn?\n",
    "        pttn = re.compile(ur'^([a-z0-9]{0,}:[a-z0-9][a-z0-9-]{0,31}:[a-z0-9()+,\\-.:=@;$_!*\\'%/?#\\[\\]]+)', re.IGNORECASE)\n",
    "        urn_m = match(potential_url, pttn)\n",
    "        if urn_m:\n",
    "            return ''\n",
    "\n",
    "        # does it even have a scheme?\n",
    "        try:\n",
    "            u = uparse(potential_url, rule='URI')\n",
    "            if not u.get('scheme'):\n",
    "                # will consider the leading blob of a URN\n",
    "                # a scheme even if we don't so this is not \n",
    "                # 100% reliable\n",
    "                return ''\n",
    "            \n",
    "            parts = potential_url.split(':', 1)\n",
    "            if not parts[1].startswith('/') or len(parts) < 2:\n",
    "                # so if it's our false urn scheme, we\n",
    "                # say it has to be :// or it's not a url\n",
    "                # and it has to have a split\n",
    "                return ''\n",
    "        except Exception as ex:\n",
    "            return ''\n",
    "\n",
    "        return self._tidy_text(potential_url)\n",
    "\n",
    "    def _extract_url(self, text):\n",
    "        # but really first, is it a urn?\n",
    "        text = self._verify_url(text)\n",
    "        if not text:\n",
    "            return '', []\n",
    "        url = self._tidy_text(unquote(text))\n",
    "        base_url, values = break_url(url)\n",
    "        values = values.split(' ') + [base_url] if base_url else []\n",
    "\n",
    "        # return the original extracted url, and the values plus \n",
    "        # the base_url for more extracting\n",
    "        return url, filter(None, [self._tidy_text(v) for v in values])\n",
    "\n",
    "    def _extract_identifiers(self, text):\n",
    "        # make sure it's not a date first\n",
    "        text = self._strip_dates(text)\n",
    "        if not text:\n",
    "            yield '', ''\n",
    "        for pattern_type, pattern in _pattern_set:\n",
    "            m = match(text, pattern)\n",
    "            if not m:\n",
    "                continue\n",
    "\n",
    "            # if the pattern type doesn't match the\n",
    "            # verification ie urn is not actually urn\n",
    "            # bounce (this is mostly for urls)\n",
    "            urn_verified = self._verify_urn(m)\n",
    "            is_urn = urn_verified != ''\n",
    "            if urn_verified:\n",
    "                yield 'urn', m\n",
    "            \n",
    "            if pattern_type == 'urn' and not is_urn:\n",
    "                continue\n",
    "                \n",
    "            url_verified = self._verify_url(m)\n",
    "            is_url = url_verified.strip() != ''\n",
    "            if is_url and not is_urn:\n",
    "                yield 'url', m\n",
    " \n",
    "            if pattern_type == 'url' and not is_url:\n",
    "                continue\n",
    "            \n",
    "            m = self._tidy_text(m)\n",
    "            if not m:\n",
    "                continue\n",
    "            yield pattern_type, m\n",
    "\n",
    "    def _check(self, comparison_identifier, arr):\n",
    "        return len([a for a in arr if a.has(comparison_identifier)]) > 0\n",
    "\n",
    "    def process_text(self):\n",
    "        def _append(potential_identifier):\n",
    "            # clean it up (make sure it's still a good thing)\n",
    "            # before checking against knowns and appending\n",
    "            cleaned_text = self._tidy_text(potential_identifier.potential_identifier)\n",
    "            if cleaned_text:\n",
    "                potential_identifier.potential_identifier = cleaned_text\n",
    "            if cleaned_text and not self._check(\n",
    "                    cleaned_text, self.identifieds):\n",
    "                self.identifieds.append(potential_identifier)\n",
    "            if not self._check(potential_identifier.original_text, self.seen_texts):\n",
    "                self.texts.append((potential_identifier.tag, cleaned_text))\n",
    "                self.seen_texts.append(potential_identifier)\n",
    "        \n",
    "        while self.texts:\n",
    "            tag, text = self.texts.pop()\n",
    "            if self._check(text, self.seen_texts) or not text.strip():\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                j = json.loads(text)\n",
    "                j.keys()  # it will decode a quoted string without error\n",
    "                continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # check the tag against the rule set \n",
    "            # we don't need to worry about xpaths\n",
    "            # just a bit of string matching - we've\n",
    "            # extracted the tags\n",
    "            if any(r[1] in tag for r in _rule_set):\n",
    "                # see if it matches a pattern \n",
    "                # better to know it's a doi or url\n",
    "                for match_type, match_text in self._extract_identifiers(text):\n",
    "                    if not match_type and not match_text:\n",
    "                        continue\n",
    "                    # add it if it matches any pattern\n",
    "                    _append(Identifier(tag, 'rule_set', match_type, text, match_text))\n",
    "                    \n",
    "                # if nothing else is a match, just add the text, as is\n",
    "                _append(Identifier(tag, 'rule_set', 'text', text, text))\n",
    "                    \n",
    "            url, values = self._extract_url(text)\n",
    "            values = [v for v in values if not self._check(v, self.seen_texts)]\n",
    "            self.texts += list(iter(izip([tag] * len(values), values)))\n",
    "                \n",
    "            if url and not self._check(url, self.identifieds) \\\n",
    "                    and not self._check(url, self.seen_texts):\n",
    "                _append(Identifier(tag, 'extract', 'url', text, url))\n",
    "\n",
    "            # now run the OTHER regex\n",
    "            for match_type, match_text in self._extract_identifiers(text):\n",
    "                if not match_type and not match_text:\n",
    "                    continue\n",
    "                _append(Identifier(tag, 'regex', match_type, text, match_text))\n",
    "                    \n",
    "        # exclude from here\n",
    "        for i, j in enumerate(self.identifieds):\n",
    "            # type tag text\n",
    "            text = self._strip_excludes(j.match_type, j.tag, j.potential_identifier)\n",
    "            if text:\n",
    "                yield j\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# exclude lists\n",
    "\n",
    "# the equality excludes \n",
    "equality = []\n",
    "files = ['namespaces.txt']\n",
    "for f in files:\n",
    "    equality += load_token_list(f)\n",
    "    \n",
    "# equality = ['http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml']\n",
    "    \n",
    "contains = []\n",
    "files = ['cat_interop_urns.txt', 'mimetypes.txt', 'namespaces.txt', 'excludes_by_contains.txt']\n",
    "for f in files:\n",
    "    contains += load_token_list(f)\n",
    "\n",
    "tag_paths = ['@codeList', '@schemaLocation', 'identifier/@type', \n",
    "             'Details/Licence', 'binding/@transport', '@template',\n",
    "            '@uri-type', 'rights/@resource', 'digform/digtinfo/formspec',\n",
    "            'license/@href', 'definition/@href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grab the clean text from the rds\n",
    "with open('../local/big_rds.conf', 'r') as f:\n",
    "    conf = js.loads(f.read())\n",
    "\n",
    "# our connection\n",
    "engine = sqla.create_engine(conf.get('connection'))\n",
    "Session = sessionmaker()\n",
    "Session.configure(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response_id = 599943\n",
    "response_url, response_cleaned, = session.query(\n",
    "    Response.source_url,\n",
    "    Response.cleaned_content\n",
    ").filter(Response.id==response_id).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extractor = IdentifierExtractor(response_url, response_cleaned, equality, contains, tag_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extractor._parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extractor.texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in extractor.process_text():\n",
    "    print i.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's run some identifiers (a few k to see what borks, etc)\n",
    "\n",
    "session.rollback()\n",
    "\n",
    "with open('../local/local_pg.conf', 'r') as f:\n",
    "    local_conf = js.loads(f.read())\n",
    "\n",
    "# our connection\n",
    "local_engine = sqla.create_engine(local_conf.get('connection'))\n",
    "local_Session = sessionmaker()\n",
    "local_Session.configure(bind=local_engine)\n",
    "local_session = local_Session()\n",
    "\n",
    "sketchy_sql ='''WITH t AS (SELECT *, row_number() OVER () AS rn FROM responses WHERE format = 'xml')\n",
    "SELECT * FROM (\n",
    "    SELECT trunc(random() * (SELECT max(rn) FROM t))::int + 1 AS rn\n",
    "    FROM   generate_series(1, 200) g\n",
    "    ) r\n",
    "JOIN   t USING (rn);'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the model\n",
    "import sqlalchemy as sqla\n",
    "from sqlalchemy import (\n",
    "    MetaData,\n",
    "    Column,\n",
    "    String,\n",
    "    DateTime,\n",
    "    Integer,\n",
    "    Boolean,\n",
    "    ForeignKey,\n",
    ")\n",
    "from sqlalchemy.dialects.postgresql import *\n",
    "from sqlalchemy.orm import relationship, backref\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "local_Base = declarative_base()\n",
    "\n",
    "class Extracted(local_Base):\n",
    "    __tablename__ = 'identifiers'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    source_url = Column(String)\n",
    "    tag = Column(String)\n",
    "    extraction_type = Column(String)\n",
    "    match_type = Column(String)\n",
    "    original_text = Column(String)\n",
    "    potential_identifier = Column(String)\n",
    "    response_id = Column(Integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fails = []\n",
    "for result in session.execute(sketchy_sql):\n",
    "#     print result['id']\n",
    "    identifiers = []\n",
    "    try:\n",
    "        extractor = IdentifierExtractor(result['source_url'], result['cleaned_content'], equality, contains, tag_paths)\n",
    "        for i in extractor.process_text():\n",
    "            identifiers.append(i.to_json())\n",
    "    except:\n",
    "        fails.append(result['id'])\n",
    "        continue\n",
    "    \n",
    "    for e in extractor.process_text():\n",
    "        ex = Extracted(\n",
    "            source_url=result['source_url'],\n",
    "            tag=e.tag,\n",
    "            extraction_type=e.extraction_type,\n",
    "            match_type=e.match_type,\n",
    "            original_text=e.original_text,\n",
    "            potential_identifier=e.potential_identifier,\n",
    "            response_id=result['id']\n",
    "        )\n",
    "        try:\n",
    "            local_session.add(ex)\n",
    "            local_session.commit()\n",
    "        except Exception as a:\n",
    "            print 'failed identifier:', e.to_json()\n",
    "            print a\n",
    "            local_session.rollback()\n",
    "            continue\n",
    "        \n",
    "        \n",
    "len(fails)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Clear URN tests\n",
    "\n",
    "or: the random set is good for spotting certain types of things, but it is unclear (ie, the point of this) how frequently urns turn up. so is the sample missing the few urns (that aren't epsg codes or something) or missing a ton?\n",
    "\n",
    "no rule_sets here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('inputs/identifier_exemplars.xml', 'r') as f:\n",
    "    response_cleaned = f.read().encode('utf-8')\n",
    "\n",
    "response_url = \"http://noaa.gov/some/path/to/nowhere\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extractor = IdentifierExtractor(response_url, response_cleaned, equality, contains, tag_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extractor._parse()\n",
    "extractor.texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url extract start/urls/third http://e4ftl01.cr.usgs.gov/MOLT/MOD13Q1.005/2002.05.25/MOD13Q1.A2002145.h14v04.005.2007156103919.hdf.xml\n",
      "url extract start/urls/second http://www.nbmg.unr.edu/dox/of0330.pdf\n",
      "url extract start/urls/first http://pubs.usgs.gov/ds/618/data_files/fs/fs_e410_n3438_16/fs_e410_n3438_16.las.xml\n",
      "urn regex start/text/urn/fifth urn:uuid:6426c02b-f2b1-4326-a767-2384c303faf3\n",
      "uuid regex start/text/urn/fifth 6426c02b-f2b1-4326-a767-2384c303faf3\n",
      "urn regex start/text/urn/fourth clarin.eu:cr1:p_1271859438204\n",
      "urn regex start/text/urn/third oai:www.mpi.nl:MPI110411\n",
      "urn regex start/text/urn/first urn:nbn:nl:ui:13-01hb-3b\n",
      "urn regex start/alone/urn/second oai:easy.dans.knaw.nl:easy-dataset:53685\n",
      "url extract  http://noaa.gov/some/path/to/nowhere\n"
     ]
    }
   ],
   "source": [
    "for i in extractor.process_text():\n",
    "    print i.match_type, i.extraction_type, i.tag, i.potential_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
