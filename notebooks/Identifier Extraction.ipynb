{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is demonstration only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json as js  # name conflict with sqla\n",
    "import sqlalchemy as sqla\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "from mpp.models import Response\n",
    "# from semproc.unique_identifiers import IdentifierExtractor\n",
    "\n",
    "import re\n",
    "from semproc.bag_parser import BagParser\n",
    "from semproc.nlp_utils import load_token_list\n",
    "from semproc.utils import unquote, break_url, match\n",
    "import dateutil.parser as dateparser\n",
    "from datetime import datetime\n",
    "from itertools import chain, izip\n",
    "from rfc3987 import parse as uparse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for sorting out excludes\n",
    "\n",
    "_pattern_set = [\n",
    "    ('url', re.compile(ur\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\", re.IGNORECASE)),\n",
    "    # a urn that isn't a url\n",
    "    ('urn', re.compile(ur\"(?!(?:http://))(?!(?:https://))(?!(?:ftp://))(([a-z0-9.\\S][a-z0-9-.\\S]{0,}\\S:{1,2}\\S)+[a-z0-9()+,\\-.=@;$_!*'%/?#]+)(?=.*/)\", re.IGNORECASE)),\n",
    "    # ('urn', re.compile(ur\"\\burn:[a-z0-9][a-z0-9-]{0,31}:[a-z0-9()+,\\-.:=@;$_!*'%/?#]+\", re.IGNORECASE)),\n",
    "    ('uuid', re.compile(ur'([a-f\\d]{8}(-[a-f\\d]{4}){3}-[a-f\\d]{12}?)', re.IGNORECASE)),\n",
    "    ('doi', re.compile(ur\"(10[.][0-9]{4,}(?:[/][0-9]+)*/(?:(?![\\\"&\\\\'])\\S)+)\", re.IGNORECASE)),\n",
    "    ('md5', re.compile(ur\"(?=(\\b[A-Fa-f0-9]{32}\\b))\", re.IGNORECASE))\n",
    "]\n",
    "\n",
    "_rule_set = [\n",
    "    ('uri', 'fileIdentifier/CharacterString'),  # ISO\n",
    "    ('uri', 'identifier/*/code/CharacterString'),\n",
    "    ('uri', 'dataSetURI/CharacterString'),\n",
    "    ('uri', 'parentIdentifier/CharacterString'),\n",
    "    ('uri', 'Entry_ID'),  # DIF\n",
    "    ('uri', 'dc/identifier'),  # DC\n",
    "    ('basic', 'Layer/Name'),  # WMS\n",
    "    ('basic', 'dataset/@ID'),  # THREDDS\n",
    "    ('uri', '@URI'),  # ddi\n",
    "    ('uri', '@IDNo')  # ddi\n",
    "]\n",
    "\n",
    "\n",
    "class Identifier(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            tag,\n",
    "            extraction_type,\n",
    "            match_type,\n",
    "            original_text,\n",
    "            potential_identifier):\n",
    "        self.tag = tag\n",
    "        self.extraction_type = extraction_type\n",
    "        self.match_type = match_type\n",
    "        self.original_text = original_text\n",
    "        self.potential_identifier = potential_identifier\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (isinstance(other, self.__class__)\n",
    "                and self.__dict__ == other.__dict__)\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return not self.__eq__(other)\n",
    "\n",
    "    def has(self, comparison_identifier):\n",
    "        return self.potential_identifier == comparison_identifier\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<Identifier %s>' % js.dumps(self.__dict__)\n",
    "\n",
    "    def to_json(self):\n",
    "        return {\n",
    "            \"tag\": self.tag,\n",
    "            \"extraction_type\": self.extraction_type,\n",
    "            \"match_type\": self.match_type,\n",
    "            \"original_text\": self.original_text,\n",
    "            \"potential_identifier\": self.potential_identifier\n",
    "        }\n",
    "\n",
    "\n",
    "class IdentifierExtractor(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            source_url,\n",
    "            source_xml_as_str,\n",
    "            equality_excludes=[],\n",
    "            contains_excludes=[],\n",
    "            tag_excludes=[]\n",
    "        ):\n",
    "        self.source_url = source_url\n",
    "        self.source_xml_as_str = source_xml_as_str\n",
    "        self.identifieds = []\n",
    "        self.texts = [('', self.source_url)]\n",
    "        self.seen_texts = []\n",
    "        self.equality_excludes = equality_excludes\n",
    "        self.contains_excludes = contains_excludes\n",
    "        self.tag_excludes = tag_excludes\n",
    "\n",
    "        self._parse()\n",
    "\n",
    "    def _parse(self):\n",
    "        try:\n",
    "            parser = BagParser(self.source_xml_as_str, True, False)\n",
    "        except Exception as ex:\n",
    "            print ex\n",
    "            raise ex\n",
    "        if not parser or parser.parser.xml is None:\n",
    "            raise Exception('failed to parse')\n",
    "\n",
    "        for tag, txt in parser.strip_text():\n",
    "            self.texts += ((tag, t) for t in txt.split())\n",
    "\n",
    "    def _strip_punctuation(self, text):\n",
    "        terminal_punctuation = '(){}[].,~|\":&-<>'\n",
    "        text = text.strip(terminal_punctuation)\n",
    "        return text.strip()\n",
    "\n",
    "    def _strip_dates(self, text):\n",
    "        # this should still make it an invalid date\n",
    "        # text = text[3:] if text.startswith('NaN') else text\n",
    "        def try_format(fmt):\n",
    "            try:\n",
    "                d = datetime.strptime(text, fmt)\n",
    "            except:\n",
    "                return False\n",
    "            return True\n",
    "                \n",
    "        try:\n",
    "            d = dateparser.parse(text)\n",
    "            return ''\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        known_formats = ['%d/%m/%Y [%H:%M:%S:%f]']\n",
    "        tests = [try_format(kf) for kf in known_formats]\n",
    "        return '' if sum(tests) > 0 else text\n",
    "\n",
    "    def _strip_scales(self, text):\n",
    "        scale_pttn = ur\"(1:[\\d]{0,}(,[\\d]{3}){1,})\"\n",
    "        m = match(text, scale_pttn)\n",
    "        if m:\n",
    "            return ''\n",
    "        return text\n",
    "\n",
    "    def _strip_excludes(self, match_type, tag, text):\n",
    "        # TODO: how to handle the excludes for skipping\n",
    "        #       by tag and skipping by value and what was\n",
    "        #       the thinking a few months ago?\n",
    "        \n",
    "        # if it came from any known elem/attrib\n",
    "        if any([t.lower() in tag.lower() for t in self.tag_excludes]):\n",
    "            return ''\n",
    "        \n",
    "        # if it's a url, must equal an item\n",
    "        if match_type == 'url':\n",
    "            if text in self.equality_excludes:\n",
    "                # equality only\n",
    "                return ''\n",
    "            return text\n",
    "\n",
    "        # if it's *not* a url and contains any of these patterns\n",
    "        if any(e.lower() in text.lower() for e in self.contains_excludes):\n",
    "            return ''\n",
    "        return text\n",
    "\n",
    "    def _strip_whitespace(self, text):\n",
    "        space_pattern = re.compile(' ')\n",
    "        if space_pattern.subn(' ', text)[1] > 0:\n",
    "            # i actually don't know if this is the right index. huh.\n",
    "            return text.split(' ')[0]\n",
    "        return text\n",
    "\n",
    "    def _tidy_text(self, text):\n",
    "        text = self._strip_punctuation(text)\n",
    "        if not text:\n",
    "            return ''\n",
    "\n",
    "        text = self._strip_scales(text)\n",
    "        if not text:\n",
    "            return ''\n",
    "\n",
    "        text = self._strip_dates(text)\n",
    "        if not text:\n",
    "            return ''\n",
    "\n",
    "        text = self._strip_whitespace(text)\n",
    "        if not text:\n",
    "            return ''\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _extract_url(self, text):\n",
    "        # check first to see if it's a valid uri\n",
    "        try:\n",
    "            u = uparse(text, rule='URI')\n",
    "            if not u.get('scheme'):\n",
    "                raise Exception('')\n",
    "                \n",
    "            m = text\n",
    "        except:\n",
    "            pttn = re.compile(ur\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\",\n",
    "                              re.IGNORECASE)\n",
    "            m = match(text, pttn)\n",
    "            \n",
    "        if not m:\n",
    "            return '', []\n",
    "\n",
    "        url = self._tidy_text(unquote(m))\n",
    "        \n",
    "        # check for a scheme\n",
    "        try:\n",
    "            u = uparse(url, rule='URI')\n",
    "            if not u.get('scheme'):\n",
    "                return '', []\n",
    "        except Exception as ex:\n",
    "            return '', []\n",
    "        \n",
    "        base_url, values = break_url(url)\n",
    "        values = values.split(' ') + [base_url]\n",
    "\n",
    "        # return the original extracted url, and the values plus \n",
    "        # the base_url for more extracting\n",
    "        return url, filter(None, [self._tidy_text(v) for v in values])\n",
    "\n",
    "    def _extract_identifiers(self, text):\n",
    "        for pattern_type, pattern in _pattern_set:\n",
    "            m = match(text, pattern)\n",
    "            if not m:\n",
    "                continue\n",
    "            if pattern_type != 'url':\n",
    "                try:\n",
    "                    # if the original is a URL\n",
    "                    # it's a false positive URN.\n",
    "                    u = uparse(text, rule='URI')\n",
    "                    continue\n",
    "                except:\n",
    "                    pass\n",
    "            if pattern_type=='url':\n",
    "                try:\n",
    "                    u = uparse(m, rule='URI')\n",
    "                    if not u.get('scheme'):\n",
    "                        continue\n",
    "                except Exception as ex:\n",
    "                    continue\n",
    "                \n",
    "            if 'mailto:' in m:\n",
    "                continue\n",
    "            \n",
    "            yield pattern_type, m\n",
    "\n",
    "    def _check(self, comparison_identifier, arr):\n",
    "        return len([a for a in arr if a.has(comparison_identifier)]) > 0\n",
    "\n",
    "    def process_text(self):\n",
    "        while self.texts:\n",
    "            tag, text = self.texts.pop()\n",
    "            if self._check(text, self.seen_texts) or not text.strip():\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                j = json.loads(text)\n",
    "                j.keys()  # it will decode a quoted string without error\n",
    "                continue\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            url, values = self._extract_url(text)\n",
    "            values = [v for v in values if not self._check(v, self.seen_texts)]\n",
    "            self.texts += list(iter(izip([tag] * len(values), values)))\n",
    "                \n",
    "            if url and not self._check(url, self.identifieds) \\\n",
    "                    and not self._check(url, self.seen_texts):\n",
    "                identify = Identifier(tag, 'regex', 'url', text, url)\n",
    "                self.identifieds.append(identify)\n",
    "                self.seen_texts.append(identify)\n",
    "\n",
    "            # now run the OTHER regex\n",
    "            for match_type, match_text in self._extract_identifiers(text):\n",
    "                # clean it up (make sure it's still a good thing)\n",
    "                # before checking against knowns and appending\n",
    "                cleaned_text = self._tidy_text(match_text)\n",
    "\n",
    "                if cleaned_text and not self._check(\n",
    "                        cleaned_text, self.identifieds):\n",
    "                    self.identifieds.append(\n",
    "                        Identifier(\n",
    "                            tag, 'regex', match_type, text, cleaned_text\n",
    "                        )\n",
    "                    )\n",
    "                if not self._check(cleaned_text, self.seen_texts):\n",
    "                    self.texts.append((tag, cleaned_text))\n",
    "                    self.seen_texts.append(\n",
    "                        Identifier(\n",
    "                            tag, 'regex', match_type, text, cleaned_text)\n",
    "                    )\n",
    "                    \n",
    "        # exclude from here\n",
    "        for i in self.identifieds:\n",
    "            # type tag text\n",
    "            text = self._strip_excludes(i.match_type, i.tag, i.potential_identifier)\n",
    "            if text:\n",
    "                yield i\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# exclude lists\n",
    "\n",
    "# the equality excludes \n",
    "equality = []\n",
    "files = ['namespaces.txt']\n",
    "for f in files:\n",
    "    equality += load_token_list(f)\n",
    "    \n",
    "equality = ['http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml']\n",
    "    \n",
    "contains = []\n",
    "files = ['cat_interop_urns.txt', 'mimetypes.txt', 'namespaces.txt', 'excludes_by_contains.txt']\n",
    "for f in files:\n",
    "    contains += load_token_list(f)\n",
    "\n",
    "tag_paths = ['@codeList', '@schemaLocation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def strip_excludes(match_type, tag, text):\n",
    "    # TODO: how to handle the excludes for skipping\n",
    "    #       by tag and skipping by value and what was\n",
    "    #       the thinking a few months ago?\n",
    "\n",
    "    # if it came from any known elem/attrib\n",
    "    if any([t.lower() in tag.lower() for t in tag_paths]):\n",
    "        return ''\n",
    "\n",
    "    # if it's a url, must equal an item\n",
    "    if match_type == 'url':\n",
    "        if text in equality:\n",
    "            # equality only\n",
    "            return ''\n",
    "        return text\n",
    "\n",
    "    # if it's *not* a url and contains any of these patterns\n",
    "    if any(e.lower() in text.lower() for e in contains):\n",
    "        return ''\n",
    "    return text\n",
    "\n",
    "strip_excludes(\n",
    "    'url',\n",
    "    'MD_Metadata/dataQualityInfo/DQ_DataQuality/scope/DQ_Scope/level/MD_ScopeCode',\n",
    "    'http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grab the clean text from the rds\n",
    "with open('../local/big_rds.conf', 'r') as f:\n",
    "    conf = js.loads(f.read())\n",
    "\n",
    "# our connection\n",
    "engine = sqla.create_engine(conf.get('connection'))\n",
    "Session = sessionmaker()\n",
    "Session.configure(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response_id = 599943\n",
    "response_url, response_cleaned, = session.query(\n",
    "    Response.source_url,\n",
    "    Response.cleaned_content\n",
    ").filter(Response.id==response_id).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extractor = IdentifierExtractor(response_url, response_cleaned, equality, contains, tag_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original_text': u'srm://esg.lbl.gov:6288/srm/v2/server?SFN=/garchive.nersc.gov/', 'extraction_type': 'regex', 'tag': 'catalog/service/@base', 'match_type': 'url', 'potential_identifier': u'srm://esg.lbl.gov:6288/srm/v2/server?SFN=/garchive.nersc.gov/'}\n",
      "{'original_text': u'srm://esg.lbl.gov:6288/srm/v2/server', 'extraction_type': 'regex', 'tag': 'catalog/service/@base', 'match_type': 'url', 'potential_identifier': u'srm://esg.lbl.gov:6288/srm/v2/server'}\n",
      "{'original_text': u'srm://esg2-sdnl1.ccs.ornl.gov:46790/srm/v2/server?SFN=/esg2-sdnl1.ccs.ornl.gov/', 'extraction_type': 'regex', 'tag': 'catalog/service/@base', 'match_type': 'url', 'potential_identifier': u'srm://esg2-sdnl1.ccs.ornl.gov:46790/srm/v2/server?SFN=/esg2-sdnl1.ccs.ornl.gov/'}\n",
      "{'original_text': u'srm://esg2-sdnl1.ccs.ornl.gov:46790/srm/v2/server', 'extraction_type': 'regex', 'tag': 'catalog/service/@base', 'match_type': 'url', 'potential_identifier': u'srm://esg2-sdnl1.ccs.ornl.gov:46790/srm/v2/server'}\n",
      "{'original_text': u'srm://vetsarchiveprod.ucar.edu:49582/srm/v2/server?SFN=/vetsarchiveprod.ucar.edu/', 'extraction_type': 'regex', 'tag': 'catalog/service/@base', 'match_type': 'url', 'potential_identifier': u'srm://vetsarchiveprod.ucar.edu:49582/srm/v2/server?SFN=/vetsarchiveprod.ucar.edu/'}\n",
      "{'original_text': u'srm://vetsarchiveprod.ucar.edu:49582/srm/v2/server', 'extraction_type': 'regex', 'tag': 'catalog/service/@base', 'match_type': 'url', 'potential_identifier': u'srm://vetsarchiveprod.ucar.edu:49582/srm/v2/server'}\n",
      "{'original_text': u'gsiftp://vetsman.ucar.edu:2811/', 'extraction_type': 'regex', 'tag': 'catalog/service/service/@base', 'match_type': 'url', 'potential_identifier': u'gsiftp://vetsman.ucar.edu:2811/'}\n",
      "{'original_text': u'http://tds.ucar.edu/thredds/esgcet/65/ucar.cgd.ccsm4.CESM_CAM5_BGC_LE.ocn.proc.monthly_ave.ATM_ALT_CO2.v4.xml', 'extraction_type': 'regex', 'tag': '', 'match_type': 'url', 'potential_identifier': u'http://tds.ucar.edu/thredds/esgcet/65/ucar.cgd.ccsm4.CESM_CAM5_BGC_LE.ocn.proc.monthly_ave.ATM_ALT_CO2.v4.xml'}\n"
     ]
    }
   ],
   "source": [
    "for i in extractor.process_text():\n",
    "    print i.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's run some identifiers (a few k to see what borks, etc)\n",
    "\n",
    "# session.rollback()\n",
    "\n",
    "with open('../local/local_pg.conf', 'r') as f:\n",
    "    local_conf = js.loads(f.read())\n",
    "\n",
    "# our connection\n",
    "local_engine = sqla.create_engine(local_conf.get('connection'))\n",
    "local_Session = sessionmaker()\n",
    "local_Session.configure(bind=local_engine)\n",
    "local_session = local_Session()\n",
    "\n",
    "sketchy_sql ='''WITH t AS (SELECT *, row_number() OVER () AS rn FROM responses WHERE format = 'xml')\n",
    "SELECT * FROM (\n",
    "    SELECT trunc(random() * (SELECT max(rn) FROM t))::int + 1 AS rn\n",
    "    FROM   generate_series(1, 100) g\n",
    "    ) r\n",
    "JOIN   t USING (rn);'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the model\n",
    "import sqlalchemy as sqla\n",
    "from sqlalchemy import (\n",
    "    MetaData,\n",
    "    Column,\n",
    "    String,\n",
    "    DateTime,\n",
    "    Integer,\n",
    "    Boolean,\n",
    "    ForeignKey,\n",
    ")\n",
    "from sqlalchemy.dialects.postgresql import *\n",
    "from sqlalchemy.orm import relationship, backref\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "local_Base = declarative_base()\n",
    "\n",
    "class Extracted(local_Base):\n",
    "    __tablename__ = 'identifiers'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    source_url = Column(String)\n",
    "    tag = Column(String)\n",
    "    extraction_type = Column(String)\n",
    "    match_type = Column(String)\n",
    "    original_text = Column(String)\n",
    "    potential_identifier = Column(String)\n",
    "    response_id = Column(Integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fails = []\n",
    "for result in session.execute(sketchy_sql):\n",
    "#     print result['id']\n",
    "    identifiers = []\n",
    "    try:\n",
    "        extractor = IdentifierExtractor(result['source_url'], result['cleaned_content'], equality, contains, tag_paths)\n",
    "        for i in extractor.process_text():\n",
    "            identifiers.append(i.to_json())\n",
    "    except:\n",
    "        fails.append(result['id'])\n",
    "        continue\n",
    "    \n",
    "    for e in extractor.process_text():\n",
    "        ex = Extracted(\n",
    "            source_url=result['source_url'],\n",
    "            tag=e.tag,\n",
    "            extraction_type=e.extraction_type,\n",
    "            match_type=e.match_type,\n",
    "            original_text=e.original_text,\n",
    "            potential_identifier=e.potential_identifier,\n",
    "            response_id=result['id']\n",
    "        )\n",
    "        try:\n",
    "            local_session.add(ex)\n",
    "            local_session.commit()\n",
    "        except Exception as a:\n",
    "            print 'failed identifier:', e.to_json()\n",
    "            print a\n",
    "            local_session.rollback()\n",
    "            continue\n",
    "        \n",
    "        \n",
    "len(fails)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for e in extractor.identifieds:\n",
    "    print e.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(extractor.identifieds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regex issues \n",
    "\n",
    "first leading 's' issue:\n",
    "\"SC:MOD11_L2.004:2009533550\" : \"C:MOD11_L2.004:2009533550\" (urn)\n",
    "\n",
    "dropping ftp on the url:\n",
    "\"ftp://coast.noaa.gov/pub/DigitalCoast/lidar1_z/geoid12a/data/1419\" : \"url\" : \"coast.noaa.gov/pub/DigitalCoast/lidar1_z/geoid12a/data/1419\"\n",
    "\n",
    "\n",
    "drop email first:\n",
    "\n",
    "\"apfo.sales@slc.usda.gov\" : \"url\" : \"usda.gov\"\n",
    "\n",
    "second leading 's' issue:\n",
    "\"spase://VMO/NumericalData/SAMBA/Escudero/Magnetometer/PT1S\" : \"urn\" : \"ase://VMO/NumericalData/SAMBA/Escudero/Magnetometer/PT1S\"\n",
    "\n",
    "\n",
    "md5 should be in whitespace or punctuation:\n",
    "\"f3f1ab2412999a87692116c2102bf9795c31e48dd231afced41bf444b7b5c0ed\" : \"md5\" : \"f3f1ab2412999a87692116c2102bf979\"\n",
    "\n",
    "many problems here:\n",
    "\"type:opendap.grid:http://stellwagen.er.usgs.gov:80/opendap/DEEP_REEF/6413pt-a.nc\" : \"urn\" :  \"ype:opendap.grid:http://stellwagen.er.usgs.gov:80/opendap/DEEP_REEF/6413pt-a.nc\"\n",
    "\n",
    "not it's not:\n",
    "\"/DEEP_REEF/6413pt-a.nc\" : \"url\" : \"6413pt-a.nc\"\n",
    "\n",
    "okay:\n",
    "\"http://hdl.handle.net/10022/AC:P:13943\" : \"urn\" : \"dl.handle.net/10022/AC:P:13943\"\n",
    "\n",
    "\n",
    "not a urn??\n",
    "\"info:oai:www.esrl.noaa.gov:728\";\"www.esrl.noaa.gov\"\n",
    "\n",
    "\n",
    "\"regex\";\"url\";\"dans:oai:easy.dans.knaw.nl:easy-dataset:54740\";\"easy.dans.knaw.nl\";585468\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'authority': 'standards.iso.org',\n",
       " 'fragment': 'MD_ScopeCode',\n",
       " 'path': '/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml',\n",
       " 'query': None,\n",
       " 'scheme': 'http'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uparse(\"http://standards.iso.org/ittf/PubliclyAvailableStandards/ISO_19139_Schemas/resources/Codelist/gmxCodelists.xml#MD_ScopeCode\", rule='URI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'srm'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = uparse(\"srm://esg.lbl.gov:6288/srm/v2/server?SFN=/garchive.nersc.gov/\", rule='URI')\n",
    "u.get('scheme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
