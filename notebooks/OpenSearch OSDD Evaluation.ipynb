{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Can we generate a valid search URL from the URL Template provided?\n",
    "    a. given the time frame of the harvests, this has to be considered alongside linkrot in the osdd (failed search query may not mean the template is incorrect, could just mean the service url is no longer valid at all).\n",
    "2. Can we identify best practices (uses esip spatial, uses the time namespace, uses parameter elements)?\n",
    "3. dataset/granule search (i don't think we have any data to support this at all).\n",
    "\n",
    "Other info - can you identify the parent osdd of a resultset or of a nested osdd? can you grok it from the url only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from lxml import etree\n",
    "import urlparse\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_namespaces(xml):\n",
    "    '''\n",
    "    Pull all of the namespaces in the source document\n",
    "    and generate a list of tuples (prefix, URI) to dict\n",
    "    '''\n",
    "    if xml is None:\n",
    "        return {}\n",
    "\n",
    "    document_namespaces = dict(xml.xpath('/*/namespace::*'))\n",
    "    if None in document_namespaces:\n",
    "        document_namespaces['default'] = document_namespaces[None]\n",
    "        del document_namespaces[None]\n",
    "\n",
    "    # now run through any child namespace issues\n",
    "    all_namespaces = xml.xpath('//namespace::*')\n",
    "    for i, ns in enumerate(all_namespaces):\n",
    "        if ns[1] in document_namespaces.values():\n",
    "            continue\n",
    "        new_key = ns[0] if ns[0] else 'default%s' % i\n",
    "        document_namespaces[new_key] = ns[1]\n",
    "\n",
    "    return document_namespaces\n",
    "\n",
    "def extract_urls(xml, mimetype='atom+xml'):\n",
    "    return xml.xpath('//*[local-name()=\"Url\" and (@*[local-name()=\"type\"]=\"application/%(mimetype)s\" or @*[local-name()=\"type\"]=\"text/%(mimetype)s\")]' % {'mimetype': mimetype})\n",
    "\n",
    "def extract_template(url, append_limit=True):\n",
    "    # get the base url from the template\n",
    "    template_parts = urlparse.urlparse(url)\n",
    "    \n",
    "    if not template_parts.scheme:\n",
    "        return '', '', {}, False\n",
    "    \n",
    "    base_url = urlparse.urlunparse((\n",
    "        template_parts.scheme,\n",
    "        template_parts.netloc,\n",
    "        template_parts.path,\n",
    "        None,\n",
    "        None,\n",
    "        None\n",
    "    ))\n",
    "\n",
    "    qp = {k: v[0] for k, v in urlparse.parse_qs(template_parts.query).iteritems()}\n",
    "\n",
    "    # get the hard-coded params\n",
    "    defaults = {k:v for k, v in qp.iteritems() \n",
    "            if not v.startswith('{') \n",
    "            and not v.endswith('}')}\n",
    "    \n",
    "    # a flag for some hard-coded response format type to manage\n",
    "    # accept headers or no\n",
    "    format_defined = len([v for k, v in defaults.iteritems() if 'atom' in v.lower() or 'rss' in v.lower()]) > 0\n",
    "\n",
    "    # get the rest (and ignore the optional/namespaces)\n",
    "    parameters = {k: v[1:-1] for k, v in qp.iteritems() \n",
    "            if v.startswith('{') \n",
    "            and v.endswith('}')}\n",
    "    \n",
    "    if append_limit:\n",
    "        terms = extract_parameter_key('count', parameters)\n",
    "        if terms:\n",
    "            defaults = dict(\n",
    "                chain(defaults.items(), {k: 5 for k in terms.keys()}.items())\n",
    "            )\n",
    "            \n",
    "    # note: not everyone manages url-encoded query parameter delimiters\n",
    "    #       and not everyone manages non-url-encoded values so yeah. we are\n",
    "    #       ignoring the non-url-encoded group tonight.\n",
    "    # return the base, defaults, parameters as dict\n",
    "    return base_url, defaults, parameters, format_defined\n",
    "\n",
    "def extract_parameter_key(value, params):\n",
    "    # sort out the query parameter name for a parameter\n",
    "    # and don't send curly bracketed things, please\n",
    "    return {k: v.split(':')[-1].replace('?', '') for k, v \n",
    "                in params.iteritems() \n",
    "                if value in v}\n",
    "\n",
    "def extract_parameter_defs(url_elem, defined_terms):\n",
    "    # could just go with a namespace check but\n",
    "    # namespaces are included and not used more\n",
    "    # than i'd like. safety first.\n",
    "    params = url_elem.xpath('*[local-name()=\"Parameter\"]')\n",
    "    \n",
    "    # and go crazy pedant with a) does each query param value\n",
    "    # have a defined parameter element? (don't know if it should) \n",
    "    # and b) required or not parameters?\n",
    "    for p in params:\n",
    "        p_value = p.attrib.get('value', '')\n",
    "        if not p_value:\n",
    "            continue\n",
    "        \n",
    "        qps = {k:v for k, v in defined_terms.iteritems() if p_value[1:-1] in v}\n",
    "        \n",
    "        if not qps:\n",
    "            continue\n",
    "            \n",
    "def extract_query_terms(xml, param_name):\n",
    "    # find a query element that contains an example\n",
    "    # for the provided param_name (no namespace, no optional flag)\n",
    "    example_queries = {}\n",
    "    xp = '//*[local-name()=\"Query\" and @*[local-name()=\"role\"]=\"example\"]/@*[local-name()=\"{0}\"]'.format(param_name)\n",
    "    try:\n",
    "        example_queries = xml.xpath(xp)\n",
    "    except:\n",
    "        print 'failed example query: ', xp\n",
    "        return []\n",
    "    \n",
    "    return example_queries\n",
    "\n",
    "def extract_search_rel(xml):\n",
    "    elem = next(\n",
    "        iter(\n",
    "                xml.xpath('/*/*[local-name()=\"link\" and (@*[local-name()=\"rel\"]=\"search\" or @*[local-name()=\"rel\"]=\"http://esipfed.org/ns/fedsearch/1.0/search#\")]')\n",
    "            ), None\n",
    "        )\n",
    "    return elem\n",
    "\n",
    "def extract_item_links(xml):\n",
    "    # item or entry links from a secondary search\n",
    "    return xml.xpath('//*[local-name()=\"entry\" or local-name()=\"item\"]/*[local-name()=\"link\"]')\n",
    "\n",
    "def extract_response_stats(xml):\n",
    "    total = next(iter(xml.xpath('//*[local-name()=\"totalResults\"]/text()')), 'Unknown')\n",
    "    subset = next(iter(xml.xpath('//*[local-name()=\"itemsPerPage\"]/text()')), 'Unknown')\n",
    "    \n",
    "    if subset == 'Unknown':\n",
    "        subset = len(xml.xpath('//*[local-name()=\"entry\" or local-name()=\"item\"]'))\n",
    "    \n",
    "    return subset, total\n",
    "\n",
    "def generate_requests(url_elem):\n",
    "    # rebuild the url(s)\n",
    "    url_base, defaults, params, format_defined = extract_template(url_elem.attrib.get('template'))\n",
    "    accept_type = url_elem.attrib.get('type', '')\n",
    "    \n",
    "    headers = {'Accept': accept_type} if format_defined == False else {}\n",
    "    \n",
    "    search_terms = extract_parameter_key('searchTerms', params)\n",
    "    \n",
    "    search_urls = []\n",
    "    \n",
    "    # build the empty search\n",
    "    if search_terms:\n",
    "        qps = dict(\n",
    "            chain(\n",
    "                defaults.items(),\n",
    "                {search_terms.keys()[0]: ''}.items()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        search_urls.append(url_base + '?' + urllib.urlencode(qps.items()))\n",
    "    \n",
    "    # try to build an example search\n",
    "    query_examples = list(\n",
    "        chain.from_iterable(\n",
    "            [extract_query_terms(url_elem.getparent(), p) for p in search_terms.values()]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if query_examples:\n",
    "        test_query = {search_terms.keys()[0]: query_examples[0]}\n",
    "        qps = dict(\n",
    "            chain(\n",
    "                defaults.items(),\n",
    "                test_query.items()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        search_urls.append(url_base + '?' + urllib.urlencode(qps.items()))\n",
    "    \n",
    "    return search_urls, headers\n",
    "\n",
    "def execute_request(url, headers={}):\n",
    "    try:\n",
    "        req = requests.get(url, headers=headers)\n",
    "    except:\n",
    "        logger.error('\\tSkipping connection issue\\'s')\n",
    "        return '-999', '', ''\n",
    "    \n",
    "    return req.status_code, req.content, req.headers\n",
    "\n",
    "def parse_response(content, headers={}):\n",
    "    # see if it has content, see if the xml parses, see if it's even xml\n",
    "    if not content:\n",
    "        return {'error': 'No content'}\n",
    "    \n",
    "    if 'html' in headers.get('content-type'):\n",
    "        return {'error': 'HTML response'}\n",
    "\n",
    "    try:\n",
    "        xml = etree.fromstring(content)\n",
    "    except:\n",
    "        return {'error': 'XML Parse error'}\n",
    "\n",
    "    return {'xml': xml}\n",
    "\n",
    "def parse_osdd(osdd):\n",
    "    # get the url template to test basic search\n",
    "    #    get the parameter list (prefix:term)\n",
    "    # get the parameter elements\n",
    "    #    match to parameter list\n",
    "    # get namespaces \n",
    "    output = {}\n",
    "    \n",
    "    output['namespaces'] = extract_namespaces(osdd)\n",
    "    output['templates'] = []\n",
    "    \n",
    "    for extracted_elem in extract_urls(osdd):\n",
    "        template_base, template_defaults, template_params, format_defined = extract_template(extracted_elem.attrib.get('template'))\n",
    "        accept_type = extracted_elem.attrib.get('type', '')\n",
    "        \n",
    "        output['templates'].append({\n",
    "            'base': template_base,\n",
    "            'defaults': template_defaults,\n",
    "            'parameters': template_params,\n",
    "            'format_definition': format_defined,\n",
    "            'accept_type': accept_type\n",
    "        })\n",
    "    \n",
    "    parameter_definitions = extract_parameter_defs(extracted_elem, template_params)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from doug, see notes re: uptime\n",
    "cwic_links = [\n",
    "    'http://dap.onc.uvic.ca/erddap/opensearch1.1/description.xml',\n",
    "    'http://gcmd.gsfc.nasa.gov/KeywordSearch/default/openSearch.jsp?Portal=cwic',\n",
    "    'http://podaac.jpl.nasa.gov/ws/search/dataset/osd.xml',\n",
    "    # 'http://nsidc.org/api/opensearch/1.1/dataset/description',  # we're just not going to run this\n",
    "    'http://ghrc.nsstc.nasa.gov/hydro/ghost.xml',\n",
    "    'http://mirador.gsfc.nasa.gov/mirador_dataset_opensearch.xml',\n",
    "    'http://eo-virtual-archive4.esa.int/search/ER02_SAR_RAW_0P/description',\n",
    "    'http://www1.usgs.gov/erddap/opensearch1.1/description.xml',\n",
    "    # 'http://bison.usgs.ornl.gov/doc/api.jsp',  # this is now a dead link\n",
    "    # 'http://ceocat.ccrs.nrcan.gc.ca/opensearch_description_document.xml',  # this is 403 access forbidden\n",
    "    # 'http://rs211980.rs.hosteurope.de/mule/os-description/',  # 503 service down\n",
    "    'http://geo.spacebel.be/opensearch/description.xml',  # from the fedeo documentation page listed\n",
    "    'http://lance-modis.eosdis.nasa.gov/user_services/dataset_opensearch.xml'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing http://dap.onc.uvic.ca/erddap/opensearch1.1/description.xml\n",
      "\tCatalog searchTerms:  {'searchTerms': 'searchTerms'}\n",
      "\tExample queries:  ['temperature']\n",
      "Processing http://gcmd.gsfc.nasa.gov/KeywordSearch/default/openSearch.jsp?Portal=cwic\n",
      "\tCatalog searchTerms:  {'searchTerms': 'searchTerms'}\n",
      "\tExample queries:  ['modis']\n",
      "Processing http://podaac.jpl.nasa.gov/ws/search/dataset/osd.xml\n",
      "\tCatalog searchTerms:  {'keyword': 'searchTerms'}\n",
      "\tExample queries:  []\n",
      "Processing http://ghrc.nsstc.nasa.gov/hydro/ghost.xml\n",
      "\tFailed request\n",
      "Processing http://mirador.gsfc.nasa.gov/mirador_dataset_opensearch.xml\n",
      "\tCatalog searchTerms:  {'keyword': 'searchTerms'}\n",
      "\tExample queries:  ['ozone', 'Surface Air Temperature']\n",
      "Processing http://eo-virtual-archive4.esa.int/search/ER02_SAR_RAW_0P/description\n",
      "\tCatalog searchTerms:  {'q': 'searchTerms'}\n",
      "\tExample queries:  []\n",
      "Processing http://www1.usgs.gov/erddap/opensearch1.1/description.xml\n",
      "\tCatalog searchTerms:  {'searchTerms': 'searchTerms'}\n",
      "\tExample queries:  ['datasetID']\n",
      "Processing http://geo.spacebel.be/opensearch/description.xml\n",
      "\tCatalog searchTerms:  {'query': 'searchTerms'}\n",
      "\tExample queries:  []\n",
      "\tCatalog searchTerms:  {'query': 'searchTerms'}\n",
      "\tExample queries:  []\n",
      "Processing http://lance-modis.eosdis.nasa.gov/user_services/dataset_opensearch.xml\n",
      "\tCatalog searchTerms:  {'pattern': 'searchTerms'}\n",
      "\tExample queries:  ['adjusted reflectance']\n"
     ]
    }
   ],
   "source": [
    "for cwic_link in cwic_links:\n",
    "    print 'Processing {0}'.format(cwic_link)\n",
    "    \n",
    "    req = requests.get(cwic_link)\n",
    "    if req.status_code != 200:\n",
    "        print '\\tFailed request'\n",
    "        continue\n",
    "    \n",
    "    xml = etree.fromstring(req.content)\n",
    "    \n",
    "    extracted_url_elems = extract_urls(xml)\n",
    "    \n",
    "    for extracted_elem in extracted_url_elems:\n",
    "        template_base, template_defaults, template_params, format_defined = extract_template(extracted_elem.attrib.get('template'))\n",
    "        accept_type = extracted_elem.attrib.get('type', '')\n",
    "        \n",
    "        # let's see if the url includes some enumerations as Parameter children\n",
    "        # parameter_definitions = extract_parameter_defs(extracted_elem, template_params)\n",
    "        \n",
    "        catalog_params = extract_parameter_key('searchTerms', template_params)\n",
    "        print '\\tCatalog searchTerms: ', catalog_params\n",
    "    \n",
    "        if not catalog_params:\n",
    "            continue\n",
    "        \n",
    "        query_examples = list(chain.from_iterable([extract_query_terms(xml, p) for p in catalog_params.values()]))\n",
    "        \n",
    "        print '\\tExample queries: ', query_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
