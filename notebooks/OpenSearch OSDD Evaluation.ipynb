{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Can we generate a valid search URL from the URL Template provided?\n",
    "    a. given the time frame of the harvests, this has to be considered alongside linkrot in the osdd (failed search query may not mean the template is incorrect, could just mean the service url is no longer valid at all).\n",
    "2. Can we identify best practices (uses esip spatial, uses the time namespace, uses parameter elements)?\n",
    "3. dataset/granule search (i don't think we have any data to support this at all).\n",
    "\n",
    "Other info - can you identify the parent osdd of a resultset or of a nested osdd? can you grok it from the url only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from lxml import etree\n",
    "import urlparse\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_namespaces(xml):\n",
    "    '''\n",
    "    Pull all of the namespaces in the source document\n",
    "    and generate a list of tuples (prefix, URI) to dict\n",
    "    '''\n",
    "    if xml is None:\n",
    "        return {}\n",
    "\n",
    "    document_namespaces = dict(xml.xpath('/*/namespace::*'))\n",
    "    if None in document_namespaces:\n",
    "        document_namespaces['default'] = document_namespaces[None]\n",
    "        del document_namespaces[None]\n",
    "\n",
    "    # now run through any child namespace issues\n",
    "    all_namespaces = xml.xpath('//namespace::*')\n",
    "    for i, ns in enumerate(all_namespaces):\n",
    "        if ns[1] in document_namespaces.values():\n",
    "            continue\n",
    "        new_key = ns[0] if ns[0] else 'default%s' % i\n",
    "        document_namespaces[new_key] = ns[1]\n",
    "\n",
    "    return document_namespaces\n",
    "\n",
    "def extract_urls(xml, mimetype='atom+xml'):\n",
    "    return xml.xpath('//*[local-name()=\"Url\" and (@*[local-name()=\"type\"]=\"application/%(mimetype)s\" or @*[local-name()=\"type\"]=\"text/%(mimetype)s\")]' % {'mimetype': mimetype})\n",
    "\n",
    "def extract_template(url, append_limit=True):\n",
    "    # get the base url from the template\n",
    "    template_parts = urlparse.urlparse(url)\n",
    "    \n",
    "    if not template_parts.scheme:\n",
    "        return '', '', {}, False\n",
    "    \n",
    "    base_url = urlparse.urlunparse((\n",
    "        template_parts.scheme,\n",
    "        template_parts.netloc,\n",
    "        template_parts.path,\n",
    "        None,\n",
    "        None,\n",
    "        None\n",
    "    ))\n",
    "\n",
    "    qp = {k: v[0] for k, v in urlparse.parse_qs(template_parts.query).iteritems()}\n",
    "\n",
    "    # get the hard-coded params\n",
    "    defaults = {k:v for k, v in qp.iteritems() \n",
    "            if not v.startswith('{') \n",
    "            and not v.endswith('}')}\n",
    "    \n",
    "    # a flag for some hard-coded response format type to manage\n",
    "    # accept headers or no\n",
    "    format_defined = len([v for k, v in defaults.iteritems() if 'atom' in v.lower() or 'rss' in v.lower()]) > 0\n",
    "\n",
    "    # get the rest (and ignore the optional/namespaces)\n",
    "    parameters = {k: v[1:-1] for k, v in qp.iteritems() \n",
    "            if v.startswith('{') \n",
    "            and v.endswith('}')}\n",
    "    \n",
    "    if append_limit:\n",
    "        terms = extract_parameter_key('count', parameters)\n",
    "        if terms:\n",
    "            defaults = dict(\n",
    "                chain(defaults.items(), {k: 5 for k in terms.keys()}.items())\n",
    "            )\n",
    "            \n",
    "    # note: not everyone manages url-encoded query parameter delimiters\n",
    "    #       and not everyone manages non-url-encoded values so yeah. we are\n",
    "    #       ignoring the non-url-encoded group tonight.\n",
    "    # return the base, defaults, parameters as dict\n",
    "    return base_url, defaults, parameters, format_defined\n",
    "\n",
    "def extract_parameter_key(value, params):\n",
    "    # sort out the query parameter name for a parameter\n",
    "    # and don't send curly bracketed things, please\n",
    "    return {k: v.split(':')[-1].replace('?', '') for k, v \n",
    "                in params.iteritems() \n",
    "                if value in v}\n",
    "\n",
    "def extract_parameter_defs(url_elem, defined_terms):\n",
    "    # could just go with a namespace check but\n",
    "    # namespaces are included and not used more\n",
    "    # than i'd like. safety first.\n",
    "    params = url_elem.xpath('*[local-name()=\"Parameter\"]')\n",
    "    \n",
    "    output = {}\n",
    "    for i, param in enumerate(params):\n",
    "        pname = param.attrib.get('name', i)\n",
    "        pval = param.attrib.get('value', '')\n",
    "        poptions = param.xpath('*[local-name()=\"Option\"]')\n",
    "        options = [(o.attrib.get('value'), o.attrib.get('label')) for o in poptions]\n",
    "        \n",
    "        output[pname] = {\n",
    "            \"value\": pval,\n",
    "            \"options\": options\n",
    "        }\n",
    "    \n",
    "    return output\n",
    "    \n",
    "    # and go crazy pedant with a) does each query param value\n",
    "    # have a defined parameter element? (don't know if it should) \n",
    "    # and b) required or not parameters?\n",
    "#     for p in params:\n",
    "#         p_value = p.attrib.get('value', '')\n",
    "#         if not p_value:\n",
    "#             continue\n",
    "        \n",
    "#         qps = {k:v for k, v in defined_terms.iteritems() if p_value[1:-1] in v}\n",
    "        \n",
    "#         if not qps:\n",
    "#             continue\n",
    "            \n",
    "def extract_query_terms(xml, param_name):\n",
    "    # find a query element that contains an example\n",
    "    # for the provided param_name (no namespace, no optional flag)\n",
    "    example_queries = {}\n",
    "    xp = '//*[local-name()=\"Query\" and @*[local-name()=\"role\"]=\"example\"]/@*[local-name()=\"{0}\"]'.format(param_name)\n",
    "    try:\n",
    "        example_queries = xml.xpath(xp)\n",
    "    except:\n",
    "        print 'failed example query: ', xp\n",
    "        return []\n",
    "    \n",
    "    return example_queries\n",
    "\n",
    "def extract_search_rels(xml):\n",
    "#     application/opensearchdescription+xml\n",
    "    for elem in xml.xpath('//*/*[local-name()=\"link\" and (@*[local-name()=\"type\"]=\"application/opensearchdescription+xml\") and (@*[local-name()=\"rel\"]=\"search\" or @*[local-name()=\"rel\"]=\"http://esipfed.org/ns/fedsearch/1.0/search#\")]'):\n",
    "        parent = next(iter(elem.getparent().xpath('*[local-name()=\"title\"]')), None)\n",
    "        yield {\n",
    "            \"link_url\": elem.attrib.get('href', ''),\n",
    "            \"link_title\": elem.attrib.get('title', ''),\n",
    "            \"link_type\": elem.attrib.get('type', ''),\n",
    "            \"parent_title\": parent.text if parent is not None else ''\n",
    "        }\n",
    "            \n",
    "\n",
    "def extract_response_stats(xml):\n",
    "    total = next(iter(xml.xpath('//*[local-name()=\"totalResults\"]/text()')), 'Unknown')\n",
    "    subset = next(iter(xml.xpath('//*[local-name()=\"itemsPerPage\"]/text()')), 'Unknown')\n",
    "    \n",
    "    return subset, total\n",
    "\n",
    "def execute_request(url, headers={}):\n",
    "    try:\n",
    "        req = requests.get(url, headers=headers)\n",
    "    except:\n",
    "        logger.error('\\tSkipping connection issue\\'s')\n",
    "        return '-999', '', ''\n",
    "    \n",
    "    return req.status_code, req.content, req.headers\n",
    "\n",
    "def parse_response(content, headers={}):\n",
    "    output = {}\n",
    "    \n",
    "    # see if it has content, see if the xml parses, see if it's even xml\n",
    "    if not content:\n",
    "        return {'error': 'No content'}\n",
    "    \n",
    "    if 'html' in headers.get('content-type'):\n",
    "        return {'error': 'HTML response'}\n",
    "\n",
    "    try:\n",
    "        xml = etree.fromstring(content)\n",
    "    except:\n",
    "        return {'error': 'XML Parse error'}\n",
    "\n",
    "    subset, total = extract_response_stats(xml)\n",
    "    \n",
    "    # this would get us to some nested search\n",
    "    # there is no guarantee it is dataset/granule!\n",
    "    # or can be identified as such!\n",
    "    search_rels = [e for e in extract_search_rels(xml)]\n",
    "    \n",
    "    output.update({\n",
    "        'subset': subset,\n",
    "        'total': total,\n",
    "    })\n",
    "    if search_rels is not None:\n",
    "        output.update({'search_rels': search_rels})\n",
    "        \n",
    "    return output\n",
    "\n",
    "def parse_osdd(osdd):\n",
    "    # get the url template to test basic search\n",
    "    #    get the parameter list (prefix:term)\n",
    "    # get the parameter elements\n",
    "    #    match to parameter list\n",
    "    # get namespaces \n",
    "    output = {}\n",
    "    \n",
    "    output['namespaces'] = extract_namespaces(osdd)\n",
    "    output['templates'] = []\n",
    "    \n",
    "    for extracted_elem in extract_urls(osdd):\n",
    "        template_base, template_defaults, template_params, format_defined = extract_template(extracted_elem.attrib.get('template'))\n",
    "        accept_type = extracted_elem.attrib.get('type', '')\n",
    "        \n",
    "        search_url = ''\n",
    "        search_terms = extract_parameter_key('searchTerms', template_params)\n",
    "    \n",
    "        if search_terms:\n",
    "            qps = dict(\n",
    "                chain(\n",
    "                    template_defaults.items(),\n",
    "                    {search_terms.keys()[0]: ''}.items()\n",
    "                )\n",
    "            )\n",
    "            search_url = template_base + '?' + urllib.urlencode(qps.items())\n",
    "        \n",
    "        example_url = ''\n",
    "        example_terms = list(\n",
    "            chain.from_iterable(\n",
    "                [extract_query_terms(extracted_elem.getparent(), s) for s in search_terms.values()]\n",
    "            )\n",
    "        )\n",
    "        if example_terms:\n",
    "            qps = dict(\n",
    "                chain(\n",
    "                    template_defaults.items(),\n",
    "                    {search_terms.keys()[0]: example_terms[0]}.items()\n",
    "                )\n",
    "            )\n",
    "            example_url = template_base + '?' + urllib.urlencode(qps.items())\n",
    "        \n",
    "        default_url = template_base + '?' + urllib.urlencode(template_defaults.items())\n",
    "        \n",
    "        output['templates'].append({\n",
    "            'base': template_base,\n",
    "            'defaults': template_defaults,\n",
    "            'parameters': template_params,\n",
    "            'format_definition': format_defined,\n",
    "            'accept_type': accept_type,\n",
    "            'search_url': search_url,  # empty searchTerms\n",
    "            'example_url': example_url,  # searchTerms w/ provided keywords\n",
    "            'default_url': default_url,  # only default params, see cwic dataset osdds\n",
    "            'param_defs': extract_parameter_defs(extracted_elem, template_params)\n",
    "        })\n",
    "    \n",
    "    # get the basic definition bits (keywords, name, etc)\n",
    "    output['has_title'] = len(osdd.xpath('*[local-name()=\"ShortName\"]')) > 0\n",
    "    output['has_desc'] = len(osdd.xpath('*[local-name()=\"Description\"]')) > 0\n",
    "    output['has_keywords'] = len(osdd.xpath('*[local-name()=\"Tags\"]')) > 0\n",
    "    output['has_contact'] = len(osdd.xpath('*[local-name()=\"Contact\"]')) > 0\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from doug, see notes re: uptime\n",
    "cwic_links = [\n",
    "    'http://dap.onc.uvic.ca/erddap/opensearch1.1/description.xml',\n",
    "    'http://gcmd.gsfc.nasa.gov/KeywordSearch/default/openSearch.jsp?Portal=cwic',\n",
    "    'http://podaac.jpl.nasa.gov/ws/search/dataset/osd.xml',\n",
    "    # 'http://nsidc.org/api/opensearch/1.1/dataset/description',  # we're just not going to run this\n",
    "    'http://ghrc.nsstc.nasa.gov/hydro/ghost.xml',\n",
    "    'http://mirador.gsfc.nasa.gov/mirador_dataset_opensearch.xml',\n",
    "    'http://eo-virtual-archive4.esa.int/search/ER02_SAR_RAW_0P/description',\n",
    "    'http://www1.usgs.gov/erddap/opensearch1.1/description.xml',\n",
    "    # 'http://bison.usgs.ornl.gov/doc/api.jsp',  # this is now a dead link\n",
    "    # 'http://ceocat.ccrs.nrcan.gc.ca/opensearch_description_document.xml',  # this is 403 access forbidden\n",
    "    # 'http://rs211980.rs.hosteurope.de/mule/os-description/',  # 503 service down\n",
    "    'http://geo.spacebel.be/opensearch/description.xml',  # from the fedeo documentation page listed\n",
    "    'http://lance-modis.eosdis.nasa.gov/user_services/dataset_opensearch.xml'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://dap.onc.uvic.ca/erddap/opensearch1.1/description.xml\n",
      "Downloading http://gcmd.gsfc.nasa.gov/KeywordSearch/default/openSearch.jsp?Portal=cwic\n",
      "Downloading http://podaac.jpl.nasa.gov/ws/search/dataset/osd.xml\n",
      "Downloading http://ghrc.nsstc.nasa.gov/hydro/ghost.xml\n",
      "\tFailed request\n",
      "Downloading http://mirador.gsfc.nasa.gov/mirador_dataset_opensearch.xml\n",
      "Downloading http://eo-virtual-archive4.esa.int/search/ER02_SAR_RAW_0P/description\n",
      "Downloading http://www1.usgs.gov/erddap/opensearch1.1/description.xml\n",
      "Downloading http://geo.spacebel.be/opensearch/description.xml\n",
      "Downloading http://lance-modis.eosdis.nasa.gov/user_services/dataset_opensearch.xml\n"
     ]
    }
   ],
   "source": [
    "# to download the osdds\n",
    "cwic_osdds = []\n",
    "\n",
    "for cwic_link in cwic_links:\n",
    "    print 'Downloading {0}'.format(cwic_link)\n",
    "\n",
    "    req = requests.get(cwic_link)\n",
    "    osdd = {\n",
    "        'url': cwic_link,\n",
    "        'status': req.status_code\n",
    "    }\n",
    "    if req.status_code != 200:\n",
    "        print '\\tFailed request'\n",
    "        cwic_osdds.append(osdd)\n",
    "        continue\n",
    "    \n",
    "    # just checking\n",
    "    xml = etree.fromstring(req.content)\n",
    "    osdd.update({'xml':req.content})\n",
    "    cwic_osdds.append(osdd)\n",
    "    \n",
    "with open('outputs/cwic_osdds.json', 'w') as f:\n",
    "    f.write(json.dumps(cwic_osdds, indent=4))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to reload from disk for parsing, etc\n",
    "with open('outputs/cwic_osdds.json', 'r') as f:\n",
    "    cwic_osdds = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, osdd in enumerate(cwic_osdds):\n",
    "    if osdd.get('status') != 200:\n",
    "        continue\n",
    "        \n",
    "    xml = etree.fromstring(osdd.get('xml').encode('utf-8'))\n",
    "    parsed_osdd = parse_osdd(xml)\n",
    "    \n",
    "    #print parsed_osdd\n",
    "    \n",
    "    # try the two example queries\n",
    "    for j, template in enumerate(parsed_osdd.get('templates', [])):\n",
    "        accept_type = template.get('accept_type', '')\n",
    "        headers = {'Accept': accept_type} if accept_type else {}\n",
    "        example_url = template.get('example_url', '')\n",
    "        search_url = template.get('search_url', '')\n",
    "        default_url = template.get('default_url', '')\n",
    "        \n",
    "        if search_url:\n",
    "            try:\n",
    "                req = requests.get(search_url, headers=headers, timeout=15)\n",
    "                ex = {\n",
    "                    'status': req.status_code,\n",
    "                    'has_content': req.content is not None\n",
    "                }\n",
    "                output = parse_response(req.content, req.headers)\n",
    "                ex.update(output)\n",
    "                template.update({'search_url_response': ex})\n",
    "            except requests.exceptions.ReadTimeout:\n",
    "                template.update({'search_url_response': {'status': 'timeout'}})\n",
    "            \n",
    "            \n",
    "        if example_url:\n",
    "            try:\n",
    "                req = requests.get(example_url, headers=headers, timeout=15)\n",
    "                ex = {\n",
    "                    'status': req.status_code,\n",
    "                    'has_content': req.content is not None\n",
    "                }\n",
    "                output = parse_response(req.content, req.headers)\n",
    "                ex.update(output)\n",
    "                template.update({'example_url_response': ex})\n",
    "            except requests.exceptions.ReadTimeout:\n",
    "                template.update({'example_url_response': {'status': 'timeout'}})\n",
    "                \n",
    "        if default_url:\n",
    "            try:\n",
    "                req = requests.get(default_url, headers=headers, timeout=15)\n",
    "                ex = {\n",
    "                    'status': req.status_code,\n",
    "                    'has_content': req.content is not None\n",
    "                }\n",
    "                output = parse_response(req.content, req.headers)\n",
    "                ex.update(output)\n",
    "                template.update({'default_url_response': ex})\n",
    "            except requests.exceptions.ReadTimeout:\n",
    "                template.update({'default_url_response': {'status': 'timeout'}})\n",
    "                \n",
    "        parsed_osdd['templates'][j] = template\n",
    "    \n",
    "    osdd.update(parsed_osdd)\n",
    "    cwic_osdds[i] = osdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('outputs/cwic_osdds_extended.json', 'w') as f:\n",
    "    f.write(json.dumps(cwic_osdds, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('outputs/cwic_osdds_extended.json', 'r') as f:\n",
    "    cwic_osdds = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's see what we get from the nested osdds\n",
    "\n",
    "tpl_types = ['search', 'example', 'default']\n",
    "\n",
    "for i, ex_osdd in enumerate(cwic_osdds):\n",
    "    for j, template in enumerate(ex_osdd.get('templates', [])):\n",
    "        for tpl in tpl_types:\n",
    "            key = '{0}_url_response'.format(tpl)\n",
    "            rsp = template.get(key, {})\n",
    "            if not rsp:\n",
    "                continue\n",
    "            \n",
    "            if rsp.get('status') != 200:\n",
    "                continue\n",
    "            \n",
    "            for rel in rsp.get('search_rels', []):\n",
    "                # let's see if we get a good osdd and add the results\n",
    "                # and this is a resultset so they may not be\n",
    "                # very different (param diff)\n",
    "                rel_link = rel.get('link_url')\n",
    "                if not rel_link:\n",
    "                    continue\n",
    "                try:\n",
    "                    req = requests.get(rel_link, timeout=15)\n",
    "                except requests.exceptions.ReadTimeout:\n",
    "                    rel.update({\"status\": \"timeout\"})\n",
    "                    template.update({key: rel})\n",
    "                    ex_osdd.get('templates')[j] = template\n",
    "            \n",
    "                if req.status_code != 200:\n",
    "                    rel.update({\"status\": req.status_code})\n",
    "                    template.update({key: rel})\n",
    "                    ex_osdd.get('templates')[j] = template\n",
    "                \n",
    "                try:\n",
    "                    xml = etree.fromstring(req.content)\n",
    "                except:\n",
    "                    rel.update({\"status\": \"invalid xml\"})\n",
    "                    template.update({key: rel})\n",
    "                    ex_osdd.get('templates')[j] = template\n",
    "            \n",
    "                parsed = parse_osdd(xml)\n",
    "                rel.update({\n",
    "                    \"status\": req.status_code,\n",
    "                    \"xml\": etree.tostring(xml)\n",
    "                })\n",
    "                rel.update(parsed)\n",
    "                template.update({key: rel})\n",
    "                ex_osdd.get('templates')[j] = template\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('outputs/cwic_osdds_nested.json', 'w') as f:\n",
    "    f.write(json.dumps(cwic_osdds, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
