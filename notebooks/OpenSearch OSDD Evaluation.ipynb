{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1. Can we generate a valid search URL from the URL Template provided?\n",
    "    a. given the time frame of the harvests, this has to be considered alongside linkrot in the osdd (failed search query may not mean the template is incorrect, could just mean the service url is no longer valid at all).\n",
    "2. Can we identify best practices (uses esip spatial, uses the time namespace, uses parameter elements)?\n",
    "3. dataset/granule search (i don't think we have any data to support this at all).\n",
    "\n",
    "Other info - can you identify the parent osdd of a resultset or of a nested osdd? can you grok it from the url only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json as js\n",
    "import os\n",
    "from lxml import etree\n",
    "import urlparse\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import chain\n",
    "from datetime import datetime\n",
    "\n",
    "# for pinging the database\n",
    "import sqlalchemy as sqla\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.dialects.postgresql import *\n",
    "from sqlalchemy import and_\n",
    "from semproc.xml_utils import *\n",
    "from mpp.models import Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_namespaces(xml):\n",
    "    '''\n",
    "    Pull all of the namespaces in the source document\n",
    "    and generate a list of tuples (prefix, URI) to dict\n",
    "    '''\n",
    "    if xml is None:\n",
    "        return {}\n",
    "\n",
    "    document_namespaces = dict(xml.xpath('/*/namespace::*'))\n",
    "    if None in document_namespaces:\n",
    "        document_namespaces['default'] = document_namespaces[None]\n",
    "        del document_namespaces[None]\n",
    "\n",
    "    # now run through any child namespace issues\n",
    "    all_namespaces = xml.xpath('//namespace::*')\n",
    "    for i, ns in enumerate(all_namespaces):\n",
    "        if ns[1] in document_namespaces.values():\n",
    "            continue\n",
    "        new_key = ns[0] if ns[0] else 'default%s' % i\n",
    "        document_namespaces[new_key] = ns[1]\n",
    "\n",
    "    return document_namespaces\n",
    "\n",
    "def extract_urls(xml, mimetype='atom+xml'):\n",
    "    return xml.xpath('//*[local-name()=\"Url\" and (@*[local-name()=\"type\"]=\"application/%(mimetype)s\" or @*[local-name()=\"type\"]=\"text/%(mimetype)s\")]' % {'mimetype': mimetype})\n",
    "\n",
    "def extract_template(url, append_limit=True):\n",
    "    # get the base url from the template\n",
    "    template_parts = urlparse.urlparse(url)\n",
    "    \n",
    "    if not template_parts.scheme:\n",
    "        return '', '', {}, False\n",
    "    \n",
    "    base_url = urlparse.urlunparse((\n",
    "        template_parts.scheme,\n",
    "        template_parts.netloc,\n",
    "        template_parts.path,\n",
    "        None,\n",
    "        None,\n",
    "        None\n",
    "    ))\n",
    "\n",
    "    qp = {k: v[0] for k, v in urlparse.parse_qs(template_parts.query).iteritems()}\n",
    "\n",
    "    # get the hard-coded params\n",
    "    defaults = {k:v for k, v in qp.iteritems() \n",
    "            if not v.startswith('{') \n",
    "            and not v.endswith('}')}\n",
    "    \n",
    "    # a flag for some hard-coded response format type to manage\n",
    "    # accept headers or no\n",
    "    format_defined = len([v for k, v in defaults.iteritems() if 'atom' in v.lower() or 'rss' in v.lower()]) > 0\n",
    "\n",
    "    # get the rest (and ignore the optional/namespaces)\n",
    "    parameters = {k: v[1:-1] for k, v in qp.iteritems() \n",
    "            if v.startswith('{') \n",
    "            and v.endswith('}')}\n",
    "    \n",
    "    if append_limit:\n",
    "        terms = extract_parameter_key('count', parameters)\n",
    "        if terms:\n",
    "            defaults = dict(\n",
    "                chain(defaults.items(), {k: 5 for k in terms.keys()}.items())\n",
    "            )\n",
    "            \n",
    "    # note: not everyone manages url-encoded query parameter delimiters\n",
    "    #       and not everyone manages non-url-encoded values so yeah. we are\n",
    "    #       ignoring the non-url-encoded group tonight.\n",
    "    # return the base, defaults, parameters as dict\n",
    "    return base_url, defaults, parameters, format_defined\n",
    "\n",
    "def extract_parameter_key(value, params):\n",
    "    # sort out the query parameter name for a parameter\n",
    "    # and don't send curly bracketed things, please\n",
    "    return {k: v.split(':')[-1].replace('?', '') for k, v \n",
    "                in params.iteritems() \n",
    "                if value in v}\n",
    "\n",
    "def extract_parameter_defs(url_elem, defined_terms):\n",
    "    # could just go with a namespace check but\n",
    "    # namespaces are included and not used more\n",
    "    # than i'd like. safety first.\n",
    "    params = url_elem.xpath('*[local-name()=\"Parameter\"]')\n",
    "    \n",
    "    output = {}\n",
    "    for i, param in enumerate(params):\n",
    "        pname = param.attrib.get('name', i)\n",
    "        pval = param.attrib.get('value', '')\n",
    "        poptions = param.xpath('*[local-name()=\"Option\"]')\n",
    "        options = [(o.attrib.get('value'), o.attrib.get('label')) for o in poptions]\n",
    "        \n",
    "        output[pname] = {\n",
    "            \"value\": pval,\n",
    "            \"options\": options\n",
    "        }\n",
    "    \n",
    "    return output\n",
    "            \n",
    "def extract_query_terms(xml, param_name):\n",
    "    # find a query element that contains an example\n",
    "    # for the provided param_name (no namespace, no optional flag)\n",
    "    example_queries = {}\n",
    "    xp = '//*[local-name()=\"Query\" and @*[local-name()=\"role\"]=\"example\"]/@*[local-name()=\"{0}\"]'.format(param_name)\n",
    "    try:\n",
    "        example_queries = xml.xpath(xp)\n",
    "    except:\n",
    "        print 'failed example query: ', xp\n",
    "        return []\n",
    "    \n",
    "    return example_queries\n",
    "\n",
    "def extract_search_rels(xml):\n",
    "#     application/opensearchdescription+xml\n",
    "    for elem in xml.xpath('//*/*[local-name()=\"link\" and (@*[local-name()=\"type\"]=\"application/opensearchdescription+xml\") and (@*[local-name()=\"rel\"]=\"search\" or @*[local-name()=\"rel\"]=\"http://esipfed.org/ns/fedsearch/1.0/search#\")]'):\n",
    "        parent = next(iter(elem.getparent().xpath('*[local-name()=\"title\"]')), None)\n",
    "        yield {\n",
    "            \"link_url\": elem.attrib.get('href', ''),\n",
    "            \"link_title\": elem.attrib.get('title', ''),\n",
    "            \"link_type\": elem.attrib.get('type', ''),\n",
    "            \"parent_title\": parent.text if parent is not None else ''\n",
    "        }\n",
    "            \n",
    "\n",
    "def extract_response_stats(xml):\n",
    "    total = next(iter(xml.xpath('//*[local-name()=\"totalResults\"]/text()')), 'Unknown')\n",
    "    subset = next(iter(xml.xpath('//*[local-name()=\"itemsPerPage\"]/text()')), 'Unknown')\n",
    "    \n",
    "    return subset, total\n",
    "\n",
    "def execute_request(url, headers={}):\n",
    "    try:\n",
    "        req = requests.get(url, headers=headers)\n",
    "    except:\n",
    "        logger.error('\\tSkipping connection issue\\'s')\n",
    "        return '-999', '', ''\n",
    "    \n",
    "    return req.status_code, req.content, req.headers\n",
    "\n",
    "def parse_response(content, headers={}):\n",
    "    output = {}\n",
    "    \n",
    "    # see if it has content, see if the xml parses, see if it's even xml\n",
    "    if not content:\n",
    "        return {'error': 'No content'}\n",
    "    \n",
    "    if 'html' in headers.get('content-type'):\n",
    "        return {'error': 'HTML response'}\n",
    "\n",
    "    try:\n",
    "        xml = etree.fromstring(content)\n",
    "    except:\n",
    "        return {'error': 'XML Parse error'}\n",
    "\n",
    "    subset, total = extract_response_stats(xml)\n",
    "    \n",
    "    # this would get us to some nested search\n",
    "    # there is no guarantee it is dataset/granule!\n",
    "    # or can be identified as such!\n",
    "    search_rels = []\n",
    "    for search_rel in [e for e in extract_search_rels(xml)]:\n",
    "        try:\n",
    "            rsp = requests.get(search_rel.get('link_url'))\n",
    "            search_rel.update({\"status\": rsp.status_code})\n",
    "            try:\n",
    "                xml = etree.fromstring(rsp.content)\n",
    "            except:\n",
    "                search_rel.update({\"error\": \"invalid xml\"})\n",
    "                search_rels.append(search_rel)\n",
    "                continue\n",
    "                \n",
    "            parsed_rel = parse_osdd(xml)\n",
    "            \n",
    "            search_rel.update({\n",
    "                \"content\": rsp.content,\n",
    "                \"response\": parsed_rel\n",
    "            })\n",
    "        except requests.exceptions.ReadTimeout:\n",
    "            search_rel.update({\"error\": \"timeout\"})\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            search_rel.update({\"error\": \"connection error\"})\n",
    "        except:\n",
    "            search_rel.update({\"error\": \"unspecified\"})\n",
    "            \n",
    "        search_rels.append(search_rel)\n",
    "    \n",
    "    output.update({\n",
    "        'subset': subset,\n",
    "        'total': total,\n",
    "    })\n",
    "    if search_rels:\n",
    "        output.update({'search_rels': search_rels})\n",
    "        \n",
    "    return output\n",
    "\n",
    "def parse_osdd(osdd):\n",
    "    # get the url template to test basic search\n",
    "    #    get the parameter list (prefix:term)\n",
    "    # get the parameter elements\n",
    "    #    match to parameter list\n",
    "    # get namespaces \n",
    "    output = {}\n",
    "    \n",
    "    output['namespaces'] = extract_namespaces(osdd)\n",
    "    output['templates'] = []\n",
    "    \n",
    "    # looking for resultset requests (atom or rss)\n",
    "    for extracted_elem in extract_urls(osdd) + extract_urls(osdd,'rss+xml'):\n",
    "        template_base, template_defaults, template_params, format_defined = extract_template(extracted_elem.attrib.get('template'))\n",
    "        accept_type = extracted_elem.attrib.get('type', '')\n",
    "        \n",
    "        search_url = ''\n",
    "        search_terms = extract_parameter_key('searchTerms', template_params)\n",
    "    \n",
    "        if search_terms:\n",
    "            qps = dict(\n",
    "                chain(\n",
    "                    template_defaults.items(),\n",
    "                    {search_terms.keys()[0]: ''}.items()\n",
    "                )\n",
    "            )\n",
    "            search_url = template_base + '?' + urllib.urlencode(qps.items())\n",
    "        \n",
    "        example_url = ''\n",
    "        example_terms = list(\n",
    "            chain.from_iterable(\n",
    "                [extract_query_terms(extracted_elem.getparent(), s) for s in search_terms.values()]\n",
    "            )\n",
    "        )\n",
    "        if example_terms:\n",
    "            qps = dict(\n",
    "                chain(\n",
    "                    template_defaults.items(),\n",
    "                    {search_terms.keys()[0]: example_terms[0]}.items()\n",
    "                )\n",
    "            )\n",
    "            example_url = template_base + '?' + urllib.urlencode(qps.items())\n",
    "        \n",
    "        default_url = template_base + '?' + urllib.urlencode(template_defaults.items()) if isinstance(template_defaults, dict) else template_defaults\n",
    "        \n",
    "        output['templates'].append({\n",
    "            'base': template_base,\n",
    "            'defaults': template_defaults,\n",
    "            'parameters': template_params,\n",
    "            'format_definition': format_defined,\n",
    "            'accept_type': accept_type,\n",
    "            'search_url': search_url,  # empty searchTerms\n",
    "            'example_url': example_url,  # searchTerms w/ provided keywords\n",
    "            'default_url': default_url,  # only default params, see cwic dataset osdds\n",
    "            'param_defs': extract_parameter_defs(extracted_elem, template_params)\n",
    "        })\n",
    "    \n",
    "    # get the basic definition bits (keywords, name, etc)\n",
    "    output['has_title'] = len(osdd.xpath('*[local-name()=\"ShortName\"]')) > 0\n",
    "    output['has_desc'] = len(osdd.xpath('*[local-name()=\"Description\"]')) > 0\n",
    "    output['has_keywords'] = len(osdd.xpath('*[local-name()=\"Tags\"]')) > 0\n",
    "    output['has_contact'] = len(osdd.xpath('*[local-name()=\"Contact\"]')) > 0\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting the resultset (links and harvested osdd)\n",
    "sketchy_sql = '''with i\n",
    "as (\n",
    "    select d.response_id, jsonb_array_elements(d.identity::jsonb) ident\n",
    "    from identities d\n",
    "    where d.identity is not null\n",
    ")\n",
    "\n",
    "select r.id, r.source_url, r.cleaned_content\n",
    "from responses r join i on i.response_id = r.id\n",
    "where i.ident->>'protocol' = 'OpenSearch' \n",
    "    and i.ident#>>'{service,name}' = 'OpenSearchDescription';\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the postgres connection file\n",
    "with open('../local/big_rds.conf', 'r') as f:\n",
    "    conf = js.loads(f.read())\n",
    "\n",
    "# our connection\n",
    "engine = sqla.create_engine(conf.get('connection'))\n",
    "Session = sessionmaker()\n",
    "Session.configure(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just for resets\n",
    "session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run through the osdd resultset\n",
    "result = session.execute(sketchy_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for r in result:\n",
    "    if os.path.exists('outputs/osdds/{0}.json'.format(r['id'])):\n",
    "        continue\n",
    "    try:\n",
    "        xml = etree.fromstring(r['cleaned_content'].encode('utf-8'))\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    parsed_osdd = parse_osdd(xml)\n",
    "    parsed_osdd.update({\n",
    "        \"response_id\": r['id'],\n",
    "        \"source_url\": r['source_url'],\n",
    "        \"validated_on\": datetime.now().isoformat()\n",
    "    })\n",
    "    \n",
    "    # run a quick linkrot check\n",
    "    # note that this doesn't compare the\n",
    "    # harvested response with the currently available one\n",
    "    url = r['source_url']\n",
    "    try:\n",
    "        rsp = requests.head(url)\n",
    "        parsed_osdd.update({\"status\": rsp.status_code})\n",
    "    except:\n",
    "        parsed_osdd.update({\"error\": \"linkrot error\"})\n",
    "    \n",
    "    for j, template in enumerate(parsed_osdd.get('templates', [])):\n",
    "        accept_type = template.get('accept_type', '')\n",
    "        headers = {'Accept': accept_type} if accept_type else {}\n",
    "        \n",
    "        template_responses = []\n",
    "        for url_type in ['search', 'example', 'default']:\n",
    "            url = template.get('%s_url' % url_type, '')\n",
    "            if not url:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                req = requests.get(url, headers=headers, timeout=15)\n",
    "                ex = {\n",
    "                    'status': req.status_code,\n",
    "                    'has_content': req.content is not None\n",
    "                }\n",
    "                parsed_rsp = parse_response(req.content, req.headers)\n",
    "                ex.update(parsed_rsp)\n",
    "                template_responses.append({'%s_response' % url_type: ex})\n",
    "            except requests.exceptions.ReadTimeout:\n",
    "                template_responses.append({'%s_response' % url_type: {'error': 'timeout'}})\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                template_responses.append({'%s_response' % url_type: {'error': 'connection error'}})\n",
    "        \n",
    "        template.update({\"responses\": template_responses})\n",
    "        parsed_osdd['templates'][j] = template\n",
    "        \n",
    "    with open('outputs/osdds/{0}.json'.format(r['id']), 'w') as g:\n",
    "        g.write(js.dumps(parsed_osdd, indent=4))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So now we have most of what we're interested in, let's put it in the RDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import sqlalchemy as sqla\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import (\n",
    "    MetaData,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    "    Boolean,\n",
    "    DateTime,\n",
    ")\n",
    "from sqlalchemy.dialects.postgresql import *\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class OSDD(Base):\n",
    "    __tablename__ = 'osdds'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    status_code = Column(Integer)\n",
    "    url = Column(String)\n",
    "    has_title = Column(Boolean)\n",
    "    has_description = Column(Boolean)\n",
    "    has_contact = Column(Boolean)\n",
    "    has_keywords = Column(Boolean)\n",
    "    url_templates = Column(JSON)\n",
    "    namespaces = Column(JSON)\n",
    "    date_verified = Column(DateTime)\n",
    "    error=Column(String)\n",
    "    response_id = Column(Integer)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../local/big_rds.conf', 'r') as f:\n",
    "    conf = js.loads(f.read())\n",
    "\n",
    "files = glob.glob('outputs/osdds/*.json')\n",
    "    \n",
    "# our connection\n",
    "engine = sqla.create_engine(conf.get('connection'))\n",
    "Session = sessionmaker()\n",
    "Session.configure(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in files[5:]:\n",
    "    with open(f, 'r') as g:\n",
    "        data = js.loads(g.read())\n",
    "    \n",
    "    osdd = OSDD(\n",
    "        status_code=data.get('status'),\n",
    "        url=data.get(\"source_url\"),\n",
    "        has_title=data.get('has_title'),\n",
    "        has_description=data.get('has_desc'),\n",
    "        has_contact=data.get('has_contact'),\n",
    "        has_keywords=data.get('has_keywords'),\n",
    "        url_templates=data.get('templates', []),\n",
    "        namespaces=data.get('namespaces', {}),\n",
    "        response_id=data.get('response_id'),\n",
    "        date_verified=data.get('validated_on'),\n",
    "        error=data.get('error')\n",
    "    )\n",
    "    \n",
    "    session.add(osdd)\n",
    "    try:\n",
    "        session.commit()\n",
    "    except:\n",
    "        print 'commit failed', f\n",
    "        session.rollback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Some notes about the stats.\n",
    "\n",
    "\n",
    "Basic namespace use:\n",
    "\n",
    "| Namespace                                                                                    | Frequency | \n",
    "|----------------------------------------------------------------------------------------------|-----------| \n",
    "| (alf,http://www.alfresco.org)                                                                | 1         | \n",
    "| (alien,http://alien.jrc.ec.europa.eu/species)                                                | 1         | \n",
    "| (atom,http://www.w3.org/2005/Atom)                                                           | 1         | \n",
    "| (bvh,http://www.hospitalsdatabase.lshtm.ac.uk/opensearch/)                                   | 1         | \n",
    "| (chronam,http://chroniclingamerica.loc.gov)                                                  | 2         | \n",
    "| (custom,http://example.com/opensearchextensions/1.0/)                                        | 1         | \n",
    "| (dc,http://purl.org/dc/elements/1.1/)                                                        | 6         | \n",
    "| (dclite4g,http://xmlns.com/2008/dclite4g#)                                                   | 1         | \n",
    "| (dct,http://purl.org/dc/terms/)                                                              | 1         | \n",
    "| (default,http://a9.com/-/spec/opensearch/1.1/)                                               | 4608      | \n",
    "| (eop,http://www.genesi-dr.eu/spec/opensearch/extensions/eop/1.0/)                            | 1         | \n",
    "| (eum,http://a9.com/-/opensearch/extensions/eumetsat/1.0/)                                    | 1         | \n",
    "| (geo,http://a9.com/-/opensearch/extensions/geo/1.0/)                                         | 68        | \n",
    "| (ical,http://www.w3.org/2002/12/cal/ical#)                                                   | 1         | \n",
    "| (ie,http://schemas.microsoft.com/Search/2008/)                                               | 4         | \n",
    "| (MODAPSParameters,http://modwebsrv.modaps.eosdis.nasa.gov/opensearchextensions/1.0/)         | 1         | \n",
    "| (moz,http://www.mozilla.org/2006/browser/search/)                                            | 1049      | \n",
    "| (moz,http:/www.mozilla.org/2006/browser/search/)                                             | 10        | \n",
    "| (mozilla,http://www.mozilla.org/2006/browser/search/)                                        | 4         | \n",
    "| (mp,http://modwebsrv.modaps.eosdis.nasa.gov/opensearchextensions/1.0/)                       | 8         | \n",
    "| (nsidc,http://nsidc.org/ns/opensearch/1.1/)                                                  | 7         | \n",
    "| (opensearch,http://a9.com/-/spec/opensearch/1.1/)                                            | 5         | \n",
    "| (opensearchgeo,http://a9.com/-/opensearch/extensions/geo/1.0/)                               | 1         | \n",
    "| (os,http://a9.com/-/spec/opensearch/1.1/)                                                    | 1         | \n",
    "| (parameters,http://a9.com/-/spec/opensearch/extensions/parameters/1.0/)                      | 6         | \n",
    "| (podaac,http://podaac.jpl.nasa.gov/opensearch/)                                              | 1         | \n",
    "| (py,http://genshi.edgewall.org/)                                                             | 2         | \n",
    "| (rdf,http://www.w3.org/1999/02/22-rdf-syntax-ns#)                                            | 1         | \n",
    "| (referrer,http://a9.com/-/opensearch/extensions/referrer/)                                   | 8         | \n",
    "| (sar,http://earth.esa.int/sar)                                                               | 1         | \n",
    "| (sru,http://a9.com/-/opensearch/extensions/sru/2.0/)                                         | 1         | \n",
    "| (suggestions,http://www.opensearch.org/specifications/opensearch/extensions/suggestions/1.1) | 1         | \n",
    "| (time,http://a9.com/-/opensearch/extensions/time/1.0/)                                       | 32        | \n",
    "| (wcapi,http://www.worldcat.org/devnet/wiki/SearchAPIDetails)                                 | 2         | \n",
    "| (ws,http://dclite4g.xmlns.com/ws.rdf#)                                                       | 1         | \n",
    "| (xml,http://www.w3.org/XML/1998/namespace)                                                   | 4608      | \n",
    "| (xsd,http://www.w3.org/2001/XMLSchema)                                                       | 19        | \n",
    "| (xsi,http://www.w3.org/2001/XMLSchema-instance)                                              | 19        | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
