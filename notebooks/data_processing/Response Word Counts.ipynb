{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Basic word (token) counts for XML responses.\n",
    "\n",
    "I will note, wherever necessary, assumptions related to the stopwords corpora. For this count, mimetypes, namespaces and URNs (specifically those related to EPSG codes and the like). Also ditching numbers and timestamps.\n",
    "\n",
    "Given that we are looking at a simple token count, the initial BoW vectors will include some errant punctuation. Punctuation-only tokens are dropped but things like '(This' are left as is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import glob\n",
    "import re\n",
    "from datetime import datetime\n",
    "import dateutil.parser as dateparser\n",
    "from itertools import chain\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import WordListCorpusReader\n",
    "\n",
    "from semproc.rawresponse import RawResponse\n",
    "from semproc.bag_parser import BagParser\n",
    "\n",
    "import json as js  # name conflict with sqla\n",
    "import sqlalchemy as sqla\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.dialects.postgresql import *\n",
    "from sqlalchemy import and_\n",
    "from mpp.models import Response\n",
    "from mpp.models import BagOfWords\n",
    "from semproc.nlp_utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_header_list(headers):\n",
    "    '''\n",
    "    convert from the list of strings, one string\n",
    "    per kvp, to a dict with keys normalized\n",
    "    '''\n",
    "    return dict(\n",
    "        (k.strip().lower(), v.strip()) for k, v in (\n",
    "            h.split(':', 1) for h in headers)\n",
    "    )\n",
    "\n",
    "\n",
    "# def remove_numeric(text):\n",
    "#     match_pttn = ur'\\w*\\b-?\\d\\s*\\w*'\n",
    "#     captures = re.findall(match_pttn, u' {0} '.format(text))\n",
    "\n",
    "#     # strip them out\n",
    "#     if captures:\n",
    "#         text = re.sub('|'.join(captures), ' ', text)\n",
    "#         return '' if text == '0' else text\n",
    "\n",
    "#     return text\n",
    "\n",
    "# def strip_dates(text):\n",
    "#         # this should still make it an invalid date\n",
    "#         # text = text[3:] if text.startswith('NaN') else text\n",
    "#         try:\n",
    "#             d = dateparser.parse(text)\n",
    "#             return ''\n",
    "#         except ValueError:\n",
    "#             return text\n",
    "#         except OverflowError:\n",
    "#             return text\n",
    "        \n",
    "def strip_filenames(text):\n",
    "    # we'll see\n",
    "    exts = ('png', 'jpg', 'hdf', 'xml', 'doc', 'pdf', 'txt', 'jar', 'nc', 'XSL', 'kml', 'xsd')\n",
    "    return '' if text.endswith(exts) else text\n",
    "    \n",
    "def strip_identifiers(texts):\n",
    "    # chuck any urns, urls, uuids\n",
    "    _pattern_set = [\n",
    "        ('url', ur\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"),\n",
    "        # a urn that isn't a url\n",
    "        ('urn', ur\"(?![http://])(?![https://])(?![ftp://])(([a-z0-9.\\S][a-z0-9-.\\S]{0,}\\S:{1,2}\\S)+[a-z0-9()+,\\-.=@;$_!*'%/?#]+)\"),\n",
    "        ('uuid', ur'([a-f\\d]{8}(-[a-f\\d]{4}){3}-[a-f\\d]{12}?)'),\n",
    "        ('doi', ur\"(10[.][0-9]{4,}(?:[/][0-9]+)*/(?:(?![\\\"&\\\\'])\\S)+)\"),\n",
    "        ('md5', ur\"([a-f0-9]{32})\")\n",
    "    ]\n",
    "    for pattern_type, pattern in _pattern_set:\n",
    "        for m in re.findall(re.compile(pattern), texts):\n",
    "            m = max(m) if isinstance(m, tuple) else m\n",
    "            try:\n",
    "                texts = texts.replace(m, '')\n",
    "            except Exception as ex:\n",
    "                print ex\n",
    "                print m\n",
    "                \n",
    "    files = ['cat_interop_urns.txt', 'mimetypes.txt', 'namespaces.txt']\n",
    "    for f in files:\n",
    "        texts = remove_tokens(f, texts)\n",
    "    return texts.split()\n",
    "    \n",
    "def clean(text):\n",
    "    text = strip_dates(text)\n",
    "    text = remove_numeric(text)\n",
    "    \n",
    "    text = remove_punctuation(text.strip()).strip()\n",
    "    text = strip_terminal_punctuation(text)\n",
    "    \n",
    "    text = strip_filenames(text)\n",
    "    \n",
    "    return text\n",
    "        \n",
    "exclude_tags = ['schemaLocation', 'noNamespaceSchemaLocation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grab the clean text from the rds\n",
    "with open('../local/big_rds.conf', 'r') as f:\n",
    "    conf = js.loads(f.read())\n",
    "\n",
    "# our connection\n",
    "engine = sqla.create_engine(conf.get('connection'))\n",
    "Session = sessionmaker()\n",
    "Session.configure(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get a count of the xml responses\n",
    "total = session.query(Response).filter(Response.format=='xml').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LIMIT = 100\n",
    "\n",
    "# total = 5\n",
    "# LIMIT= total\n",
    "\n",
    "for i in xrange(0, total, LIMIT):\n",
    "    # get some responses\n",
    "    responses = session.query(Response).filter(Response.format=='xml').limit(LIMIT).offset(i).all()\n",
    "    \n",
    "    appends = []\n",
    "    \n",
    "    for response in responses:\n",
    "        cleaned_content = response.cleaned_content\n",
    "        \n",
    "        # strip the html cruft but ignore the a tags\n",
    "        bp = BagParser(cleaned_content.encode('utf-8'), True, False)\n",
    "        if bp.parser.xml is None:\n",
    "            print 'NOT XML: ', cleaned_content[:100]\n",
    "            continue\n",
    "        # we don't care about the fully qualified namespace here\n",
    "        stripped_text = [b[1].split() for b in bp.strip_text(exclude_tags) if b[1]]\n",
    "        stripped_text = list(chain.from_iterable(stripped_text))\n",
    "        cleaned_text = [clean(s) for s in stripped_text]\n",
    "\n",
    "        bow = strip_identifiers(' '.join([c for c in cleaned_text if c]))\n",
    "        \n",
    "        bag = BagOfWords(\n",
    "            generated_on=datetime.now().isoformat(),\n",
    "            bag_of_words=bow,\n",
    "            method=\"basic\",\n",
    "            response_id=response.id\n",
    "        )\n",
    "        appends.append(bag)\n",
    "    \n",
    "    try:\n",
    "        session.add_all(appends)\n",
    "        session.commit()\n",
    "    except Exception as ex:\n",
    "        print 'failed ', i, ex\n",
    "        session.rollback()\n",
    "    \n",
    "session.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<document ENCODING=\"Cp1252\" ModsDocAuthor=\"Bolton, B. &amp; Fisher, B. L.\" ModsDocDate=\"2011\" ModsDocID=\"23490\" ModsDocOrigin=\"http://antbase.org/ants/publications/23490/23490.pdf\" ModsDocTitle=\"Taxon\n"
     ]
    }
   ],
   "source": [
    "# czech\n",
    "# r_id = 348255\n",
    "# r_id = 736811\n",
    "# r_id = 785501\n",
    "\n",
    "# junk punctuation\n",
    "r_id = 380653\n",
    "cleaned_content, = session.query(Response.cleaned_content).filter(Response.id==r_id).first()\n",
    "\n",
    "print cleaned_content[:200]\n",
    "\n",
    "bp = BagParser(cleaned_content.encode('utf-8'), True, False)\n",
    "# we don't care about the fully qualified namespace here\n",
    "stripped_text = [b[1].split() for b in bp.strip_text(exclude_tags) if b[1]]\n",
    "stripped_text = list(chain.from_iterable(stripped_text))\n",
    "cleaned_text = [clean(s) for s in stripped_text]\n",
    "\n",
    "bow = strip_identifiers(' '.join([c for c in cleaned_text if c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Cp1252', u'Bolton', u'B', u'Fisher', u'B', u'L', u'Taxonomy', u'Afrotropical', u'West', u'Palaearctic', u'ants', u'the', u'ponerine', u'genus', u'Hypoponera', u'Santschi', u'Hymenoptera', u'Formicidae', u'added', u'donat', u'Bolton', u'B', u'Fisher', u'B', u'L', u'AE18DBF4BBFAF9D0A9B1B64FA6FB3A41', u'en', u'Zootaxa', u'Hypoponera', u'aprora', u'Bolton', u'Fisher', u'sp', u'n', u'C7AFAB2882DB861B2AC799DC582E378E', u'Taxonomy', u'Afrotropical', u'West', u'Palaearctic', u'ants', u'the', u'ponerine', u'genus', u'Hypoponera', u'Santschi', u'Hymenoptera', u'Formicidae', u'admin', u'Taxonomy', u'Afrotropical', u'West', u'Palaearctic', u'ants', u'the', u'ponerine', u'genus', u'Hypoponera', u'Santschi', u'Hymenoptera', u'Formicidae', u'personal', u'Author', u'Bolton', u'B', u'personal', u'Author', u'Fisher', u'B', u'L', u'text', u'host', u'Zootaxa', u'volume', u'page', u'journal', u'article', u'HNS-PUB', u'urn', u'lsid', u'plazi', u'treatment', u'AE18DBF4BBFAF9D0A9B1B64FA6FB3A41', u'http', u'treatment', u'plazi', u'org', u'id', u'AE18DBF4BBFAF9D0A9B1B64FA6FB3A41', u'nomenclature', u'Hypoponera', u'aprora', u'Bolton', u'Fisher', u'urn', u'lsid', u'biosci', u'ohio-state', u'edu', u'osuc_concepts', u'Insecta', u'Formicidae', u'Hypoponera', u'CoL', u'Animalia', u'Hypoponera', u'aprora', u'Bolton', u'Fisher', u'Hymenoptera', u'Arthropoda', u'species', u'aprora', u'sp', u'n', u'species', u'description', u'Figs', u'WORKER', u'holotype', u'diagnosis', u'Conforming', u'to', u'the', u'general', u'description', u'given', u'for', u'dis', u'but', u'lacking', u'a', u'prora', u'In', u'aprora', u'urn', u'lsid', u'biosci', u'ohio-state', u'edu', u'osuc_concepts', u'Insecta', u'Formicidae', u'Hypoponera', u'missing', u'matching', u'species', u'CoL', u'Animalia', u'Hypoponera', u'aprora', u'Bolton', u'Fisher', u'Hymenoptera', u'Arthropoda', u'species', u'aprora', u'materials_examined', u'SAMC', u'P', u'Hawkes', u'J', u'Makwati', u'R', u'Mtana', u'Tanzania', u'Tanga', u'Region', u'CEPF-TZ', u'F', u'Holotype', u'Holotype', u'Tanzania', u'Tanga', u'Region', u'urn', u'lsid', u'plazi', u'treatment', u'AE18DBF4BBFAF9D0A9B1B64FA6FB3A41', u'Tanzania', u'Tanga', u'Region', u'Kilindi', u'Forest', u'Reserve', u'urn', u'lsid', u'plazi', u'treatment', u'AE18DBF4BBFAF9D0A9B1B64FA6FB3A41', u'Tanzania', u'Kilindi', u'Forest', u'Reserve', u'viii', u'CEPF-', u'-F09', u'south', u'latitude', u'east', u'longitude', u'Winkler', u'urn', u'lsid', u'plazi', u'treatment', u'AE18DBF4BBFAF9D0A9B1B64FA6FB3A41', u'Tanzania', u'Winkler', u'P', u'Hawkes', u'J', u'Makwati', u'R', u'Mtana', u'SAMC', u'South', u'Africa', u'Cape', u'Town', u'Iziko', u'Museum', u'Capetown', u'formerly', u'South', u'African', u'Museum', u'SAMC', u'CASC', u'AFRC', u'BBRC', u'P', u'Hawkes', u'J', u'Makwati', u'R', u'Mtana', u'Tanzania', u'Tanga', u'Region', u'CEPF-TZ', u'F', u'Paratypes', u'Paratypes', u'SAMC', u'South', u'Africa', u'Cape', u'Town', u'Iziko', u'Museum', u'Capetown', u'formerly', u'South', u'African', u'Museum', u'CASC', u'USA', u'California', u'San', u'Francisco', u'California', u'Academy', u'Sciences', u'AFRC', u'BBRC', u'diagnosis', u'This', u'start', u'occidentalis', u'urn', u'lsid', u'biosci', u'ohio-state', u'edu', u'osuc_concepts', u'Insecta', u'Formicidae', u'Hypoponera', u'missing', u'matching', u'species', u'CoL', u'Animalia', u'Hypoponera', u'occidentalis', u'Bernard', u'Hymenoptera', u'Arthropoda', u'species', u'occidentalis', u'abeillei', u'urn', u'lsid', u'biosci', u'ohio-state', u'edu', u'osuc_concepts', u'Insecta', u'Formicidae', u'Ponera', u'missing', u'matching', u'species', u'CoL', u'Animalia', u'Ponera', u'abeillei', u'Andr', u'Hymenoptera', u'Arthropoda', u'species', u'abeillei', u'occidentalis', u'urn', u'lsid', u'biosci', u'ohio-state', u'edu', u'osuc_concepts', u'Insecta', u'Formicidae', u'Hypoponera', u'missing', u'matching', u'species', u'CoL', u'Animalia', u'Hypoponera', u'occidentalis', u'Bernard', u'Hymenoptera', u'Arthropoda', u'species', u'occidentalis']\n"
     ]
    }
   ],
   "source": [
    "print bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
