{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Basic word (token) counts for XML responses.\n",
    "\n",
    "I will note, wherever necessary, assumptions related to the stopwords corpora. For this count, mimetypes, namespaces and URNs (specifically those related to EPSG codes and the like). Also ditching numbers and timestamps.\n",
    "\n",
    "Given that we are looking at a simple token count, the initial BoW vectors will include some errant punctuation. Punctuation-only tokens are dropped but things like '(This' are left as is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import glob\n",
    "import re\n",
    "from datetime import datetime\n",
    "import dateutil.parser as dateparser\n",
    "from itertools import chain\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import WordListCorpusReader\n",
    "\n",
    "from semproc.rawresponse import RawResponse\n",
    "from semproc.bag_parser import BagParser\n",
    "\n",
    "import json as js  # name conflict with sqla\n",
    "import sqlalchemy as sqla\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.dialects.postgresql import *\n",
    "from sqlalchemy import and_\n",
    "from mpp.models import Response\n",
    "from mpp.models import BagOfWords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_header_list(headers):\n",
    "    '''\n",
    "    convert from the list of strings, one string\n",
    "    per kvp, to a dict with keys normalized\n",
    "    '''\n",
    "    return dict(\n",
    "        (k.strip().lower(), v.strip()) for k, v in (\n",
    "            h.split(':', 1) for h in headers)\n",
    "    )\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''\n",
    "    remove any known english stopwords from a\n",
    "    piece of text (bag of words or otherwise)\n",
    "    '''\n",
    "    _stopwords = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    words = words if isinstance(words, list) else words.split()\n",
    "    return ' '.join([w for w in words if w not in _stopwords and w])\n",
    "\n",
    "\n",
    "def load_token_list(term_file):\n",
    "    '''\n",
    "    load some stopword list from the corpus\n",
    "    '''\n",
    "    __location__ = '../corpora/'\n",
    "    tokens = WordListCorpusReader(__location__, term_file)\n",
    "    return [w.replace('+', '\\+') for w in tokens.words()]\n",
    "\n",
    "\n",
    "def remove_tokens(term_file, text):\n",
    "    '''\n",
    "    do this before something like tokenize or the\n",
    "    resplit option will split the mimetypes to not\n",
    "    be recognizable as such anymore\n",
    "    '''\n",
    "    words = load_token_list(term_file)\n",
    "\n",
    "    pttn = re.compile('|'.join(words))\n",
    "    return pttn.sub('', text)\n",
    "\n",
    "def remove_numeric(text):\n",
    "    match_pttn = ur'\\w*\\b-?\\d\\s*\\w*'\n",
    "    captures = re.findall(match_pttn, u' {0} '.format(text))\n",
    "\n",
    "    # strip them out\n",
    "    if captures:\n",
    "        text = re.sub('|'.join(captures), ' ', text)\n",
    "        return '' if text == '0' else text\n",
    "\n",
    "    return text\n",
    "\n",
    "def strip_dates(text):\n",
    "        # this should still make it an invalid date\n",
    "        # text = text[3:] if text.startswith('NaN') else text\n",
    "        try:\n",
    "            d = dateparser.parse(text)\n",
    "            return ''\n",
    "        except ValueError:\n",
    "            return text\n",
    "        except OverflowError:\n",
    "            return text\n",
    "        \n",
    "def strip_filenames(text):\n",
    "    # we'll see\n",
    "    exts = ('png', 'jpg', 'hdf', 'xml', 'doc', 'pdf', 'txt', 'jar', 'nc', 'XSL', 'kml', 'xsd')\n",
    "    return '' if text.endswith(exts) else text\n",
    "    \n",
    "def strip_identifiers(texts):\n",
    "    # chuck any urns, urls, uuids\n",
    "    _pattern_set = [\n",
    "        ('url', ur\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"),\n",
    "        # a urn that isn't a url\n",
    "        ('urn', ur\"(?![http://])(?![https://])(?![ftp://])(([a-z0-9.\\S][a-z0-9-.\\S]{0,}\\S:{1,2}\\S)+[a-z0-9()+,\\-.=@;$_!*'%/?#]+)\"),\n",
    "        ('uuid', ur'([a-f\\d]{8}(-[a-f\\d]{4}){3}-[a-f\\d]{12}?)'),\n",
    "        ('doi', ur\"(10[.][0-9]{4,}(?:[/][0-9]+)*/(?:(?![\\\"&\\\\'])\\S)+)\"),\n",
    "        ('md5', ur\"([a-f0-9]{32})\")\n",
    "    ]\n",
    "    for pattern_type, pattern in _pattern_set:\n",
    "        for m in re.findall(re.compile(pattern), texts):\n",
    "            m = max(m) if isinstance(m, tuple) else m\n",
    "            try:\n",
    "                texts = texts.replace(m, '')\n",
    "            except Exception as ex:\n",
    "                print ex\n",
    "                print m\n",
    "                \n",
    "    files = ['cat_interop_urns.txt', 'mimetypes.txt', 'namespaces.txt']\n",
    "    for f in files:\n",
    "        texts = remove_tokens(f, texts)\n",
    "    return texts.split()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    simple_pattern = r'[;|>+:=.,()/?!\\[\\]{}]'\n",
    "    text = re.sub(simple_pattern, ' ', text)\n",
    "    text = text.replace(' - ', ' ').strip()\n",
    "    return text if text != '-' else ''\n",
    "\n",
    "def strip_punctuation(text):\n",
    "    terminal_punctuation = '(){}[].,~|\":'\n",
    "    return text.strip(terminal_punctuation).strip()\n",
    "    \n",
    "def clean(text):\n",
    "    text = strip_dates(text)\n",
    "    text = remove_numeric(text)\n",
    "    \n",
    "    text = remove_punctuation(text.strip()).strip()\n",
    "    text = strip_punctuation(text)\n",
    "    \n",
    "    text = strip_filenames(text)\n",
    "    \n",
    "    return text\n",
    "        \n",
    "exclude_tags = ['schemaLocation', 'noNamespaceSchemaLocation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grab the clean text from the rds\n",
    "with open('../local/big_rds.conf', 'r') as f:\n",
    "    conf = js.loads(f.read())\n",
    "\n",
    "# our connection\n",
    "engine = sqla.create_engine(conf.get('connection'))\n",
    "Session = sessionmaker()\n",
    "Session.configure(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get a count of the xml responses\n",
    "total = session.query(Response).filter(Response.format=='xml').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LIMIT = 100\n",
    "\n",
    "# total = 5\n",
    "# LIMIT= total\n",
    "\n",
    "for i in xrange(0, total, LIMIT):\n",
    "    # get some responses\n",
    "    responses = session.query(Response).filter(Response.format=='xml').limit(LIMIT).offset(i).all()\n",
    "    \n",
    "    appends = []\n",
    "    \n",
    "    for response in responses:\n",
    "        cleaned_content = response.cleaned_content\n",
    "        \n",
    "        # strip the html cruft but ignore the a tags\n",
    "        bp = BagParser(cleaned_content.encode('utf-8'), True, False)\n",
    "        if bp.parser.xml is None:\n",
    "            print 'NOT XML: ', cleaned_content[:100]\n",
    "            continue\n",
    "        # we don't care about the fully qualified namespace here\n",
    "        stripped_text = [b[1].split() for b in bp.strip_text(exclude_tags) if b[1]]\n",
    "        stripped_text = list(chain.from_iterable(stripped_text))\n",
    "        cleaned_text = [s for s in stripped_text if clean(s)]\n",
    "\n",
    "        bow = strip_identifiers(' '.join(cleaned_text))\n",
    "        \n",
    "        bag = BagOfWords(\n",
    "            generated_on=datetime.now().isoformat(),\n",
    "            bag_of_words=bow,\n",
    "            method=\"basic\",\n",
    "            response_id=response.id\n",
    "        )\n",
    "        appends.append(bag)\n",
    "    \n",
    "    try:\n",
    "        session.add_all(appends)\n",
    "        session.commit()\n",
    "    except Exception as ex:\n",
    "        print 'failed ', i, ex\n",
    "        session.rollback()\n",
    "    \n",
    "session.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import not_, func\n",
    "\n",
    "TOTAL = session.query(Response).filter(\n",
    "    and_(Response.format=='xml', not_(func.lower(Response.cleaned_content).startswith(\"<rdf\")))).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
